{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing, extending and serializing\n",
    "\n",
    "The usual way to store generic binary data in python-blosc2 is through a `SChunk` (super-chunk) object, where the data is split into chunks of the same size. With it, you can retrieve, update or append data at item level (i.e. avoiding doing it chunk by chunk). In this tutorial, we will use NumPy arrays to show operation, but you can replace them with any Python object supporting the [Buffer Protocol](https://docs.python.org/3/c-api/buffer.html).\n",
    "\n",
    "To see how this works, let's first create our own `SChunk` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:26.170931Z",
     "start_time": "2024-11-27T12:54:24.424779Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import blosc2\n",
    "\n",
    "nchunks = 10\n",
    "data = np.arange(200 * 1000 * nchunks, dtype=np.int32)\n",
    "cparams = blosc2.CParams(typesize=4)\n",
    "schunk = blosc2.SChunk(chunksize=200 * 1000 * 4, data=data, cparams=cparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to set the `typesize` correctly as these methods will work with items and not with bytes.\n",
    "\n",
    "## Getting data from a SChunk\n",
    "\n",
    "Let's begin by retrieving the data from the whole SChunk. We could use the `decompress_chunk` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:26.187892Z",
     "start_time": "2024-11-27T12:54:26.175998Z"
    }
   },
   "outputs": [],
   "source": [
    "out = np.empty(200 * 1000 * nchunks, dtype=np.int32)\n",
    "for i in range(nchunks):\n",
    "    schunk.decompress_chunk(i, out[200 * 1000 * i : 200 * 1000 * (i + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But instead of the code above, we can simply use the `__getitem__` or the `get_slice` methods. Let's begin with `__getitem__`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:26.252087Z",
     "start_time": "2024-11-27T12:54:26.240646Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_slice = schunk[:]\n",
    "type(out_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data is returned as a bytes object. If we want to get a more meaningful container instead, we can use `get_slice`, where you can pass any Python object (supporting the Buffer Protocol) as the `out` param to fill it with the data.  In this case we will use a NumPy array container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:26.374961Z",
     "start_time": "2024-11-27T12:54:26.355669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "out_slice = np.empty(200 * 1000 * nchunks, dtype=np.int32)\n",
    "schunk.get_slice(out=out_slice)\n",
    "np.array_equal(out, out_slice)\n",
    "print(out_slice[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the expected data indeed!\n",
    "\n",
    "## Setting (and enlarging) data in a SChunk\n",
    "\n",
    "We can also set the data of a `SChunk` area from any Python object supporting the Buffer Protocol. Let's see a quick example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:26.489672Z",
     "start_time": "2024-11-27T12:54:26.483499Z"
    }
   },
   "outputs": [],
   "source": [
    "start = 34\n",
    "stop = 1000 * 200 * 4\n",
    "new_value = np.ones(stop - start, dtype=np.int32)\n",
    "schunk[start:stop] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to get or set data. But what if we would like to add data? Well, you can still do that with `__setitem__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:26.525337Z",
     "start_time": "2024-11-27T12:54:26.520111Z"
    }
   },
   "outputs": [],
   "source": [
    "schunk_nelems = 1000 * 200 * nchunks\n",
    "\n",
    "new_value = np.zeros(1000 * 200 * 2 + 53, dtype=np.int32)\n",
    "start = schunk_nelems - 123\n",
    "new_nitems = start + new_value.size\n",
    "schunk[start:new_nitems] = new_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Here, `start` is less than the number of elements in `SChunk` and `new_items` is larger than this; that means that `__setitem__` can update and append data at the same time, and you don't have to worry about whether you are exceeding the limits of the `SChunk`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a SChunk from/as a contiguous buffer\n",
    "\n",
    "Furthermore, you can convert a SChunk to a contiguous, serialized buffer and vice-versa. Let's get that buffer (aka `cframe`) first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:26.544496Z",
     "start_time": "2024-11-27T12:54:26.539453Z"
    }
   },
   "outputs": [],
   "source": [
    "buf = schunk.to_cframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the other way around:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:26.556900Z",
     "start_time": "2024-11-27T12:54:26.554139Z"
    }
   },
   "outputs": [],
   "source": [
    "schunk2 = blosc2.schunk_from_cframe(cframe=buf, copy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we set the `copy` param to `True`. If you do not want to copy the buffer, be mindful that you will have to keep a reference to it until you do not want the SChunk anymore.\n",
    "\n",
    "\n",
    "## Serializing NumPy arrays\n",
    "\n",
    "If what you want is to create a serialized, compressed version of a NumPy array, you can use the newer (and faster) functions to store it either in-memory or on-disk.  The specification of such a contiguous compressed representation, aka **cframe** can be seen [here](https://github.com/Blosc/c-blosc2/blob/main/README_CFRAME_FORMAT.rst).\n",
    "\n",
    "### In-memory\n",
    "\n",
    "For obtaining an in-memory representation, you can use `pack_tensor`. In comparison with its former version (`pack_array`), it is way faster and does not have the 2 GB size limitation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:33.361503Z",
     "start_time": "2024-11-27T12:54:26.570171Z"
    }
   },
   "outputs": [],
   "source": [
    "np_array = np.arange(2**30, dtype=np.int32)  # 4 GB array\n",
    "\n",
    "packed_arr2 = blosc2.pack_tensor(np_array)\n",
    "unpacked_arr2 = blosc2.unpack_tensor(packed_arr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-disk\n",
    "\n",
    "To store the serialized buffer on-disk you want to use `save_tensor` and `load_tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T12:54:41.174737Z",
     "start_time": "2024-11-27T12:54:33.514280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blosc2.save_tensor(np_array, urlpath=\"ondisk_array.b2frame\", mode=\"w\")\n",
    "np_array2 = blosc2.load_tensor(\"ondisk_array.b2frame\")\n",
    "np.array_equal(np_array, np_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Now python-blosc2 offers an easy, yet fast way of creating, getting, setting and expanding data via the `SChunk` class.  Moreover, you can get a contiguous compressed representation (aka [cframe](https://github.com/Blosc/c-blosc2/blob/main/README_CFRAME_FORMAT.rst)) of it and re-create it again later with no sweat.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
