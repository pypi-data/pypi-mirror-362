"""å¸‚åœºæ•°æ®æœåŠ¡æ¨¡å—ã€‚

æä¾›åŠ å¯†è´§å¸å¸‚åœºæ•°æ®è·å–ã€å¤„ç†å’Œå­˜å‚¨åŠŸèƒ½ã€‚
"""

import logging
import time
import random
from concurrent.futures import ThreadPoolExecutor, as_completed
from contextlib import nullcontext
from datetime import datetime, timedelta
from decimal import Decimal
from pathlib import Path
from threading import Lock
from typing import Any, Dict, List, Optional, cast
import threading
import requests
import zipfile
from io import BytesIO
import csv
from urllib3.util.retry import Retry
from requests.adapters import HTTPAdapter

import pandas as pd
from rich.logging import RichHandler
from rich.progress import (
    BarColumn,
    Progress,
    SpinnerColumn,
    TextColumn,
    TimeElapsedColumn,
)

from cryptoservice.client import BinanceClientFactory
from cryptoservice.config import settings, RetryConfig
from cryptoservice.data import MarketDB
from cryptoservice.exceptions import (
    InvalidSymbolError,
    MarketDataFetchError,
)
from cryptoservice.interfaces import IMarketDataService
from cryptoservice.models import (
    DailyMarketTicker,
    Freq,
    HistoricalKlinesType,
    KlineMarketTicker,
    PerpetualMarketTicker,
    SortBy,
    SymbolTicker,
    UniverseConfig,
    UniverseDefinition,
    UniverseSnapshot,
    ErrorSeverity,
    IntegrityReport,
    FundingRate,
    OpenInterest,
    LongShortRatio,
)
from cryptoservice.utils import DataConverter

# é…ç½® rich logger
logging.basicConfig(
    level=logging.INFO,
    format="%(message)s",
    handlers=[RichHandler(rich_tracebacks=True)],
)
logger = logging.getLogger(__name__)

cache_lock = Lock()


class RateLimitManager:
    """APIé¢‘ç‡é™åˆ¶ç®¡ç†å™¨"""

    def __init__(self, base_delay: float = 0.5):
        self.base_delay = base_delay
        self.current_delay = base_delay
        self.last_request_time = 0.0
        self.request_count = 0
        self.window_start_time = time.time()
        self.consecutive_errors = 0
        self.max_requests_per_minute = 1800  # ä¿å®ˆä¼°è®¡ï¼Œä½äºAPIé™åˆ¶
        self.lock = threading.Lock()

    def wait_before_request(self):
        """åœ¨è¯·æ±‚å‰ç­‰å¾…é€‚å½“çš„æ—¶é—´"""
        with self.lock:
            current_time = time.time()

            # é‡ç½®è®¡æ•°çª—å£ï¼ˆæ¯åˆ†é’Ÿï¼‰
            if current_time - self.window_start_time >= 60:
                self.request_count = 0
                self.window_start_time = current_time
                # å¦‚æœé•¿æ—¶é—´æ²¡æœ‰é”™è¯¯ï¼Œé€æ¸é™ä½å»¶è¿Ÿ
                if self.consecutive_errors == 0:
                    self.current_delay = max(self.base_delay, self.current_delay * 0.9)

                    # æ£€æŸ¥æ˜¯å¦æ¥è¿‘é¢‘ç‡é™åˆ¶
            requests_this_minute = self.request_count

            if requests_this_minute >= self.max_requests_per_minute * 0.8:  # è¾¾åˆ°80%é™åˆ¶æ—¶å¼€å§‹å‡é€Ÿ
                additional_delay = 2.0
                logger.warning(f"âš ï¸ æ¥è¿‘é¢‘ç‡é™åˆ¶ï¼Œå¢åŠ å»¶è¿Ÿ: {additional_delay}ç§’")
            else:
                additional_delay = 0

            # è®¡ç®—éœ€è¦ç­‰å¾…çš„æ—¶é—´
            time_since_last = current_time - self.last_request_time
            total_delay = self.current_delay + additional_delay

            if time_since_last < total_delay:
                wait_time = total_delay - time_since_last
                if wait_time > 0.1:  # åªè®°å½•è¾ƒé•¿çš„ç­‰å¾…æ—¶é—´
                    logger.debug(f"ç­‰å¾… {wait_time:.2f}ç§’ (å½“å‰å»¶è¿Ÿ: {self.current_delay:.2f}ç§’)")
                time.sleep(wait_time)

            self.last_request_time = time.time()
            self.request_count += 1

    def handle_rate_limit_error(self):
        """å¤„ç†é¢‘ç‡é™åˆ¶é”™è¯¯"""
        with self.lock:
            self.consecutive_errors += 1

            # åŠ¨æ€å¢åŠ å»¶è¿Ÿ
            if self.consecutive_errors <= 3:
                self.current_delay = min(10.0, self.current_delay * 2)
                wait_time = 60  # ç­‰å¾…1åˆ†é’Ÿ
            elif self.consecutive_errors <= 6:
                self.current_delay = min(15.0, self.current_delay * 1.5)
                wait_time = 120  # ç­‰å¾…2åˆ†é’Ÿ
            else:
                self.current_delay = 20.0
                wait_time = 300  # ç­‰å¾…5åˆ†é’Ÿ

            logger.warning(
                f"ğŸš« é¢‘ç‡é™åˆ¶é”™è¯¯ #{self.consecutive_errors}ï¼Œç­‰å¾… {wait_time}ç§’ï¼Œè°ƒæ•´å»¶è¿Ÿè‡³ {self.current_delay:.2f}ç§’"
            )

            # é‡ç½®è¯·æ±‚è®¡æ•°
            self.request_count = 0
            self.window_start_time = time.time()

            return wait_time

    def handle_success(self):
        """å¤„ç†æˆåŠŸè¯·æ±‚"""
        with self.lock:
            if self.consecutive_errors > 0:
                self.consecutive_errors = max(0, self.consecutive_errors - 1)
                if self.consecutive_errors == 0:
                    logger.info(f"âœ… æ¢å¤æ­£å¸¸ï¼Œå½“å‰å»¶è¿Ÿ: {self.current_delay:.2f}ç§’")


class ExponentialBackoff:
    """æŒ‡æ•°é€€é¿å®ç°"""

    def __init__(self, config: RetryConfig):
        self.config = config
        self.attempt = 0

    def reset(self):
        """é‡ç½®é‡è¯•è®¡æ•°"""
        self.attempt = 0

    def wait(self) -> float:
        """è®¡ç®—å¹¶æ‰§è¡Œç­‰å¾…æ—¶é—´"""
        if self.attempt >= self.config.max_retries:
            raise Exception(f"è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°: {self.config.max_retries}")

        # è®¡ç®—åŸºç¡€å»¶è¿Ÿ
        delay = min(
            self.config.base_delay * (self.config.backoff_multiplier**self.attempt),
            self.config.max_delay,
        )

        # æ·»åŠ æŠ–åŠ¨ä»¥é¿å…æƒŠç¾¤æ•ˆåº”
        if self.config.jitter:
            delay *= 0.5 + random.random() * 0.5

        self.attempt += 1

        logger.debug(f"æŒ‡æ•°é€€é¿: ç¬¬{self.attempt}æ¬¡é‡è¯•, ç­‰å¾…{delay:.2f}ç§’")
        time.sleep(delay)

        return delay


class EnhancedErrorHandler:
    """å¢å¼ºé”™è¯¯å¤„ç†å™¨"""

    @staticmethod
    def classify_error(error: Exception) -> ErrorSeverity:
        """é”™è¯¯åˆ†ç±»"""
        error_str = str(error).lower()

        # APIé¢‘ç‡é™åˆ¶
        if any(
            keyword in error_str
            for keyword in [
                "too many requests",
                "rate limit",
                "429",
                "request limit",
                "-1003",
            ]
        ):
            return ErrorSeverity.MEDIUM

        # SSLç›¸å…³é”™è¯¯ - é€šå¸¸æ˜¯ç½‘ç»œä¸ç¨³å®šï¼Œå¯é‡è¯•
        if any(
            keyword in error_str
            for keyword in [
                "ssl",
                "sslerror",
                "ssleoferror",
                "unexpected_eof_while_reading",
                "ssl: unexpected_eof_while_reading",
                "certificate verify failed",
                "handshake failure",
                "ssl: handshake_failure",
                "ssl: tlsv1_alert_protocol_version",
                "ssl: wrong_version_number",
                "ssl context",
                "ssl: certificate_verify_failed",
                "ssl: bad_record_mac",
                "ssl: decryption_failed_or_bad_record_mac",
                "ssl: sslv3_alert_handshake_failure",
                "ssl: tlsv1_alert_internal_error",
                "ssl: connection_lost",
                "ssl: application_data_after_close_notify",
                "ssl: bad_certificate",
                "ssl: unsupported_certificate",
                "ssl: certificate_required",
                "ssl: no_shared_cipher",
                "ssl: peer_did_not_return_a_certificate",
                "ssl: certificate_unknown",
                "ssl: illegal_parameter",
                "ssl: unknown_ca",
                "ssl: access_denied",
                "ssl: decode_error",
                "ssl: decrypt_error",
                "ssl: export_restriction",
                "ssl: protocol_version",
                "ssl: insufficient_security",
                "ssl: internal_error",
                "ssl: user_cancelled",
                "ssl: no_renegotiation",
                "ssl: unsupported_extension",
                "ssl: certificate_unobtainable",
                "ssl: unrecognized_name",
                "ssl: bad_certificate_status_response",
                "ssl: bad_certificate_hash_value",
                "ssl: unknown_psk_identity",
                "eof occurred in violation of protocol",
                "connection was interrupted",
                "connection aborted",
                "connection reset by peer",
                "broken pipe",
                "connection timed out",
                "connection refused",
            ]
        ):
            return ErrorSeverity.MEDIUM

        # ç½‘ç»œç›¸å…³é”™è¯¯
        if any(keyword in error_str for keyword in ["connection", "timeout", "network", "dns", "socket"]):
            return ErrorSeverity.MEDIUM

        # æ— æ•ˆäº¤æ˜“å¯¹
        if any(keyword in error_str for keyword in ["invalid symbol", "symbol not found", "unknown symbol"]):
            return ErrorSeverity.LOW

        # æœåŠ¡å™¨é”™è¯¯
        if any(
            keyword in error_str
            for keyword in [
                "500",
                "502",
                "503",
                "504",
                "server error",
                "internal error",
            ]
        ):
            return ErrorSeverity.HIGH

        # è®¤è¯é”™è¯¯
        if any(keyword in error_str for keyword in ["unauthorized", "forbidden", "api key", "signature"]):
            return ErrorSeverity.CRITICAL

        # é»˜è®¤ä¸ºä¸­ç­‰ä¸¥é‡æ€§
        return ErrorSeverity.MEDIUM

    @staticmethod
    def should_retry(error: Exception, attempt: int, max_retries: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•"""
        severity = EnhancedErrorHandler.classify_error(error)

        if severity == ErrorSeverity.CRITICAL:
            return False

        if severity == ErrorSeverity.LOW and attempt > 1:
            return False

        return attempt < max_retries

    @staticmethod
    def get_recommended_action(error: Exception) -> str:
        """è·å–æ¨èå¤„ç†åŠ¨ä½œ"""
        severity = EnhancedErrorHandler.classify_error(error)
        error_str = str(error).lower()

        if severity == ErrorSeverity.CRITICAL:
            return "æ£€æŸ¥APIå¯†é’¥å’Œæƒé™è®¾ç½®"
        elif "rate limit" in error_str or "-1003" in error_str:
            return "é¢‘ç‡é™åˆ¶ï¼Œè‡ªåŠ¨è°ƒæ•´è¯·æ±‚é—´éš”"
        elif any(
            keyword in error_str
            for keyword in [
                "ssl",
                "sslerror",
                "ssleoferror",
                "unexpected_eof_while_reading",
            ]
        ):
            return "SSLè¿æ¥é”™è¯¯ï¼Œè‡ªåŠ¨é‡è¯•å¹¶è€ƒè™‘ç½‘ç»œç¨³å®šæ€§"
        elif "connection" in error_str:
            return "æ£€æŸ¥ç½‘ç»œè¿æ¥ï¼Œè€ƒè™‘ä½¿ç”¨ä»£ç†"
        elif "invalid symbol" in error_str:
            return "éªŒè¯äº¤æ˜“å¯¹æ˜¯å¦å­˜åœ¨å’Œå¯äº¤æ˜“"
        else:
            return "æ£€æŸ¥APIæ–‡æ¡£å’Œé”™è¯¯è¯¦æƒ…"

    @staticmethod
    def is_rate_limit_error(error: Exception) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé¢‘ç‡é™åˆ¶é”™è¯¯"""
        error_str = str(error).lower()
        return any(keyword in error_str for keyword in ["too many requests", "rate limit", "429", "-1003"])


class MarketDataService(IMarketDataService):
    """å¸‚åœºæ•°æ®æœåŠ¡å®ç°ç±»ã€‚"""

    def __init__(self, api_key: str, api_secret: str) -> None:
        """åˆå§‹åŒ–å¸‚åœºæ•°æ®æœåŠ¡ã€‚

        Args:
            api_key: ç”¨æˆ·APIå¯†é’¥
            api_secret: ç”¨æˆ·APIå¯†é’¥
        """
        self.client = BinanceClientFactory.create_client(api_key, api_secret)
        self.converter = DataConverter()
        self.db: MarketDB | None = None
        self.rate_limit_manager = RateLimitManager()
        self.failed_downloads: dict[str, list[dict]] = {}  # è®°å½•å¤±è´¥çš„ä¸‹è½½

    @staticmethod
    def _validate_and_prepare_path(path: Path | str, is_file: bool = False, file_name: str | None = None) -> Path:
        """éªŒè¯å¹¶å‡†å¤‡è·¯å¾„ã€‚

        Args:
            path: è·¯å¾„å­—ç¬¦ä¸²æˆ–Pathå¯¹è±¡
            is_file: æ˜¯å¦ä¸ºæ–‡ä»¶è·¯å¾„
            file_name: æ–‡ä»¶å
        Returns:
            Path: éªŒè¯åçš„Pathå¯¹è±¡

        Raises:
            ValueError: è·¯å¾„ä¸ºç©ºæˆ–æ— æ•ˆæ—¶
        """
        if not path:
            raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©ºï¼Œå¿…é¡»æ‰‹åŠ¨æŒ‡å®š")

        path_obj = Path(path)

        # å¦‚æœæ˜¯æ–‡ä»¶è·¯å¾„ï¼Œç¡®ä¿çˆ¶ç›®å½•å­˜åœ¨
        if is_file:
            if path_obj.is_dir():
                path_obj = path_obj.joinpath(file_name) if file_name else path_obj
            else:
                path_obj.parent.mkdir(parents=True, exist_ok=True)
        else:
            # å¦‚æœæ˜¯ç›®å½•è·¯å¾„ï¼Œç¡®ä¿ç›®å½•å­˜åœ¨
            path_obj.mkdir(parents=True, exist_ok=True)

        return path_obj

    def get_symbol_ticker(self, symbol: str | None = None) -> SymbolTicker | list[SymbolTicker]:
        """è·å–å•ä¸ªæˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°

        Returns:
            SymbolTicker | list[SymbolTicker]: å•ä¸ªäº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®æˆ–æ‰€æœ‰äº¤æ˜“å¯¹çš„è¡Œæƒ…æ•°æ®
        """
        try:
            ticker = self.client.get_symbol_ticker(symbol=symbol)
            if not ticker:
                raise InvalidSymbolError(f"Invalid symbol: {symbol}")

            if isinstance(ticker, list):
                return [SymbolTicker.from_binance_ticker(t) for t in ticker]
            return SymbolTicker.from_binance_ticker(ticker)

        except Exception as e:
            logger.error(f"[red]Error fetching ticker for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to fetch ticker: {e}") from e

    def get_perpetual_symbols(self, only_trading: bool = True, quote_asset: str = "USDT") -> list[str]:
        """è·å–å½“å‰å¸‚åœºä¸Šæ‰€æœ‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            only_trading: æ˜¯å¦åªè¿”å›å½“å‰å¯äº¤æ˜“çš„äº¤æ˜“å¯¹
            quote_asset: åŸºå‡†èµ„äº§ï¼Œé»˜è®¤ä¸ºUSDTï¼Œåªè¿”å›ä»¥è¯¥èµ„äº§ç»“å°¾çš„äº¤æ˜“å¯¹

        Returns:
            list[str]: æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            logger.info(f"è·å–å½“å‰æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹åˆ—è¡¨ï¼ˆç­›é€‰æ¡ä»¶ï¼š{quote_asset}ç»“å°¾ï¼‰")
            futures_info = self.client.futures_exchange_info()
            perpetual_symbols = [
                symbol["symbol"]
                for symbol in futures_info["symbols"]
                if symbol["contractType"] == "PERPETUAL"
                and (not only_trading or symbol["status"] == "TRADING")
                and symbol["symbol"].endswith(quote_asset)
            ]

            logger.info(f"æ‰¾åˆ° {len(perpetual_symbols)} ä¸ª{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹")
            return perpetual_symbols

        except Exception as e:
            logger.error(f"[red]è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"è·å–æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹å¤±è´¥: {e}") from e

    def _date_to_timestamp_range(self, date: str) -> tuple[str, str]:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´ï¼ˆå¼€å§‹å’Œç»“æŸï¼‰ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            tuple[str, str]: (å¼€å§‹æ—¶é—´æˆ³, ç»“æŸæ—¶é—´æˆ³)ï¼Œéƒ½æ˜¯æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
            - å¼€å§‹æ—¶é—´æˆ³: å½“å¤©çš„ 00:00:00
            - ç»“æŸæ—¶é—´æˆ³: å½“å¤©çš„ 23:59:59
        """
        start_time = int(datetime.strptime(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
        end_time = int(datetime.strptime(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
        return str(start_time), str(end_time)

    def _date_to_timestamp_start(self, date: str) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©å¼€å§‹çš„æ—¶é—´æˆ³ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            str: å½“å¤© 00:00:00 çš„æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
        """
        timestamp = int(datetime.strptime(f"{date} 00:00:00", "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
        return str(timestamp)

    def _date_to_timestamp_end(self, date: str) -> str:
        """å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºå½“å¤©ç»“æŸçš„æ—¶é—´æˆ³ã€‚

        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            str: å½“å¤© 23:59:59 çš„æ¯«ç§’çº§æ—¶é—´æˆ³å­—ç¬¦ä¸²
        """
        timestamp = int(datetime.strptime(f"{date} 23:59:59", "%Y-%m-%d %H:%M:%S").timestamp() * 1000)
        return str(timestamp)

    def check_symbol_exists_on_date(self, symbol: str, date: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸæ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            date: æ—¥æœŸï¼Œæ ¼å¼ä¸º 'YYYY-MM-DD'

        Returns:
            bool: æ˜¯å¦å­˜åœ¨è¯¥äº¤æ˜“å¯¹
        """
        try:
            # å°†æ—¥æœŸè½¬æ¢ä¸ºæ—¶é—´æˆ³èŒƒå›´
            start_time, end_time = self._date_to_timestamp_range(date)

            # å°è¯•è·å–è¯¥æ—¶é—´èŒƒå›´å†…çš„Kçº¿æ•°æ®
            klines = self.client.futures_klines(
                symbol=symbol,
                interval="1d",
                startTime=start_time,
                endTime=end_time,
                limit=1,
            )

            # å¦‚æœæœ‰æ•°æ®ï¼Œè¯´æ˜è¯¥æ—¥æœŸå­˜åœ¨è¯¥äº¤æ˜“å¯¹
            return bool(klines and len(klines) > 0)

        except Exception as e:
            logger.debug(f"æ£€æŸ¥äº¤æ˜“å¯¹ {symbol} åœ¨ {date} æ˜¯å¦å­˜åœ¨æ—¶å‡ºé”™: {e}")
            return False

    def get_top_coins(
        self,
        limit: int = settings.DEFAULT_LIMIT,
        sort_by: SortBy = SortBy.QUOTE_VOLUME,
        quote_asset: str | None = None,
    ) -> list[DailyMarketTicker]:
        """è·å–å‰Nä¸ªäº¤æ˜“å¯¹ã€‚

        Args:
            limit: æ•°é‡
            sort_by: æ’åºæ–¹å¼
            quote_asset: åŸºå‡†èµ„äº§

        Returns:
            list[DailyMarketTicker]: å‰Nä¸ªäº¤æ˜“å¯¹
        """
        try:
            tickers = self.client.get_ticker()
            market_tickers = [DailyMarketTicker.from_binance_ticker(t) for t in tickers]

            if quote_asset:
                market_tickers = [t for t in market_tickers if t.symbol.endswith(quote_asset)]

            return sorted(
                market_tickers,
                key=lambda x: getattr(x, sort_by.value),
                reverse=True,
            )[:limit]

        except Exception as e:
            logger.error(f"[red]Error getting top coins: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get top coins: {e}") from e

    def get_market_summary(self, interval: Freq = Freq.d1) -> dict[str, Any]:
        """è·å–å¸‚åœºæ¦‚è§ˆã€‚

        Args:
            interval: æ—¶é—´é—´éš”

        Returns:
            dict[str, Any]: å¸‚åœºæ¦‚è§ˆ
        """
        try:
            summary: dict[str, Any] = {"snapshot_time": datetime.now(), "data": {}}
            tickers_result = self.get_symbol_ticker()
            if isinstance(tickers_result, list):
                tickers = [ticker.to_dict() for ticker in tickers_result]
            else:
                tickers = [tickers_result.to_dict()]
            summary["data"] = tickers

            return summary

        except Exception as e:
            logger.error(f"[red]Error getting market summary: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get market summary: {e}") from e

    def get_historical_klines(
        self,
        symbol: str,
        start_time: str | datetime,
        end_time: str | datetime | None = None,
        interval: Freq = Freq.h1,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.SPOT,
    ) -> list[KlineMarketTicker]:
        """è·å–å†å²è¡Œæƒ…æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´ï¼Œå¦‚æœä¸ºNoneåˆ™ä¸ºå½“å‰æ—¶é—´
            interval: æ—¶é—´é—´éš”
            klines_type: Kçº¿ç±»å‹ï¼ˆç°è´§æˆ–æœŸè´§ï¼‰

        Returns:
            list[KlineMarketTicker]: å†å²è¡Œæƒ…æ•°æ®
        """
        try:
            # å¤„ç†æ—¶é—´æ ¼å¼
            if isinstance(start_time, str):
                start_time = datetime.fromisoformat(start_time)
            if end_time is None:
                end_time = datetime.now()
            elif isinstance(end_time, str):
                end_time = datetime.fromisoformat(end_time)

            # è½¬æ¢ä¸ºæ—¶é—´æˆ³
            start_ts = self._date_to_timestamp_start(start_time.strftime("%Y-%m-%d"))
            end_ts = self._date_to_timestamp_end(end_time.strftime("%Y-%m-%d"))

            logger.info(f"è·å– {symbol} çš„å†å²æ•°æ® ({interval.value})")

            # æ ¹æ®klines_typeé€‰æ‹©API
            if klines_type == HistoricalKlinesType.FUTURES:
                klines = self.client.futures_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )
            else:  # SPOT
                klines = self.client.get_klines(
                    symbol=symbol,
                    interval=interval.value,
                    startTime=start_ts,
                    endTime=end_ts,
                    limit=1500,
                )

            data = list(klines)
            if not data:
                logger.warning(f"æœªæ‰¾åˆ°äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…çš„æ•°æ®")
                return []

            # è½¬æ¢ä¸ºKlineMarketTickerå¯¹è±¡
            return [
                KlineMarketTicker(
                    symbol=symbol,
                    last_price=Decimal(str(kline[4])),  # æ”¶ç›˜ä»·ä½œä¸ºæœ€æ–°ä»·æ ¼
                    open_price=Decimal(str(kline[1])),
                    high_price=Decimal(str(kline[2])),
                    low_price=Decimal(str(kline[3])),
                    volume=Decimal(str(kline[5])),
                    close_time=kline[6],
                )
                for kline in data
            ]

        except Exception as e:
            logger.error(f"[red]Error getting historical data for {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"Failed to get historical data: {e}") from e

    def _fetch_symbol_data(
        self,
        symbol: str,
        start_ts: str,
        end_ts: str,
        interval: Freq,
        klines_type: HistoricalKlinesType = HistoricalKlinesType.FUTURES,
        retry_config: Optional[RetryConfig] = None,
    ) -> list[PerpetualMarketTicker]:
        """è·å–å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ® (å¢å¼ºç‰ˆ).

        Args:
            symbol: äº¤æ˜“å¯¹åç§°
            start_ts: å¼€å§‹æ—¶é—´æˆ³ (æ¯«ç§’)
            end_ts: ç»“æŸæ—¶é—´æˆ³ (æ¯«ç§’)
            interval: æ—¶é—´é—´éš”
            klines_type: è¡Œæƒ…ç±»å‹
            retry_config: é‡è¯•é…ç½®
        """
        if retry_config is None:
            retry_config = RetryConfig()

        backoff = ExponentialBackoff(retry_config)
        error_handler = EnhancedErrorHandler()

        while True:
            try:
                # æ•°æ®é¢„æ£€æŸ¥
                if start_ts and end_ts:
                    start_date = datetime.fromtimestamp(int(start_ts) / 1000).strftime("%Y-%m-%d")
                    logger.debug(f"è·å– {symbol} æ•°æ®: {start_date} ({start_ts} - {end_ts})")

                # é¢‘ç‡é™åˆ¶æ§åˆ¶ - åœ¨APIè°ƒç”¨å‰ç­‰å¾…
                self.rate_limit_manager.wait_before_request()

                # APIè°ƒç”¨
                klines = self.client.get_historical_klines_generator(
                    symbol=symbol,
                    interval=interval.value,
                    start_str=start_ts,
                    end_str=end_ts,
                    limit=1500,
                    klines_type=HistoricalKlinesType.to_binance(klines_type),
                )

                data = list(klines)
                if not data:
                    logger.debug(f"äº¤æ˜“å¯¹ {symbol} åœ¨æŒ‡å®šæ—¶é—´æ®µå†…æ— æ•°æ®")
                    self.rate_limit_manager.handle_success()
                    return []

                # æ•°æ®è´¨é‡æ£€æŸ¥
                valid_data = self._validate_kline_data(data, symbol)

                # è½¬æ¢ä¸ºå¯¹è±¡
                result = [
                    PerpetualMarketTicker(
                        symbol=symbol,
                        open_time=kline[0],
                        raw_data=kline,
                    )
                    for kline in valid_data
                ]

                logger.debug(f"æˆåŠŸè·å– {symbol}: {len(result)} æ¡è®°å½•")
                self.rate_limit_manager.handle_success()
                return result

            except Exception as e:
                severity = error_handler.classify_error(e)

                # ç‰¹æ®Šå¤„ç†é¢‘ç‡é™åˆ¶é”™è¯¯
                if error_handler.is_rate_limit_error(e):
                    wait_time = self.rate_limit_manager.handle_rate_limit_error()
                    logger.warning(f"ğŸš« é¢‘ç‡é™åˆ¶ - {symbol}ï¼Œç­‰å¾… {wait_time}ç§’åé‡è¯•")
                    time.sleep(wait_time)
                    # é‡ç½®é€€é¿è®¡æ•°ï¼Œå› ä¸ºè¿™ä¸æ˜¯çœŸæ­£çš„"é”™è¯¯"
                    backoff.reset()
                    continue

                # å¤„ç†ä¸å¯é‡è¯•çš„é”™è¯¯
                if severity == ErrorSeverity.CRITICAL:
                    logger.error(f"âŒ è‡´å‘½é”™è¯¯ - {symbol}: {e}")
                    logger.error(f"å»ºè®®: {error_handler.get_recommended_action(e)}")
                    raise e

                if "Invalid symbol" in str(e):
                    logger.warning(f"âš ï¸ æ— æ•ˆäº¤æ˜“å¯¹: {symbol}")
                    raise InvalidSymbolError(f"æ— æ•ˆçš„äº¤æ˜“å¯¹: {symbol}") from e

                # åˆ¤æ–­æ˜¯å¦é‡è¯•
                if not error_handler.should_retry(e, backoff.attempt, retry_config.max_retries):
                    logger.error(f"âŒ é‡è¯•å¤±è´¥ - {symbol}: {e}")
                    if severity == ErrorSeverity.LOW:
                        # å¯¹äºä½ä¸¥é‡æ€§é”™è¯¯ï¼Œè¿”å›ç©ºç»“æœè€Œä¸æŠ›å‡ºå¼‚å¸¸
                        return []
                    raise MarketDataFetchError(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®å¤±è´¥: {e}") from e

                # æ‰§è¡Œé‡è¯•
                logger.warning(f"ğŸ”„ é‡è¯• {backoff.attempt + 1}/{retry_config.max_retries} - {symbol}: {e}")
                logger.info(f"ğŸ’¡ å»ºè®®: {error_handler.get_recommended_action(e)}")

                try:
                    backoff.wait()
                except Exception:
                    logger.error(f"âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° - {symbol}")
                    raise MarketDataFetchError(f"è·å–äº¤æ˜“å¯¹ {symbol} æ•°æ®å¤±è´¥: è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°") from e

    def _validate_kline_data(self, data: List, symbol: str) -> List:
        """éªŒè¯Kçº¿æ•°æ®è´¨é‡"""
        if not data:
            return data

        valid_data = []
        issues = []

        for i, kline in enumerate(data):
            try:
                # æ£€æŸ¥æ•°æ®ç»“æ„
                if len(kline) < 8:
                    issues.append(f"è®°å½•{i}: æ•°æ®å­—æ®µä¸è¶³")
                    continue

                # æ£€æŸ¥ä»·æ ¼æ•°æ®æœ‰æ•ˆæ€§
                open_price = float(kline[1])
                high_price = float(kline[2])
                low_price = float(kline[3])
                close_price = float(kline[4])
                volume = float(kline[5])

                # åŸºç¡€é€»è¾‘æ£€æŸ¥
                if high_price < max(open_price, close_price, low_price):
                    issues.append(f"è®°å½•{i}: æœ€é«˜ä»·å¼‚å¸¸")
                    continue

                if low_price > min(open_price, close_price, high_price):
                    issues.append(f"è®°å½•{i}: æœ€ä½ä»·å¼‚å¸¸")
                    continue

                if volume < 0:
                    issues.append(f"è®°å½•{i}: æˆäº¤é‡ä¸ºè´Ÿ")
                    continue

                valid_data.append(kline)

            except (ValueError, IndexError) as e:
                issues.append(f"è®°å½•{i}: æ•°æ®æ ¼å¼é”™è¯¯ - {e}")
                continue

        if issues:
            issue_count = len(issues)
            total_count = len(data)
            if issue_count > total_count * 0.1:  # è¶…è¿‡10%çš„æ•°æ®æœ‰é—®é¢˜
                logger.warning(f"âš ï¸ {symbol} æ•°æ®è´¨é‡é—®é¢˜: {issue_count}/{total_count} æ¡è®°å½•å¼‚å¸¸")
                for issue in issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                    logger.debug(f"   - {issue}")
                if len(issues) > 5:
                    logger.debug(f"   - ... è¿˜æœ‰ {len(issues) - 5} ä¸ªé—®é¢˜")

        return valid_data

    def _create_integrity_report(
        self,
        symbols: List[str],
        successful_symbols: List[str],
        failed_symbols: List[str],
        missing_periods: List[Dict[str, str]],
        start_time: str,
        end_time: str,
        interval: Freq,
        db_file_path: Path,
    ) -> IntegrityReport:
        """åˆ›å»ºæ•°æ®å®Œæ•´æ€§æŠ¥å‘Š"""
        try:
            if not self.db:
                raise ValueError("æ•°æ®åº“è¿æ¥æœªåˆå§‹åŒ–")

            logger.info("ğŸ” æ‰§è¡Œæ•°æ®å®Œæ•´æ€§æ£€æŸ¥...")

            # è®¡ç®—åŸºç¡€æŒ‡æ ‡
            total_symbols = len(symbols)
            success_count = len(successful_symbols)
            basic_quality_score = success_count / total_symbols if total_symbols > 0 else 0

            recommendations = []
            detailed_issues = []

            # æ£€æŸ¥æˆåŠŸä¸‹è½½çš„æ•°æ®è´¨é‡ï¼ˆå¯¹äºæµ‹è¯•æ•°æ®é‡‡ç”¨å®½æ¾ç­–ç•¥ï¼‰
            quality_issues = 0
            sample_symbols = successful_symbols[: min(5, len(successful_symbols))]  # å‡å°‘æŠ½æ ·æ•°é‡

            # å¦‚æœæ˜¯å•æ—¥æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡å®Œæ•´æ€§æ£€æŸ¥
            if start_time == end_time:
                logger.debug("æ£€æµ‹åˆ°å•æ—¥æµ‹è¯•æ•°æ®ï¼Œè·³è¿‡è¯¦ç»†å®Œæ•´æ€§æ£€æŸ¥")
                sample_symbols = []

            for symbol in sample_symbols:
                try:
                    # è¯»å–æ•°æ®è¿›è¡Œè´¨é‡æ£€æŸ¥
                    # ç¡®ä¿æ—¶é—´æ ¼å¼æ­£ç¡®
                    check_start_time = pd.to_datetime(start_time).strftime("%Y-%m-%d")
                    check_end_time = pd.to_datetime(end_time).strftime("%Y-%m-%d")

                    df = self.db.read_data(
                        start_time=check_start_time,
                        end_time=check_end_time,
                        freq=interval,
                        symbols=[symbol],
                        raise_on_empty=False,
                    )

                    if df is not None and not df.empty:
                        # æ£€æŸ¥æ•°æ®è¿ç»­æ€§
                        symbol_data = (
                            df.loc[symbol] if symbol in df.index.get_level_values("symbol") else pd.DataFrame()
                        )
                        if not symbol_data.empty:
                            # è®¡ç®—æœŸæœ›çš„æ•°æ®ç‚¹æ•°é‡ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
                            time_diff = pd.to_datetime(check_end_time) - pd.to_datetime(check_start_time)
                            expected_points = self._calculate_expected_data_points(time_diff, interval)
                            actual_points = len(symbol_data)

                            completeness = actual_points / expected_points if expected_points > 0 else 0
                            if completeness < 0.8:  # å°‘äº80%è®¤ä¸ºæœ‰é—®é¢˜
                                quality_issues += 1
                                detailed_issues.append(
                                    f"{symbol}: æ•°æ®å®Œæ•´æ€§{completeness:.1%} ({actual_points}/{expected_points})"
                                )
                    else:
                        quality_issues += 1
                        detailed_issues.append(f"{symbol}: æ— æ³•è¯»å–å·²ä¸‹è½½çš„æ•°æ®")

                except Exception as e:
                    quality_issues += 1
                    detailed_issues.append(f"{symbol}: æ£€æŸ¥å¤±è´¥ - {e}")

            # è°ƒæ•´è´¨é‡åˆ†æ•°
            if successful_symbols:
                sample_size = min(10, len(successful_symbols))
                quality_penalty = (quality_issues / sample_size) * 0.3  # æœ€å¤šå‡å°‘30%åˆ†æ•°
                final_quality_score = max(0, basic_quality_score - quality_penalty)
            else:
                final_quality_score = 0

            # ç”Ÿæˆå»ºè®®
            if final_quality_score < 0.5:
                recommendations.append("ğŸš¨ æ•°æ®è´¨é‡ä¸¥é‡ä¸è¶³ï¼Œå»ºè®®é‡æ–°ä¸‹è½½")
            elif final_quality_score < 0.8:
                recommendations.append("âš ï¸ æ•°æ®è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®æ£€æŸ¥å¤±è´¥çš„äº¤æ˜“å¯¹")
            else:
                recommendations.append("âœ… æ•°æ®è´¨é‡è‰¯å¥½")

            if failed_symbols:
                recommendations.append(f"ğŸ“ {len(failed_symbols)}ä¸ªäº¤æ˜“å¯¹ä¸‹è½½å¤±è´¥ï¼Œå»ºè®®å•ç‹¬é‡è¯•")
                if len(failed_symbols) <= 5:
                    recommendations.append(f"å¤±è´¥äº¤æ˜“å¯¹: {', '.join(failed_symbols)}")

            if quality_issues > 0:
                recommendations.append(f"âš ï¸ å‘ç°{quality_issues}ä¸ªæ•°æ®è´¨é‡é—®é¢˜")
                recommendations.extend(detailed_issues[:3])  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜

            # ç½‘ç»œå’ŒAPIå»ºè®®
            if len(failed_symbols) > total_symbols * 0.3:
                recommendations.append("ğŸŒ å¤±è´¥ç‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé™åˆ¶")

            logger.info(f"âœ… å®Œæ•´æ€§æ£€æŸ¥å®Œæˆ: è´¨é‡åˆ†æ•° {final_quality_score:.1%}")

            return IntegrityReport(
                total_symbols=total_symbols,
                successful_symbols=success_count,
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=final_quality_score,
                recommendations=recommendations,
            )

        except Exception as e:
            logger.warning(f"âš ï¸ å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}")
            # è¿”å›åŸºç¡€æŠ¥å‘Š
            return IntegrityReport(
                total_symbols=len(symbols),
                successful_symbols=len(successful_symbols),
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=(len(successful_symbols) / len(symbols) if symbols else 0),
                recommendations=[f"å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥: {e}", "å»ºè®®æ‰‹åŠ¨éªŒè¯æ•°æ®è´¨é‡"],
            )

    def _calculate_expected_data_points(self, time_diff: timedelta, interval: Freq) -> int:
        """è®¡ç®—æœŸæœ›çš„æ•°æ®ç‚¹æ•°é‡"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºæ—¶é—´å·®å’Œé¢‘ç‡è®¡ç®—æœŸæœ›æ•°æ®ç‚¹
        total_minutes = time_diff.total_seconds() / 60

        interval_minutes = {
            Freq.m1: 1,
            Freq.m3: 3,
            Freq.m5: 5,
            Freq.m15: 15,
            Freq.m30: 30,
            Freq.h1: 60,
            Freq.h4: 240,
            Freq.d1: 1440,
        }.get(interval, 1)

        # ç¡®ä¿è‡³å°‘è¿”å›1ä¸ªæ•°æ®ç‚¹ï¼Œé¿å…é™¤é›¶é”™è¯¯
        expected_points = int(total_minutes / interval_minutes)
        return max(1, expected_points)

    def get_perpetual_data(
        self,
        symbols: list[str],
        start_time: str,
        db_path: Path | str,
        end_time: str | None = None,
        interval: Freq = Freq.h1,
        max_workers: int = 5,
        max_retries: int = 3,
        progress: Progress | None = None,
        request_delay: float = 0.5,
        # é¢å¤–å‚æ•°ï¼Œä¿æŒå‘åå…¼å®¹
        retry_config: Optional[RetryConfig] = None,
        enable_integrity_check: bool = True,
    ) -> IntegrityReport:
        """è·å–æ°¸ç»­åˆçº¦æ•°æ®å¹¶å­˜å‚¨ (å¢å¼ºç‰ˆ).

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´ (YYYY-MM-DD)
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®šï¼Œå¦‚: /path/to/market.db)
            end_time: ç»“æŸæ—¶é—´ (YYYY-MM-DD)
            interval: æ—¶é—´é—´éš”
            max_workers: æœ€å¤§çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_config: é‡è¯•é…ç½®
            progress: è¿›åº¦æ˜¾ç¤ºå™¨
            enable_integrity_check: æ˜¯å¦å¯ç”¨å®Œæ•´æ€§æ£€æŸ¥
            request_delay: æ¯æ¬¡è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.5ç§’

        Returns:
            IntegrityReport: æ•°æ®å®Œæ•´æ€§æŠ¥å‘Š
        """
        if retry_config is None:
            retry_config = RetryConfig(max_retries=max_retries)

        # åˆå§‹åŒ–ç»“æœç»Ÿè®¡
        successful_symbols = []
        failed_symbols = []
        missing_periods = []

        try:
            if not symbols:
                raise ValueError("Symbols list cannot be empty")

            # éªŒè¯å¹¶å‡†å¤‡æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)
            end_time = end_time or datetime.now().strftime("%Y-%m-%d")

            # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³
            start_ts = self._date_to_timestamp_start(start_time)
            end_ts = self._date_to_timestamp_end(end_time)

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            if self.db is None:
                self.db = MarketDB(str(db_file_path))

            # é‡æ–°åˆå§‹åŒ–é¢‘ç‡é™åˆ¶ç®¡ç†å™¨ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„åŸºç¡€å»¶è¿Ÿ
            self.rate_limit_manager = RateLimitManager(base_delay=request_delay)

            logger.info(f"ğŸš€ å¼€å§‹ä¸‹è½½ {len(symbols)} ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®")
            logger.info(f"ğŸ“… æ—¶é—´èŒƒå›´: {start_time} åˆ° {end_time}")
            logger.info(f"âš™ï¸ é‡è¯•é…ç½®: æœ€å¤§{retry_config.max_retries}æ¬¡, åŸºç¡€å»¶è¿Ÿ{retry_config.base_delay}ç§’")
            logger.info(f"â±ï¸ æ™ºèƒ½é¢‘ç‡æ§åˆ¶: åŸºç¡€å»¶è¿Ÿ{request_delay}ç§’ï¼ŒåŠ¨æ€è°ƒæ•´")

            # åˆ›å»ºè¿›åº¦è·Ÿè¸ª
            if progress is None:
                progress = Progress(
                    SpinnerColumn(),
                    TextColumn("[progress.description]{task.description}"),
                    BarColumn(),
                    TimeElapsedColumn(),
                )

            def process_symbol(symbol: str) -> Dict[str, Any]:
                """å¤„ç†å•ä¸ªäº¤æ˜“å¯¹çš„æ•°æ®è·å– (å¢å¼ºç‰ˆ)"""
                result = {
                    "symbol": symbol,
                    "success": False,
                    "records": 0,
                    "error": None,
                }

                try:
                    data = self._fetch_symbol_data(
                        symbol=symbol,
                        start_ts=start_ts,
                        end_ts=end_ts,
                        interval=interval,
                        retry_config=retry_config,
                    )

                    if data:
                        if self.db is None:
                            raise MarketDataFetchError("Database is not initialized")

                        self.db.store_data(data, interval)
                        result.update(
                            {
                                "success": True,
                                "records": len(data),
                                "time_range": f"{data[0].open_time} - {data[-1].open_time}",
                            }
                        )
                        logger.debug(f"âœ… {symbol}: {len(data)} æ¡è®°å½•")
                        successful_symbols.append(symbol)
                    else:
                        result["error"] = "æ— æ•°æ®"
                        logger.debug(f"âš ï¸ {symbol}: æ— æ•°æ®")
                        missing_periods.append(
                            {
                                "symbol": symbol,
                                "period": f"{start_time} - {end_time}",
                                "reason": "no_data",
                            }
                        )

                except InvalidSymbolError as e:
                    result["error"] = f"æ— æ•ˆäº¤æ˜“å¯¹: {e}"
                    logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆäº¤æ˜“å¯¹ {symbol}")
                    failed_symbols.append(symbol)

                except Exception as e:
                    result["error"] = str(e)
                    logger.error(f"âŒ {symbol} å¤±è´¥: {e}")
                    failed_symbols.append(symbol)
                    missing_periods.append(
                        {
                            "symbol": symbol,
                            "period": f"{start_time} - {end_time}",
                            "reason": str(e),
                        }
                    )

                return result

            # æ‰§è¡Œå¹¶è¡Œä¸‹è½½
            results = []
            with progress if progress is not None else nullcontext():
                overall_task = progress.add_task("[cyan]ä¸‹è½½äº¤æ˜“å¯¹æ•°æ®", total=len(symbols)) if progress else None

                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    futures = [executor.submit(process_symbol, symbol) for symbol in symbols]

                    for future in as_completed(futures):
                        try:
                            result = future.result()
                            results.append(result)

                            if progress and overall_task is not None:
                                progress.update(overall_task, advance=1)

                        except Exception as e:
                            logger.error(f"âŒ å¤„ç†å¼‚å¸¸: {e}")

            # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
            total_records = sum(r.get("records", 0) for r in results)
            success_rate = len(successful_symbols) / len(symbols) if symbols else 0

            logger.info("ğŸ“Š ä¸‹è½½å®Œæˆç»Ÿè®¡:")
            logger.info(f"   âœ… æˆåŠŸ: {len(successful_symbols)}/{len(symbols)} ({success_rate:.1%})")
            logger.info(f"   âŒ å¤±è´¥: {len(failed_symbols)} ä¸ª")
            logger.info(f"   ğŸ“ˆ æ€»è®°å½•æ•°: {total_records:,} æ¡")
            logger.info(f"   ğŸ’¾ æ•°æ®åº“: {db_file_path}")

            # æ‰§è¡Œå®Œæ•´æ€§æ£€æŸ¥
            if enable_integrity_check and self.db:
                integrity_report = self._create_integrity_report(
                    symbols=symbols,
                    successful_symbols=successful_symbols,
                    failed_symbols=failed_symbols,
                    missing_periods=missing_periods,
                    start_time=start_time,
                    end_time=end_time,
                    interval=interval,
                    db_file_path=db_file_path,
                )
            else:
                # ç”ŸæˆåŸºç¡€æŠ¥å‘Š
                data_quality_score = len(successful_symbols) / len(symbols) if symbols else 0
                recommendations = []
                if data_quality_score < 0.8:
                    recommendations.append("æ•°æ®æˆåŠŸç‡è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œå’ŒAPIé…ç½®")
                if failed_symbols:
                    recommendations.append(f"æœ‰{len(failed_symbols)}ä¸ªäº¤æ˜“å¯¹ä¸‹è½½å¤±è´¥ï¼Œå»ºè®®å•ç‹¬é‡è¯•")

                integrity_report = IntegrityReport(
                    total_symbols=len(symbols),
                    successful_symbols=len(successful_symbols),
                    failed_symbols=failed_symbols,
                    missing_periods=missing_periods,
                    data_quality_score=data_quality_score,
                    recommendations=recommendations,
                )

            return integrity_report

        except Exception as e:
            logger.error(f"âŒ æ•°æ®ä¸‹è½½å¤±è´¥: {e}")
            # å³ä½¿å¤±è´¥ä¹Ÿè¦è¿”å›æŠ¥å‘Š
            return IntegrityReport(
                total_symbols=len(symbols),
                successful_symbols=len(successful_symbols),
                failed_symbols=failed_symbols,
                missing_periods=missing_periods,
                data_quality_score=0.0,
                recommendations=[f"ä¸‹è½½å¤±è´¥: {e}", "æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®"],
            )

    def define_universe(
        self,
        start_date: str,
        end_date: str,
        t1_months: int,
        t2_months: int,
        t3_months: int,
        output_path: Path | str,
        top_k: int | None = None,
        top_ratio: float | None = None,
        description: str | None = None,
        delay_days: int = 7,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> UniverseDefinition:
        """å®šä¹‰universeå¹¶ä¿å­˜åˆ°æ–‡ä»¶.

        Args:
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD)
            t1_months: T1æ—¶é—´çª—å£ï¼ˆæœˆï¼‰ï¼Œç”¨äºè®¡ç®—mean daily amount
            t2_months: T2æ»šåŠ¨é¢‘ç‡ï¼ˆæœˆï¼‰ï¼Œuniverseé‡æ–°é€‰æ‹©çš„é¢‘ç‡
            t3_months: T3åˆçº¦æœ€å°åˆ›å»ºæ—¶é—´ï¼ˆæœˆï¼‰ï¼Œç”¨äºç­›é™¤æ–°åˆçº¦
            output_path: universeè¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®š)
            top_k: é€‰å–çš„topåˆçº¦æ•°é‡ (ä¸ top_ratio äºŒé€‰ä¸€)
            top_ratio: é€‰å–çš„topåˆçº¦æ¯”ç‡ (ä¸ top_k äºŒé€‰ä¸€)
            description: æè¿°ä¿¡æ¯
            delay_days: åœ¨é‡æ–°å¹³è¡¡æ—¥æœŸå‰é¢å¤–å¾€å‰æ¨çš„å¤©æ•°ï¼Œé»˜è®¤7å¤©
            api_delay_seconds: æ¯ä¸ªAPIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼Œé»˜è®¤1.0ç§’
            batch_delay_seconds: æ¯æ‰¹æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°ï¼Œé»˜è®¤3.0ç§’
            batch_size: æ¯æ‰¹æ¬¡çš„è¯·æ±‚æ•°é‡ï¼Œé»˜è®¤5ä¸ª
            quote_asset: åŸºå‡†èµ„äº§ï¼Œé»˜è®¤ä¸ºUSDTï¼Œåªç­›é€‰ä»¥è¯¥èµ„äº§ç»“å°¾çš„äº¤æ˜“å¯¹

        Returns:
            UniverseDefinition: å®šä¹‰çš„universe
        """
        try:
            # éªŒè¯å¹¶å‡†å¤‡è¾“å‡ºè·¯å¾„
            output_path_obj = self._validate_and_prepare_path(
                output_path,
                is_file=True,
                file_name=(
                    f"universe_{start_date}_{end_date}_{t1_months}_{t2_months}_{t3_months}_{top_k or top_ratio}.json"
                ),
            )

            # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼
            start_date = self._standardize_date_format(start_date)
            end_date = self._standardize_date_format(end_date)

            # åˆ›å»ºé…ç½®
            config = UniverseConfig(
                start_date=start_date,
                end_date=end_date,
                t1_months=t1_months,
                t2_months=t2_months,
                t3_months=t3_months,
                delay_days=delay_days,
                quote_asset=quote_asset,
                top_k=top_k,
                top_ratio=top_ratio,
            )

            logger.info(f"å¼€å§‹å®šä¹‰universe: {start_date} åˆ° {end_date}")
            log_selection_criteria = f"Top-K={top_k}" if top_k else f"Top-Ratio={top_ratio}"
            logger.info(f"å‚æ•°: T1={t1_months}æœˆ, T2={t2_months}æœˆ, T3={t3_months}æœˆ, {log_selection_criteria}")

            # ç”Ÿæˆé‡æ–°é€‰æ‹©æ—¥æœŸåºåˆ— (æ¯T2ä¸ªæœˆ)
            # ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¡¨ç¤ºuniverseé‡æ–°é€‰æ‹©çš„æ—¶é—´ç‚¹
            rebalance_dates = self._generate_rebalance_dates(start_date, end_date, t2_months)

            logger.info("é‡å¹³è¡¡è®¡åˆ’:")
            logger.info(f"  - æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
            logger.info(f"  - é‡å¹³è¡¡é—´éš”: æ¯{t2_months}ä¸ªæœˆ")
            logger.info(f"  - æ•°æ®å»¶è¿Ÿ: {delay_days}å¤©")
            logger.info(f"  - T1æ•°æ®çª—å£: {t1_months}ä¸ªæœˆ")
            logger.info(f"  - é‡å¹³è¡¡æ—¥æœŸ: {rebalance_dates}")

            if not rebalance_dates:
                raise ValueError("æ— æ³•ç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¯·æ£€æŸ¥æ—¶é—´èŒƒå›´å’ŒT2å‚æ•°")

            # æ”¶é›†æ‰€æœ‰å‘¨æœŸçš„snapshots
            all_snapshots = []

            # åœ¨æ¯ä¸ªé‡æ–°é€‰æ‹©æ—¥æœŸè®¡ç®—universe
            for i, rebalance_date in enumerate(rebalance_dates):
                logger.info(f"å¤„ç†æ—¥æœŸ {i + 1}/{len(rebalance_dates)}: {rebalance_date}")

                # è®¡ç®—åŸºå‡†æ—¥æœŸï¼ˆé‡æ–°å¹³è¡¡æ—¥æœŸå‰delay_dayså¤©ï¼‰
                base_date = pd.to_datetime(rebalance_date) - timedelta(days=delay_days)
                calculated_t1_end = base_date.strftime("%Y-%m-%d")

                # è®¡ç®—T1å›çœ‹æœŸé—´çš„å¼€å§‹æ—¥æœŸï¼ˆä»base_dateå¾€å‰æ¨T1ä¸ªæœˆï¼‰
                calculated_t1_start = self._subtract_months(calculated_t1_end, t1_months)

                logger.info(
                    f"å‘¨æœŸ {i + 1}: åŸºå‡†æ—¥æœŸ={calculated_t1_end} (é‡æ–°å¹³è¡¡æ—¥æœŸå‰{delay_days}å¤©), "
                    f"T1æ•°æ®æœŸé—´={calculated_t1_start} åˆ° {calculated_t1_end}"
                )

                # è·å–ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹å’Œå®ƒä»¬çš„mean daily amount
                universe_symbols, mean_amounts = self._calculate_universe_for_date(
                    calculated_t1_start,
                    calculated_t1_end,
                    t3_months=t3_months,
                    top_k=top_k,
                    top_ratio=top_ratio,
                    api_delay_seconds=api_delay_seconds,
                    batch_delay_seconds=batch_delay_seconds,
                    batch_size=batch_size,
                    quote_asset=quote_asset,
                )

                # åˆ›å»ºè¯¥å‘¨æœŸçš„snapshot
                snapshot = UniverseSnapshot.create_with_dates_and_timestamps(
                    usage_t1_start=rebalance_date,  # å®é™…ä½¿ç”¨å¼€å§‹æ—¥æœŸ
                    usage_t1_end=min(
                        end_date,
                        (pd.to_datetime(rebalance_date) + pd.DateOffset(months=t1_months)).strftime("%Y-%m-%d"),
                    ),  # å®é™…ä½¿ç”¨ç»“æŸæ—¥æœŸ
                    calculated_t1_start=calculated_t1_start,  # è®¡ç®—å‘¨æœŸå¼€å§‹æ—¥æœŸ
                    calculated_t1_end=calculated_t1_end,  # è®¡ç®—å‘¨æœŸç»“æŸæ—¥æœŸï¼ˆåŸºå‡†æ—¥æœŸï¼‰
                    symbols=universe_symbols,
                    mean_daily_amounts=mean_amounts,
                    metadata={
                        "calculated_t1_start": calculated_t1_start,
                        "calculated_t1_end": calculated_t1_end,
                        "delay_days": delay_days,
                        "quote_asset": quote_asset,
                        "selected_symbols_count": len(universe_symbols),
                    },
                )

                all_snapshots.append(snapshot)

                logger.info(f"âœ… æ—¥æœŸ {rebalance_date}: é€‰æ‹©äº† {len(universe_symbols)} ä¸ªäº¤æ˜“å¯¹")

            # åˆ›å»ºå®Œæ•´çš„universeå®šä¹‰
            universe_def = UniverseDefinition(
                config=config,
                snapshots=all_snapshots,
                creation_time=datetime.now(),
                description=description,
            )

            # ä¿å­˜æ±‡æ€»çš„universeå®šä¹‰
            universe_def.save_to_file(output_path_obj)

            logger.info("ğŸ‰ Universeå®šä¹‰å®Œæˆï¼")
            logger.info(f"ğŸ“ åŒ…å« {len(all_snapshots)} ä¸ªé‡æ–°å¹³è¡¡å‘¨æœŸ")
            logger.info(f"ğŸ“‹ æ±‡æ€»æ–‡ä»¶: {output_path_obj}")

            return universe_def

        except Exception as e:
            logger.error(f"[red]å®šä¹‰universeå¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"å®šä¹‰universeå¤±è´¥: {e}") from e

    def _standardize_date_format(self, date_str: str) -> str:
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ä¸º YYYY-MM-DDã€‚"""
        if len(date_str) == 8:  # YYYYMMDD
            return f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        return date_str

    def _generate_rebalance_dates(self, start_date: str, end_date: str, t2_months: int) -> list[str]:
        """ç”Ÿæˆé‡æ–°é€‰æ‹©universeçš„æ—¥æœŸåºåˆ—ã€‚

        ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸï¼Œè¿™äº›æ—¥æœŸè¡¨ç¤ºuniverseé‡æ–°é€‰æ‹©çš„æ—¶é—´ç‚¹ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            t2_months: é‡æ–°å¹³è¡¡é—´éš”ï¼ˆæœˆï¼‰

        Returns:
            list[str]: é‡å¹³è¡¡æ—¥æœŸåˆ—è¡¨
        """
        dates = []
        start_date_obj = pd.to_datetime(start_date)
        end_date_obj = pd.to_datetime(end_date)

        # ä»èµ·å§‹æ—¥æœŸå¼€å§‹ï¼Œæ¯éš”T2ä¸ªæœˆç”Ÿæˆé‡å¹³è¡¡æ—¥æœŸ
        current_date = start_date_obj

        while current_date <= end_date_obj:
            dates.append(current_date.strftime("%Y-%m-%d"))
            current_date = current_date + pd.DateOffset(months=t2_months)

        return dates

    def _subtract_months(self, date_str: str, months: int) -> str:
        """ä»æ—¥æœŸå‡å»æŒ‡å®šæœˆæ•°ã€‚"""
        date_obj = pd.to_datetime(date_str)
        # ä½¿ç”¨pandasçš„DateOffsetæ¥æ­£ç¡®å¤„ç†æœˆä»½è¾¹ç•Œé—®é¢˜
        result_date = date_obj - pd.DateOffset(months=months)
        return str(result_date.strftime("%Y-%m-%d"))

    def _get_available_symbols_for_period(self, start_date: str, end_date: str, quote_asset: str = "USDT") -> list[str]:
        """è·å–æŒ‡å®šæ—¶é—´æ®µå†…å®é™…å¯ç”¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹ã€‚

        Args:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            quote_asset: åŸºå‡†èµ„äº§ï¼Œç”¨äºç­›é€‰äº¤æ˜“å¯¹

        Returns:
            list[str]: åœ¨è¯¥æ—¶é—´æ®µå†…æœ‰æ•°æ®çš„äº¤æ˜“å¯¹åˆ—è¡¨
        """
        try:
            # å…ˆè·å–å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦ä½œä¸ºå€™é€‰ï¼ˆç­›é€‰æŒ‡å®šçš„åŸºå‡†èµ„äº§ï¼‰
            candidate_symbols = self.get_perpetual_symbols(only_trading=True, quote_asset=quote_asset)
            logger.info(
                f"æ£€æŸ¥ {len(candidate_symbols)} ä¸ª{quote_asset}å€™é€‰äº¤æ˜“å¯¹åœ¨ {start_date} åˆ° {end_date} æœŸé—´çš„å¯ç”¨æ€§..."
            )

            available_symbols = []
            batch_size = 50
            for i in range(0, len(candidate_symbols), batch_size):
                batch = candidate_symbols[i : i + batch_size]
                for symbol in batch:
                    # æ£€æŸ¥åœ¨èµ·å§‹æ—¥æœŸæ˜¯å¦æœ‰æ•°æ®
                    if self.check_symbol_exists_on_date(symbol, start_date):
                        available_symbols.append(symbol)

                # æ˜¾ç¤ºè¿›åº¦
                processed = min(i + batch_size, len(candidate_symbols))
                logger.info(
                    f"å·²æ£€æŸ¥ {processed}/{len(candidate_symbols)} ä¸ªäº¤æ˜“å¯¹ï¼Œæ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨äº¤æ˜“å¯¹"
                )

                # é¿å…APIé¢‘ç‡é™åˆ¶
                time.sleep(0.1)

            logger.info(
                f"åœ¨ {start_date} åˆ° {end_date} æœŸé—´æ‰¾åˆ° {len(available_symbols)} ä¸ªå¯ç”¨çš„{quote_asset}æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹"
            )
            return available_symbols

        except Exception as e:
            logger.warning(f"è·å–å¯ç”¨äº¤æ˜“å¯¹æ—¶å‡ºé”™: {e}")
            # å¦‚æœAPIæ£€æŸ¥å¤±è´¥ï¼Œè¿”å›å½“å‰æ‰€æœ‰æ°¸ç»­åˆçº¦
            return self.get_perpetual_symbols(only_trading=True, quote_asset=quote_asset)

    def _calculate_universe_for_date(
        self,
        calculated_t1_start: str,
        calculated_t1_end: str,
        t3_months: int,
        top_k: int | None = None,
        top_ratio: float | None = None,
        api_delay_seconds: float = 1.0,
        batch_delay_seconds: float = 3.0,
        batch_size: int = 5,
        quote_asset: str = "USDT",
    ) -> tuple[list[str], dict[str, float]]:
        """è®¡ç®—æŒ‡å®šæ—¥æœŸçš„universeã€‚

        Args:
            rebalance_date: é‡å¹³è¡¡æ—¥æœŸ
            t1_start_date: T1å¼€å§‹æ—¥æœŸ
            t3_months: T3æœˆæ•°
            top_k: é€‰æ‹©çš„topæ•°é‡
            top_ratio: é€‰æ‹©çš„topæ¯”ç‡
            api_delay_seconds: æ¯ä¸ªAPIè¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°
            batch_delay_seconds: æ¯æ‰¹æ¬¡è¯·æ±‚ä¹‹é—´çš„å»¶è¿Ÿç§’æ•°
            batch_size: æ¯æ‰¹æ¬¡çš„è¯·æ±‚æ•°é‡
            quote_asset: åŸºå‡†èµ„äº§ï¼Œç”¨äºç­›é€‰äº¤æ˜“å¯¹
        """
        try:
            # è·å–åœ¨è¯¥æ—¶é—´æ®µå†…å®é™…å­˜åœ¨çš„æ°¸ç»­åˆçº¦äº¤æ˜“å¯¹
            actual_symbols = self._get_available_symbols_for_period(calculated_t1_start, calculated_t1_end, quote_asset)

            # ç­›é™¤æ–°åˆçº¦ (åˆ›å»ºæ—¶é—´ä¸è¶³T3ä¸ªæœˆçš„)
            cutoff_date = self._subtract_months(calculated_t1_end, t3_months)
            eligible_symbols = [
                symbol for symbol in actual_symbols if self._symbol_exists_before_date(symbol, cutoff_date)
            ]

            if not eligible_symbols:
                logger.warning(f"æ—¥æœŸ {calculated_t1_start} åˆ° {calculated_t1_end}: æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„äº¤æ˜“å¯¹")
                return [], {}

            # é€šè¿‡APIè·å–æ•°æ®è®¡ç®—mean daily amount
            mean_amounts = {}

            logger.info(f"å¼€å§‹é€šè¿‡APIè·å– {len(eligible_symbols)} ä¸ªäº¤æ˜“å¯¹çš„å†å²æ•°æ®...")

            # åˆå§‹åŒ–ä¸“é—¨ç”¨äºuniverseè®¡ç®—çš„é¢‘ç‡ç®¡ç†å™¨
            universe_rate_manager = RateLimitManager(base_delay=api_delay_seconds)

            for i, symbol in enumerate(eligible_symbols):
                try:
                    # å°†æ—¥æœŸå­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶é—´æˆ³
                    start_ts = self._date_to_timestamp_start(calculated_t1_start)
                    end_ts = self._date_to_timestamp_end(calculated_t1_end)

                    # æ˜¾ç¤ºè¿›åº¦ï¼ˆæ¯10ä¸ªäº¤æ˜“å¯¹æ˜¾ç¤ºä¸€æ¬¡ï¼‰
                    if i % 10 == 0:
                        logger.info(f"å·²å¤„ç† {i}/{len(eligible_symbols)} ä¸ªäº¤æ˜“å¯¹...")

                    # ä¸´æ—¶ä½¿ç”¨è¿™ä¸ªé¢‘ç‡ç®¡ç†å™¨
                    original_manager = self.rate_limit_manager
                    self.rate_limit_manager = universe_rate_manager

                    try:
                        # è·å–å†å²Kçº¿æ•°æ®
                        klines = self._fetch_symbol_data(
                            symbol=symbol,
                            start_ts=start_ts,
                            end_ts=end_ts,
                            interval=Freq.d1,
                        )
                    finally:
                        # æ¢å¤åŸæ¥çš„é¢‘ç‡ç®¡ç†å™¨
                        self.rate_limit_manager = original_manager

                    if klines:
                        # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
                        expected_days = (
                            pd.to_datetime(calculated_t1_end) - pd.to_datetime(calculated_t1_start)
                        ).days + 1
                        actual_days = len(klines)

                        if actual_days < expected_days * 0.8:  # å…è®¸20%çš„æ•°æ®ç¼ºå¤±
                            logger.warning(f"äº¤æ˜“å¯¹ {symbol} æ•°æ®ä¸å®Œæ•´: æœŸæœ›{expected_days}å¤©ï¼Œå®é™…{actual_days}å¤©")

                        # è®¡ç®—å¹³å‡æ—¥æˆäº¤é¢
                        amounts = []
                        for kline in klines:
                            try:
                                # kline.raw_data[7] æ˜¯æˆäº¤é¢ï¼ˆUSDTï¼‰
                                if kline.raw_data and len(kline.raw_data) > 7:
                                    amount = float(kline.raw_data[7])
                                    amounts.append(amount)
                            except (ValueError, IndexError):
                                continue

                        if amounts:
                            mean_amount = sum(amounts) / len(amounts)
                            mean_amounts[symbol] = mean_amount
                        else:
                            logger.warning(f"äº¤æ˜“å¯¹ {symbol} åœ¨æœŸé—´å†…æ²¡æœ‰æœ‰æ•ˆçš„æˆäº¤é‡æ•°æ®")

                except Exception as e:
                    logger.warning(f"è·å– {symbol} æ•°æ®æ—¶å‡ºé”™ï¼Œè·³è¿‡: {e}")
                    continue

            # æŒ‰mean daily amountæ’åºå¹¶é€‰æ‹©top_kæˆ–top_ratio
            if mean_amounts:
                sorted_symbols = sorted(mean_amounts.items(), key=lambda x: x[1], reverse=True)

                if top_ratio is not None:
                    num_to_select = int(len(sorted_symbols) * top_ratio)
                elif top_k is not None:
                    num_to_select = top_k
                else:
                    # é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰æä¾›top_kæˆ–top_ratioï¼Œåˆ™é€‰æ‹©æ‰€æœ‰
                    num_to_select = len(sorted_symbols)

                top_symbols = sorted_symbols[:num_to_select]

                universe_symbols = [symbol for symbol, _ in top_symbols]
                final_amounts = dict(top_symbols)

                # æ˜¾ç¤ºé€‰æ‹©ç»“æœ
                if len(universe_symbols) <= 10:
                    logger.info(f"é€‰ä¸­çš„äº¤æ˜“å¯¹: {universe_symbols}")
                else:
                    logger.info(f"Top 5: {universe_symbols[:5]}")
                    logger.info("å®Œæ•´åˆ—è¡¨å·²ä¿å­˜åˆ°æ–‡ä»¶ä¸­")
            else:
                # å¦‚æœæ²¡æœ‰å¯ç”¨æ•°æ®ï¼Œè¿”å›ç©º
                universe_symbols = []
                final_amounts = {}
                logger.warning("æ— æ³•é€šè¿‡APIè·å–æ•°æ®ï¼Œè¿”å›ç©ºçš„universe")

            return universe_symbols, final_amounts

        except Exception as e:
            logger.error(f"è®¡ç®—æ—¥æœŸ {calculated_t1_start} åˆ° {calculated_t1_end} çš„universeæ—¶å‡ºé”™: {e}")
            return [], {}

    def _symbol_exists_before_date(self, symbol: str, cutoff_date: str) -> bool:
        """æ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸä¹‹å‰å°±å­˜åœ¨ã€‚"""
        try:
            # æ£€æŸ¥åœ¨cutoff_dateä¹‹å‰æ˜¯å¦æœ‰æ•°æ®
            # è¿™é‡Œæˆ‘ä»¬æ£€æŸ¥cutoff_dateå‰ä¸€å¤©çš„æ•°æ®
            check_date = (pd.to_datetime(cutoff_date) - timedelta(days=1)).strftime("%Y-%m-%d")
            return self.check_symbol_exists_on_date(symbol, check_date)
        except Exception:
            # å¦‚æœæ£€æŸ¥å¤±è´¥ï¼Œé»˜è®¤è®¤ä¸ºå­˜åœ¨
            return True

    def download_universe_data(
        self,
        universe_file: Path | str,
        db_path: Path | str,
        data_path: Path | str | None = None,
        interval: Freq = Freq.m1,
        max_workers: int = 4,
        max_retries: int = 3,
        include_buffer_days: int = 7,
        retry_config: RetryConfig | None = None,
        request_delay: float = 0.5,  # è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
        download_market_metrics: bool = True,  # æ˜¯å¦ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®ï¼ˆèµ„é‡‘è´¹ç‡ã€æŒä»“é‡ã€å¤šç©ºæ¯”ä¾‹ï¼‰
        metrics_interval: Freq = Freq.m5,  # å¸‚åœºæŒ‡æ ‡æ•°æ®çš„æ—¶é—´é—´éš”
        long_short_ratio_period: Freq = Freq.m5,  # å¤šç©ºæ¯”ä¾‹çš„æ—¶é—´å‘¨æœŸ
        long_short_ratio_types: list[str] | None = None,  # å¤šç©ºæ¯”ä¾‹ç±»å‹
        use_binance_vision: bool = False,  # æ˜¯å¦ä½¿ç”¨ Binance Vision ä¸‹è½½æŒ‡æ ‡æ•°æ®
    ) -> None:
        """æŒ‰å‘¨æœŸåˆ†åˆ«ä¸‹è½½universeæ•°æ®ï¼ˆæ›´ç²¾ç¡®çš„ä¸‹è½½æ–¹å¼ï¼‰ã€‚

        è¿™ç§æ–¹å¼ä¸ºæ¯ä¸ªé‡å¹³è¡¡å‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®ï¼Œå¯ä»¥é¿å…ä¸‹è½½ä¸å¿…è¦çš„æ•°æ®ã€‚

        Args:
            universe_file: universeå®šä¹‰æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®š)
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„ (å¿…é¡»æŒ‡å®šï¼Œå¦‚: /path/to/market.db)
            data_path: æ•°æ®æ–‡ä»¶å­˜å‚¨è·¯å¾„ (å¯é€‰ï¼Œç”¨äºå­˜å‚¨å…¶ä»–æ•°æ®æ–‡ä»¶)
            interval: æ•°æ®é¢‘ç‡
            max_workers: å¹¶å‘çº¿ç¨‹æ•°
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            include_buffer_days: ç¼“å†²å¤©æ•°
            request_delay: æ¯æ¬¡è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤0.5ç§’
            download_funding_rate: æ˜¯å¦ä¸‹è½½èµ„é‡‘è´¹ç‡æ•°æ®
            download_market_metrics: æ˜¯å¦ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®ï¼ˆèµ„é‡‘è´¹ç‡ã€æŒä»“é‡ã€å¤šç©ºæ¯”ä¾‹ï¼‰
            metrics_interval: å¸‚åœºæŒ‡æ ‡æ•°æ®çš„æ—¶é—´é—´éš”
            long_short_ratio_period: å¤šç©ºæ¯”ä¾‹çš„æ—¶é—´å‘¨æœŸ
            long_short_ratio_types: å¤šç©ºæ¯”ä¾‹ç±»å‹åˆ—è¡¨ï¼Œé»˜è®¤['account', 'position']
        """
        try:
            # éªŒè¯è·¯å¾„
            universe_file_obj = self._validate_and_prepare_path(universe_file, is_file=True)
            db_file_path = self._validate_and_prepare_path(db_path, is_file=True)

            # data_pathæ˜¯å¯é€‰çš„ï¼Œå¦‚æœæä¾›åˆ™éªŒè¯
            data_path_obj = None
            if data_path:
                data_path_obj = self._validate_and_prepare_path(data_path, is_file=False)

            # æ£€æŸ¥universeæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not universe_file_obj.exists():
                raise FileNotFoundError(f"Universeæ–‡ä»¶ä¸å­˜åœ¨: {universe_file_obj}")

            # åŠ è½½universeå®šä¹‰
            universe_def = UniverseDefinition.load_from_file(universe_file_obj)

            # è®¾ç½®å¤šç©ºæ¯”ä¾‹ç±»å‹é»˜è®¤å€¼
            if long_short_ratio_types is None:
                long_short_ratio_types = ["account", "position"]

            logger.info("ğŸ“Š æŒ‰å‘¨æœŸä¸‹è½½æ•°æ®:")
            logger.info(f"   - æ€»å¿«ç…§æ•°: {len(universe_def.snapshots)}")
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - å¹¶å‘çº¿ç¨‹: {max_workers}")
            logger.info(f"   - è¯·æ±‚é—´éš”: {request_delay}ç§’")
            logger.info(f"   - æ•°æ®åº“è·¯å¾„: {db_file_path}")
            logger.info(f"   - ä¸‹è½½å¸‚åœºæŒ‡æ ‡: {download_market_metrics}")
            if download_market_metrics:
                logger.info(f"   - æŒ‡æ ‡æ•°æ®é—´éš”: {metrics_interval}")
                logger.info(f"   - å¤šç©ºæ¯”ä¾‹ç±»å‹: {long_short_ratio_types}")
            if data_path_obj:
                logger.info(f"   - æ•°æ®æ–‡ä»¶è·¯å¾„: {data_path_obj}")

            # ä¸ºæ¯ä¸ªå‘¨æœŸå•ç‹¬ä¸‹è½½æ•°æ®
            for i, snapshot in enumerate(universe_def.snapshots):
                logger.info(f"ğŸ“… å¤„ç†å¿«ç…§ {i + 1}/{len(universe_def.snapshots)}: {snapshot.effective_date}")

                logger.info(f"   - äº¤æ˜“å¯¹æ•°é‡: {len(snapshot.symbols)}")
                logger.info(
                    f"   - è®¡ç®—æœŸé—´: {snapshot.calculated_t1_start} åˆ° {snapshot.calculated_t1_end} (å®šä¹‰universe)"
                )
                logger.info(f"   - ä½¿ç”¨æœŸé—´: {snapshot.start_date} åˆ° {snapshot.end_date} (å®é™…ä½¿ç”¨)")
                logger.info(
                    f"   - ä¸‹è½½èŒƒå›´: {snapshot.start_date} åˆ° {snapshot.end_date} (å«{include_buffer_days}å¤©ç¼“å†²)"
                )

                # ä¸‹è½½Kçº¿æ•°æ®
                self.get_perpetual_data(
                    symbols=snapshot.symbols,
                    start_time=snapshot.start_date,
                    end_time=snapshot.end_date,
                    db_path=db_file_path,
                    interval=interval,
                    max_workers=max_workers,
                    max_retries=max_retries,
                    retry_config=retry_config,
                    enable_integrity_check=True,
                    request_delay=request_delay,
                )

                # ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®
                if download_market_metrics:
                    logger.info("   ğŸ“ˆ å¼€å§‹ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®...")
                    self._download_market_metrics_for_snapshot(
                        snapshot=snapshot,
                        db_path=db_file_path,
                        interval=metrics_interval,
                        period=long_short_ratio_period,
                        long_short_ratio_types=long_short_ratio_types,
                        request_delay=request_delay,
                        use_binance_vision=use_binance_vision,
                    )

                logger.info(f"   âœ… å¿«ç…§ {snapshot.effective_date} ä¸‹è½½å®Œæˆ")

            logger.info("ğŸ‰ æ‰€æœ‰universeæ•°æ®ä¸‹è½½å®Œæˆ!")
            logger.info(f"ğŸ“ æ•°æ®å·²ä¿å­˜åˆ°: {db_file_path}")

        except Exception as e:
            logger.error(f"[red]æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"æŒ‰å‘¨æœŸä¸‹è½½universeæ•°æ®å¤±è´¥: {e}") from e

    def _download_market_metrics_for_snapshot(
        self,
        snapshot,
        db_path: Path,
        interval: Freq = Freq.m5,
        period: Freq = Freq.m5,
        long_short_ratio_types: list[str] | None = None,
        request_delay: float = 0.5,
        use_binance_vision: bool = False,
    ) -> None:
        """ä¸ºå•ä¸ªå¿«ç…§ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®ã€‚

        Args:
            snapshot: Universeå¿«ç…§
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            interval: æ—¶é—´é—´éš”
            period: å¤šç©ºæ¯”ä¾‹çš„æ—¶é—´å‘¨æœŸ
            long_short_ratio_types: å¤šç©ºæ¯”ä¾‹ç±»å‹åˆ—è¡¨
            request_delay: è¯·æ±‚é—´éš”
            use_binance_vision: æ˜¯å¦ä½¿ç”¨ Binance Vision ä¸‹è½½æ•°æ®
        """
        try:
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
            if self.db is None:
                self.db = MarketDB(str(db_path))

            # è®¾ç½®é»˜è®¤å€¼
            if long_short_ratio_types is None:
                long_short_ratio_types = ["account"]

            symbols = snapshot.symbols
            start_time = snapshot.start_date
            end_time = snapshot.end_date

            if use_binance_vision:
                # ä¸‹è½½èµ„é‡‘è´¹ç‡æ•°æ®
                self._download_funding_rate_batch(
                    symbols=symbols,
                    start_time=start_time,
                    end_time=end_time,
                    request_delay=request_delay,
                )
                logger.info("      ğŸ“Š ä½¿ç”¨ Binance Vision ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®...")
                # ä½¿ç”¨ Binance Vision ä¸‹è½½æ•°æ®
                self.download_binance_vision_metrics(
                    symbols=symbols,
                    start_date=start_time,
                    end_date=end_time,
                    data_types=["openInterest", "longShortRatio"],
                    request_delay=request_delay,
                )
            else:
                logger.info("      ğŸ“Š ä½¿ç”¨ API ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®...")
                # ä½¿ç”¨ä¼ ç»Ÿ API æ–¹å¼ä¸‹è½½æ•°æ®
                self._download_funding_rate_batch(
                    symbols=symbols,
                    start_time=start_time,
                    end_time=end_time,
                    request_delay=request_delay,
                )

                self._download_open_interest_batch(
                    symbols=symbols,
                    start_time=start_time,
                    end_time=end_time,
                    interval=interval,
                    request_delay=request_delay,
                )

                for ratio_type in long_short_ratio_types:
                    logger.info(f"        - ç±»å‹: {ratio_type}")
                    self._download_long_short_ratio_batch(
                        symbols=symbols,
                        start_time=start_time,
                        end_time=end_time,
                        period=period,
                        ratio_type=ratio_type,
                        request_delay=request_delay,
                    )

            logger.info("      âœ… å¸‚åœºæŒ‡æ ‡æ•°æ®ä¸‹è½½å®Œæˆ")

        except Exception as e:
            logger.error(f"[red]ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"ä¸‹è½½å¸‚åœºæŒ‡æ ‡æ•°æ®å¤±è´¥: {e}") from e

    def _download_funding_rate_batch(
        self,
        symbols: list[str],
        start_time: str,
        end_time: str,
        request_delay: float = 0.5,
    ) -> None:
        """æ‰¹é‡ä¸‹è½½èµ„é‡‘è´¹ç‡æ•°æ®ã€‚

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            request_delay: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰

        Note:
            - é€Ÿç‡é™åˆ¶: ä¸/fapi/v1/fundingInfoå…±äº«500è¯·æ±‚/5åˆ†é’Ÿ/IPé™åˆ¶
            - å¦‚æœä¸å‘é€æ—¶é—´å‚æ•°ï¼Œè¿”å›æœ€è¿‘çš„æ•°æ®
            - æ•°æ®æŒ‰å‡åºæ’åˆ—
        """
        try:
            logger.info("    ğŸ’° æ‰¹é‡ä¸‹è½½èµ„é‡‘è´¹ç‡æ•°æ®")

            all_funding_rates = []
            downloaded_count = 0
            failed_count = 0

            for i, symbol in enumerate(symbols):
                try:
                    logger.debug(f"        è·å– {symbol} èµ„é‡‘è´¹ç‡ ({i + 1}/{len(symbols)})")

                    # é¢‘ç‡é™åˆ¶ - æ¯”å…¶ä»–APIæ›´ä¸¥æ ¼ (500/5min vs 1000/5min)
                    if request_delay > 0:
                        time.sleep(request_delay)

                    funding_rates = self.get_funding_rate(
                        symbol=symbol,
                        start_time=start_time,
                        end_time=end_time,
                        limit=1000,  # ä½¿ç”¨æœ€å¤§å€¼ä»¥è·å–æ›´å¤šæ•°æ®
                    )

                    if funding_rates:
                        all_funding_rates.extend(funding_rates)
                        downloaded_count += 1
                        logger.debug(f"        âœ… {symbol}: {len(funding_rates)} æ¡è®°å½•")
                    else:
                        logger.debug(f"        âš ï¸ {symbol}: æ— æ•°æ®")

                except Exception as e:
                    failed_count += 1
                    error_msg = str(e).lower()
                    if any(keyword in error_msg for keyword in ["rate", "limit", "429", "exceeded"]):
                        logger.warning(f"        âš ï¸ {symbol}: å¯èƒ½é‡åˆ°é€Ÿç‡é™åˆ¶ - {e}")
                        # é‡åˆ°é€Ÿç‡é™åˆ¶æ—¶å¢åŠ å»¶è¿Ÿ
                        if request_delay < 2.0:
                            time.sleep(2.0)
                    else:
                        logger.warning(f"        âŒ {symbol}: {e}")
                    continue

            # æ‰¹é‡å­˜å‚¨
            if all_funding_rates and self.db:
                self.db.store_funding_rate(all_funding_rates)
                logger.info(f"        âœ… å­˜å‚¨äº† {len(all_funding_rates)} æ¡èµ„é‡‘è´¹ç‡è®°å½•")

            # æ±‡æ€»ç»“æœ
            logger.info(f"    ğŸ’° èµ„é‡‘è´¹ç‡æ•°æ®ä¸‹è½½å®Œæˆ: æˆåŠŸ {downloaded_count}/{len(symbols)}ï¼Œå¤±è´¥ {failed_count}")

        except Exception as e:
            logger.error(f"[red]æ‰¹é‡ä¸‹è½½èµ„é‡‘è´¹ç‡å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"æ‰¹é‡ä¸‹è½½èµ„é‡‘è´¹ç‡å¤±è´¥: {e}") from e

    def _download_open_interest_batch(
        self,
        symbols: list[str],
        start_time: str,
        end_time: str,
        interval: Freq = Freq.m5,
        request_delay: float = 0.5,
    ) -> None:
        """æ‰¹é‡ä¸‹è½½æŒä»“é‡æ•°æ®ã€‚"""
        try:
            logger.info("    ğŸ“Š æ‰¹é‡ä¸‹è½½æŒä»“é‡æ•°æ®")

            all_open_interests = []
            downloaded_count = 0
            failed_count = 0

            for i, symbol in enumerate(symbols):
                try:
                    logger.debug(f"        è·å– {symbol} æŒä»“é‡ ({i + 1}/{len(symbols)})")

                    # é¢‘ç‡é™åˆ¶
                    if request_delay > 0:
                        time.sleep(request_delay)

                    open_interests = self.get_open_interest(
                        symbol=symbol,
                        period=interval,
                        start_time=start_time,
                        end_time=end_time,
                        limit=500,
                    )

                    if open_interests:
                        all_open_interests.extend(open_interests)
                        downloaded_count += 1
                        logger.debug(f"        âœ… {symbol}: {len(open_interests)} æ¡è®°å½•")
                    else:
                        logger.debug(f"        âš ï¸ {symbol}: æ— æ•°æ®")

                except Exception as e:
                    failed_count += 1
                    error_msg = str(e).lower()
                    if any(keyword in error_msg for keyword in ["invalid", "time", "range", "data", "starttime"]):
                        logger.debug(f"        âš ï¸ {symbol}: æ—¶é—´èŒƒå›´é—®é¢˜ - {e}")
                    else:
                        logger.warning(f"        âŒ {symbol}: {e}")
                    continue

            # æ‰¹é‡å­˜å‚¨
            if all_open_interests and self.db:
                self.db.store_open_interest(all_open_interests)
                logger.info(f"        âœ… å­˜å‚¨äº† {len(all_open_interests)} æ¡æŒä»“é‡è®°å½•")

            # æ±‡æ€»ç»“æœ
            logger.info(f"    ğŸ“Š æŒä»“é‡æ•°æ®ä¸‹è½½å®Œæˆ: æˆåŠŸ {downloaded_count}/{len(symbols)}ï¼Œå¤±è´¥ {failed_count}")

        except Exception as e:
            logger.error(f"[red]æ‰¹é‡ä¸‹è½½æŒä»“é‡å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"æ‰¹é‡ä¸‹è½½æŒä»“é‡å¤±è´¥: {e}") from e

    def _download_long_short_ratio_batch(
        self,
        symbols: list[str],
        start_time: str,
        end_time: str,
        period: str = "5m",
        ratio_type: str = "account",
        request_delay: float = 0.5,
    ) -> None:
        """æ‰¹é‡ä¸‹è½½å¤šç©ºæ¯”ä¾‹æ•°æ®ã€‚

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            period: æ—¶é—´å‘¨æœŸï¼Œæ”¯æŒ "5m","15m","30m","1h","2h","4h","6h","12h","1d"
            ratio_type: æ¯”ä¾‹ç±»å‹
            request_delay: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰

        Note:
            - æ ¹æ®å¸å®‰APIé™åˆ¶ï¼Œåªæœ‰æœ€è¿‘30å¤©çš„æ•°æ®å¯ç”¨
            - è‡ªåŠ¨è·³è¿‡è¶…å‡º30å¤©é™åˆ¶çš„æ—¶é—´èŒƒå›´
        """
        try:
            logger.info(f"    ğŸ“Š æ‰¹é‡ä¸‹è½½å¤šç©ºæ¯”ä¾‹æ•°æ® (ç±»å‹: {ratio_type})")

            # æ£€æŸ¥30å¤©é™åˆ¶
            current_time = datetime.now()
            thirty_days_ago = current_time - timedelta(days=30)

            # è§£ææ—¶é—´å­—ç¬¦ä¸²
            try:
                start_dt = datetime.fromisoformat(
                    start_time.replace("Z", "+00:00") if start_time.endswith("Z") else start_time
                )
                end_dt = datetime.fromisoformat(end_time.replace("Z", "+00:00") if end_time.endswith("Z") else end_time)
            except ValueError:
                # å¦‚æœæ—¶é—´æ ¼å¼ä¸å¯¹ï¼Œå°è¯•å…¶ä»–æ ¼å¼
                start_dt = pd.to_datetime(start_time)
                end_dt = pd.to_datetime(end_time)

            # æ£€æŸ¥æ—¶é—´èŒƒå›´æ˜¯å¦è¶…å‡º30å¤©é™åˆ¶
            if end_dt < thirty_days_ago:
                logger.warning(f"    âš ï¸ è¯·æ±‚æ—¶é—´èŒƒå›´å®Œå…¨è¶…å‡º30å¤©é™åˆ¶ ({end_dt} < {thirty_days_ago})ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                return

            # è°ƒæ•´å¼€å§‹æ—¶é—´ä»¥ç¬¦åˆ30å¤©é™åˆ¶
            original_start_time = start_time
            if start_dt < thirty_days_ago:
                logger.warning("    âš ï¸ å¼€å§‹æ—¶é—´è¶…å‡º30å¤©é™åˆ¶ï¼Œè°ƒæ•´ä¸ºæœ€è¿‘30å¤©")
                start_time = thirty_days_ago.strftime("%Y-%m-%d")

            # å‚æ•°éªŒè¯
            valid_periods = ["5m", "15m", "30m", "1h", "2h", "4h", "6h", "12h", "1d"]
            if period not in valid_periods:
                logger.error(f"    âŒ æ— æ•ˆçš„periodå‚æ•°: {period}ï¼Œæ”¯æŒçš„å€¼: {valid_periods}")
                return

            all_long_short_ratios = []
            downloaded_count = 0
            failed_count = 0

            for i, symbol in enumerate(symbols):
                try:
                    logger.debug(f"        è·å– {symbol} å¤šç©ºæ¯”ä¾‹ ({i + 1}/{len(symbols)})")

                    # é¢‘ç‡é™åˆ¶
                    if request_delay > 0:
                        time.sleep(request_delay)

                    long_short_ratios = self.get_long_short_ratio(
                        symbol=symbol,
                        period=period,
                        ratio_type=ratio_type,
                        start_time=start_time,
                        end_time=end_time,
                        limit=500,
                    )

                    if long_short_ratios:
                        all_long_short_ratios.extend(long_short_ratios)
                        downloaded_count += 1
                        logger.debug(f"        âœ… {symbol}: {len(long_short_ratios)} æ¡è®°å½•")
                    else:
                        logger.debug(f"        âš ï¸ {symbol}: æ— æ•°æ® (å¯èƒ½è¶…å‡º30å¤©é™åˆ¶)")

                except Exception as e:
                    failed_count += 1
                    error_msg = str(e).lower()
                    if any(keyword in error_msg for keyword in ["invalid", "time", "range", "data"]):
                        logger.debug(f"        âš ï¸ {symbol}: å¯èƒ½è¶…å‡º30å¤©é™åˆ¶ - {e}")
                    else:
                        logger.warning(f"        âŒ {symbol}: {e}")
                    continue

            # æ‰¹é‡å­˜å‚¨
            if all_long_short_ratios and self.db:
                self.db.store_long_short_ratio(all_long_short_ratios)
                logger.info(f"        âœ… å­˜å‚¨äº† {len(all_long_short_ratios)} æ¡å¤šç©ºæ¯”ä¾‹è®°å½•")

            # æ±‡æ€»ç»“æœ
            logger.info(f"    ğŸ“Š å¤šç©ºæ¯”ä¾‹æ•°æ®ä¸‹è½½å®Œæˆ: æˆåŠŸ {downloaded_count}/{len(symbols)}ï¼Œå¤±è´¥ {failed_count}")

            if original_start_time != start_time:
                logger.info(f"    ğŸ“… æ—¶é—´èŒƒå›´å·²è°ƒæ•´: {original_start_time} -> {start_time} (å—30å¤©é™åˆ¶)")

        except Exception as e:
            logger.error(f"[red]æ‰¹é‡ä¸‹è½½å¤šç©ºæ¯”ä¾‹å¤±è´¥: {e}[/red]")
            raise MarketDataFetchError(f"æ‰¹é‡ä¸‹è½½å¤šç©ºæ¯”ä¾‹å¤±è´¥: {e}") from e

    def _analyze_universe_data_requirements(
        self,
        universe_def: UniverseDefinition,
        buffer_days: int = 7,
        extend_to_present: bool = True,
    ) -> dict[str, Any]:
        """åˆ†æuniverseæ•°æ®ä¸‹è½½éœ€æ±‚ã€‚

        æ³¨æ„ï¼šè¿™ä¸ªæ–¹æ³•è®¡ç®—æ€»ä½“èŒƒå›´ï¼Œä½†å®é™…ä¸‹è½½åº”è¯¥ä½¿ç”¨å„å¿«ç…§çš„ä½¿ç”¨æœŸé—´ã€‚
        æ¨èä½¿ç”¨ download_universe_data_by_periods æ–¹æ³•è¿›è¡Œç²¾ç¡®ä¸‹è½½ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            buffer_days: ç¼“å†²å¤©æ•°
            extend_to_present: æ˜¯å¦æ‰©å±•åˆ°å½“å‰æ—¥æœŸ

        Returns:
            Dict: ä¸‹è½½è®¡åˆ’è¯¦æƒ…
        """
        import pandas as pd

        # æ”¶é›†æ‰€æœ‰çš„äº¤æ˜“å¯¹å’Œå®é™…ä½¿ç”¨æ—¶é—´èŒƒå›´
        all_symbols = set()
        usage_dates = []  # ä½¿ç”¨æœŸé—´çš„æ—¥æœŸ
        calculation_dates = []  # è®¡ç®—æœŸé—´çš„æ—¥æœŸ

        for snapshot in universe_def.snapshots:
            all_symbols.update(snapshot.symbols)

            # ä½¿ç”¨æœŸé—´ - å®é™…éœ€è¦ä¸‹è½½çš„æ•°æ®
            usage_dates.extend(
                [
                    snapshot.start_date,  # å®é™…ä½¿ç”¨å¼€å§‹
                    snapshot.end_date,  # å®é™…ä½¿ç”¨ç»“æŸ
                ]
            )

            # è®¡ç®—æœŸé—´ - ç”¨äºå®šä¹‰universeçš„æ•°æ®
            calculation_dates.extend(
                [
                    snapshot.calculated_t1_start,
                    snapshot.calculated_t1_end,
                    snapshot.effective_date,
                ]
            )

        # è®¡ç®—æ€»ä½“æ—¶é—´èŒƒå›´ - åŸºäºä½¿ç”¨æœŸé—´è€Œä¸æ˜¯è®¡ç®—æœŸé—´
        start_date = pd.to_datetime(min(usage_dates)) - timedelta(days=buffer_days)
        end_date = pd.to_datetime(max(usage_dates)) + timedelta(days=buffer_days)

        if extend_to_present:
            end_date = max(end_date, pd.to_datetime("today"))

        # æ·»åŠ æ›´å¤šè¯¦ç»†ä¿¡æ¯
        return {
            "unique_symbols": sorted(all_symbols),
            "total_symbols": len(all_symbols),
            "overall_start_date": start_date.strftime("%Y-%m-%d"),
            "overall_end_date": end_date.strftime("%Y-%m-%d"),
            "usage_period_start": pd.to_datetime(min(usage_dates)).strftime("%Y-%m-%d"),
            "usage_period_end": pd.to_datetime(max(usage_dates)).strftime("%Y-%m-%d"),
            "calculation_period_start": pd.to_datetime(min(calculation_dates)).strftime("%Y-%m-%d"),
            "calculation_period_end": pd.to_datetime(max(calculation_dates)).strftime("%Y-%m-%d"),
            "snapshots_count": len(universe_def.snapshots),
            "note": "æ¨èä½¿ç”¨download_universe_data_by_periodsæ–¹æ³•è¿›è¡Œç²¾ç¡®ä¸‹è½½",
        }

    def get_funding_rate(
        self,
        symbol: str,
        start_time: str | datetime | None = None,
        end_time: str | datetime | None = None,
        limit: int = 100,  # æ”¹ä¸ºAPIé»˜è®¤å€¼
    ) -> list[FundingRate]:
        """è·å–æ°¸ç»­åˆçº¦èµ„é‡‘è´¹ç‡å†å²ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°ï¼Œå¦‚ 'BTCUSDT'
            start_time: å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³æˆ–æ—¥æœŸå­—ç¬¦ä¸²ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³æˆ–æ—¥æœŸå­—ç¬¦ä¸²ï¼‰
            limit: è¿”å›æ•°é‡é™åˆ¶ï¼Œé»˜è®¤100ï¼Œæœ€å¤§1000

        Returns:
            list[FundingRate]: èµ„é‡‘è´¹ç‡æ•°æ®åˆ—è¡¨

        Note:
            - å¦‚æœä¸å‘é€startTimeå’ŒendTimeï¼Œè¿”å›æœ€è¿‘çš„limitæ¡æ•°æ®
            - å¦‚æœæ—¶é—´èŒƒå›´å†…æ•°æ®è¶…è¿‡limitï¼Œä»startTimeå¼€å§‹è¿”å›limitæ¡
            - æ•°æ®æŒ‰å‡åºæ’åˆ—
            - é€Ÿç‡é™åˆ¶: ä¸/fapi/v1/fundingInfoå…±äº«500/5åˆ†é’Ÿ/IPé™åˆ¶

        Raises:
            MarketDataFetchError: è·å–æ•°æ®å¤±è´¥æ—¶
        """
        try:
            logger.info(f"è·å– {symbol} çš„èµ„é‡‘è´¹ç‡æ•°æ®")

            # å‚æ•°éªŒè¯
            if limit < 1 or limit > 1000:
                raise ValueError(f"limitå‚æ•°å¿…é¡»åœ¨1-1000èŒƒå›´å†…ï¼Œå½“å‰å€¼: {limit}")

            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "symbol": symbol,
                "limit": limit,
            }

            # å¤„ç†æ—¶é—´å‚æ•°
            if start_time is not None:
                if isinstance(start_time, str):
                    start_time_ts = self._date_to_timestamp_start(start_time)
                elif isinstance(start_time, datetime):
                    start_time_ts = str(int(start_time.timestamp() * 1000))
                else:
                    start_time_ts = str(start_time)
                params["startTime"] = start_time_ts

            if end_time is not None:
                if isinstance(end_time, str):
                    end_time_ts = self._date_to_timestamp_end(end_time)
                elif isinstance(end_time, datetime):
                    end_time_ts = str(int(end_time.timestamp() * 1000))
                else:
                    end_time_ts = str(end_time)
                params["endTime"] = end_time_ts

            # é¢‘ç‡é™åˆ¶æ§åˆ¶ - Funding Rate API: 500è¯·æ±‚/5åˆ†é’Ÿ/IP (æ›´ä¸¥æ ¼)
            self.rate_limit_manager.wait_before_request()

            # è°ƒç”¨Binance API
            data = self.client.futures_funding_rate(**params)

            if not data:
                logger.warning(f"æœªæ‰¾åˆ° {symbol} çš„èµ„é‡‘è´¹ç‡æ•°æ®")
                return []

            # è½¬æ¢ä¸ºFundingRateå¯¹è±¡
            funding_rates = [FundingRate.from_binance_response(item) for item in data]

            logger.info(f"æˆåŠŸè·å– {symbol} çš„ {len(funding_rates)} æ¡èµ„é‡‘è´¹ç‡è®°å½•")
            self.rate_limit_manager.handle_success()

            return funding_rates

        except ValueError as e:
            logger.error(f"[red]å‚æ•°éªŒè¯å¤±è´¥ {symbol}: {e}[/red]")
            raise
        except Exception as e:
            logger.error(f"[red]è·å–èµ„é‡‘è´¹ç‡å¤±è´¥ {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"è·å–èµ„é‡‘è´¹ç‡å¤±è´¥: {e}") from e

    def get_open_interest(
        self,
        symbol: str,
        period: str = "5m",
        start_time: str | datetime | None = None,
        end_time: str | datetime | None = None,
        limit: int = 500,
    ) -> list[OpenInterest]:
        """è·å–æ°¸ç»­åˆçº¦æŒä»“é‡æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°ï¼Œå¦‚ 'BTCUSDT'
            period: æ—¶é—´å‘¨æœŸï¼Œæ”¯æŒ "5m","15m","30m","1h","2h","4h","6h","12h","1d"
            start_time: å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³æˆ–æ—¥æœŸå­—ç¬¦ä¸²ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³æˆ–æ—¥æœŸå­—ç¬¦ä¸²ï¼‰
            limit: è¿”å›æ•°é‡é™åˆ¶ï¼Œé»˜è®¤500ï¼Œæœ€å¤§500

        Returns:
            list[OpenInterest]: æŒä»“é‡æ•°æ®åˆ—è¡¨

        Raises:
            MarketDataFetchError: è·å–æ•°æ®å¤±è´¥æ—¶
        """
        try:
            logger.info(f"è·å– {symbol} çš„æŒä»“é‡æ•°æ®")

            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "symbol": symbol,
                "period": period,
                "limit": min(limit, 500),
            }

            # å¤„ç†æ—¶é—´å‚æ•°
            if start_time is not None:
                if isinstance(start_time, str):
                    start_time_ts = self._date_to_timestamp_start(start_time)
                elif isinstance(start_time, datetime):
                    start_time_ts = str(int(start_time.timestamp() * 1000))
                else:
                    start_time_ts = str(start_time)
                params["startTime"] = start_time_ts

            if end_time is not None:
                if isinstance(end_time, str):
                    end_time_ts = self._date_to_timestamp_end(end_time)
                elif isinstance(end_time, datetime):
                    end_time_ts = str(int(end_time.timestamp() * 1000))
                else:
                    end_time_ts = str(end_time)
                params["endTime"] = end_time_ts

            # é¢‘ç‡é™åˆ¶æ§åˆ¶
            self.rate_limit_manager.wait_before_request()

            # è°ƒç”¨Binance API - è·å–å†å²æŒä»“é‡æ•°æ®
            data = self.client.futures_open_interest_hist(**params)

            if not data:
                logger.warning(f"æœªæ‰¾åˆ° {symbol} çš„æŒä»“é‡æ•°æ®")
                return []

            # è½¬æ¢ä¸ºOpenInterestå¯¹è±¡
            open_interests = [OpenInterest.from_binance_response(item) for item in data]

            logger.info(f"æˆåŠŸè·å– {symbol} çš„ {len(open_interests)} æ¡æŒä»“é‡è®°å½•")
            self.rate_limit_manager.handle_success()

            return open_interests

        except Exception as e:
            logger.error(f"[red]è·å–æŒä»“é‡å¤±è´¥ {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"è·å–æŒä»“é‡å¤±è´¥: {e}") from e

    def get_long_short_ratio(
        self,
        symbol: str,
        period: str = "5m",
        ratio_type: str = "account",
        start_time: str | datetime | None = None,
        end_time: str | datetime | None = None,
        limit: int = 500,
    ) -> list[LongShortRatio]:
        """è·å–å¤šç©ºæ¯”ä¾‹æ•°æ®ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹åç§°ï¼Œå¦‚ 'BTCUSDT'
            period: æ—¶é—´å‘¨æœŸï¼Œæ”¯æŒ "5m","15m","30m","1h","2h","4h","6h","12h","1d"
            ratio_type: æ¯”ä¾‹ç±»å‹:
                - "account": é¡¶çº§äº¤æ˜“è€…è´¦æˆ·å¤šç©ºæ¯”
                - "position": é¡¶çº§äº¤æ˜“è€…æŒä»“å¤šç©ºæ¯”
                - "global": å…¨å±€å¤šç©ºæ¯”
                - "taker": å¤§é¢äº¤æ˜“è€…å¤šç©ºæ¯”
            start_time: å¼€å§‹æ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³æˆ–æ—¥æœŸå­—ç¬¦ä¸²ï¼‰
            end_time: ç»“æŸæ—¶é—´ï¼ˆæ¯«ç§’æ—¶é—´æˆ³æˆ–æ—¥æœŸå­—ç¬¦ä¸²ï¼‰
            limit: è¿”å›æ•°é‡é™åˆ¶ï¼Œé»˜è®¤500ï¼Œæœ€å¤§500

        Returns:
            list[LongShortRatio]: å¤šç©ºæ¯”ä¾‹æ•°æ®åˆ—è¡¨

        Raises:
            MarketDataFetchError: è·å–æ•°æ®å¤±è´¥æ—¶
        """
        try:
            logger.info(f"è·å– {symbol} çš„å¤šç©ºæ¯”ä¾‹æ•°æ® (ç±»å‹: {ratio_type})")

            # æ„å»ºè¯·æ±‚å‚æ•°
            params = {
                "symbol": symbol,
                "period": period,
                "limit": min(limit, 500),
            }

            # å¤„ç†æ—¶é—´å‚æ•°
            if start_time is not None:
                if isinstance(start_time, str):
                    start_time_ts = self._date_to_timestamp_start(start_time)
                elif isinstance(start_time, datetime):
                    start_time_ts = str(int(start_time.timestamp() * 1000))
                else:
                    start_time_ts = str(start_time)
                params["startTime"] = start_time_ts

            if end_time is not None:
                if isinstance(end_time, str):
                    end_time_ts = self._date_to_timestamp_end(end_time)
                elif isinstance(end_time, datetime):
                    end_time_ts = str(int(end_time.timestamp() * 1000))
                else:
                    end_time_ts = str(end_time)
                params["endTime"] = end_time_ts

            # é¢‘ç‡é™åˆ¶æ§åˆ¶
            self.rate_limit_manager.wait_before_request()

            # æ ¹æ®ratio_typeé€‰æ‹©ä¸åŒçš„APIç«¯ç‚¹
            if ratio_type == "account":
                data = self.client.futures_top_longshort_account_ratio(**params)
            elif ratio_type == "position":
                data = self.client.futures_top_longshort_position_ratio(**params)
            elif ratio_type == "global":
                data = self.client.futures_global_longshort_ratio(**params)
            elif ratio_type == "taker":
                data = self.client.futures_taker_longshort_ratio(**params)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ratio_type: {ratio_type}")

            if not data:
                logger.warning(f"æœªæ‰¾åˆ° {symbol} çš„å¤šç©ºæ¯”ä¾‹æ•°æ®")
                return []

            # è½¬æ¢ä¸ºLongShortRatioå¯¹è±¡
            long_short_ratios = [LongShortRatio.from_binance_response(item, ratio_type) for item in data]

            logger.info(f"æˆåŠŸè·å– {symbol} çš„ {len(long_short_ratios)} æ¡å¤šç©ºæ¯”ä¾‹è®°å½•")
            self.rate_limit_manager.handle_success()

            return long_short_ratios

        except Exception as e:
            logger.error(f"[red]è·å–å¤šç©ºæ¯”ä¾‹å¤±è´¥ {symbol}: {e}[/red]")
            raise MarketDataFetchError(f"è·å–å¤šç©ºæ¯”ä¾‹å¤±è´¥: {e}") from e

    def _verify_universe_data_integrity(
        self,
        universe_def: UniverseDefinition,
        db_path: Path,
        interval: Freq,
        download_plan: dict[str, Any],
    ) -> None:
        """éªŒè¯ä¸‹è½½çš„universeæ•°æ®å®Œæ•´æ€§ã€‚

        Args:
            universe_def: Universeå®šä¹‰
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            interval: æ•°æ®é¢‘ç‡
            download_plan: ä¸‹è½½è®¡åˆ’
        """
        try:
            from cryptoservice.data import MarketDB

            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ - ç›´æ¥ä½¿ç”¨æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            db = MarketDB(str(db_path))

            logger.info("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
            incomplete_symbols: list[str] = []
            missing_data: list[dict[str, str]] = []
            successful_snapshots = 0

            for snapshot in universe_def.snapshots:
                try:
                    # æ£€æŸ¥è¯¥å¿«ç…§çš„ä¸»è¦äº¤æ˜“å¯¹æ•°æ®ï¼ŒåŸºäºä½¿ç”¨æœŸé—´éªŒè¯
                    # ä½¿ç”¨æ‰©å±•çš„æ—¶é—´èŒƒå›´ä»¥ç¡®ä¿èƒ½å¤Ÿæ‰¾åˆ°æ•°æ®
                    usage_start = pd.to_datetime(snapshot.start_date) - timedelta(days=3)
                    usage_end = pd.to_datetime(snapshot.end_date) + timedelta(days=3)

                    df = db.read_data(
                        symbols=snapshot.symbols[:3],  # åªæ£€æŸ¥å‰3ä¸ªä¸»è¦äº¤æ˜“å¯¹
                        start_time=usage_start.strftime("%Y-%m-%d"),
                        end_time=usage_end.strftime("%Y-%m-%d"),
                        freq=interval,
                        raise_on_empty=False,  # ä¸åœ¨æ²¡æœ‰æ•°æ®æ—¶æŠ›å‡ºå¼‚å¸¸
                    )

                    if df is not None and not df.empty:
                        # æ£€æŸ¥æ•°æ®è¦†ç›–çš„äº¤æ˜“å¯¹æ•°é‡
                        available_symbols = df.index.get_level_values("symbol").unique()
                        missing_symbols = set(snapshot.symbols[:3]) - set(available_symbols)
                        if missing_symbols:
                            incomplete_symbols.extend(missing_symbols)
                            logger.debug(f"å¿«ç…§ {snapshot.effective_date}ç¼ºå°‘äº¤æ˜“å¯¹: {list(missing_symbols)}")
                        else:
                            successful_snapshots += 1
                            logger.debug(f"å¿«ç…§ {snapshot.effective_date} éªŒè¯æˆåŠŸ")
                    else:
                        logger.debug(f"å¿«ç…§ {snapshot.effective_date} åœ¨æ‰©å±•æ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°æ•°æ®")
                        missing_data.append(
                            {
                                "snapshot_date": snapshot.effective_date,
                                "error": "No data in extended time range",
                            }
                        )

                except Exception as e:
                    logger.debug(f"éªŒè¯å¿«ç…§ {snapshot.effective_date} æ—¶å‡ºé”™: {e}")
                    # ä¸å†è®°å½•ä¸ºä¸¥é‡é”™è¯¯ï¼Œåªæ˜¯è®°å½•è°ƒè¯•ä¿¡æ¯
                    missing_data.append({"snapshot_date": snapshot.effective_date, "error": str(e)})

            # æŠ¥å‘ŠéªŒè¯ç»“æœ - æ›´å‹å¥½çš„æŠ¥å‘Šæ–¹å¼
            total_snapshots = len(universe_def.snapshots)
            success_rate = successful_snapshots / total_snapshots if total_snapshots > 0 else 0

            logger.info("âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯å®Œæˆ")
            logger.info(f"   - å·²ä¸‹è½½äº¤æ˜“å¯¹: {download_plan['total_symbols']} ä¸ª")
            logger.info(f"   - æ—¶é—´èŒƒå›´: {download_plan['overall_start_date']} åˆ° {download_plan['overall_end_date']}")
            logger.info(f"   - æ•°æ®é¢‘ç‡: {interval.value}")
            logger.info(f"   - æˆåŠŸéªŒè¯å¿«ç…§: {successful_snapshots}/{total_snapshots} ({success_rate:.1%})")

            # åªæœ‰åœ¨æˆåŠŸç‡å¾ˆä½æ—¶æ‰ç»™å‡ºè­¦å‘Š
            if success_rate < 0.5:
                logger.warning(f"âš ï¸ éªŒè¯æˆåŠŸç‡è¾ƒä½: {success_rate:.1%}")
                if incomplete_symbols:
                    unique_incomplete = set(incomplete_symbols)
                    logger.warning(f"   - æ•°æ®ä¸å®Œæ•´çš„äº¤æ˜“å¯¹: {len(unique_incomplete)} ä¸ª")
                    if len(unique_incomplete) <= 5:
                        logger.warning(f"   - å…·ä½“äº¤æ˜“å¯¹: {list(unique_incomplete)}")

                if missing_data:
                    logger.warning(f"   - æ— æ³•éªŒè¯çš„å¿«ç…§: {len(missing_data)} ä¸ª")
            else:
                logger.info("ğŸ“Š æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®è¿›è¡Œåç»­åˆ†æ")

        except Exception as e:
            logger.warning(f"æ•°æ®å®Œæ•´æ€§éªŒè¯è¿‡ç¨‹ä¸­å‡ºç°é—®é¢˜ï¼Œä½†ä¸å½±å“æ•°æ®ä½¿ç”¨: {e}")
            logger.info("ğŸ’¡ æç¤º: éªŒè¯å¤±è´¥ä¸ä»£è¡¨æ•°æ®ä¸‹è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•æŸ¥è¯¢å…·ä½“æ•°æ®è¿›è¡Œç¡®è®¤")

    def download_binance_vision_metrics(
        self,
        symbols: list[str],
        start_date: str,
        end_date: str,
        data_types: list[str] | None = None,
        request_delay: float = 1.0,
    ) -> None:
        """ä» Binance Vision ä¸‹è½½æœŸè´§æŒ‡æ ‡æ•°æ® (OI å’Œ Long-Short Ratio)ã€‚

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            start_date: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
            data_types: æ•°æ®ç±»å‹åˆ—è¡¨ï¼Œæ”¯æŒ "openInterest", "longShortRatio"
            request_delay: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        if data_types is None:
            data_types = ["openInterest", "longShortRatio"]

        try:
            logger.info(f"å¼€å§‹ä» Binance Vision ä¸‹è½½æŒ‡æ ‡æ•°æ®: {data_types}")

            if self.db is None:
                raise ValueError("æ•°æ®åº“æœªåˆå§‹åŒ–")

            # åˆ›å»ºæ—¥æœŸèŒƒå›´
            date_range = pd.date_range(start=start_date, end=end_date, freq="D")

            for date in date_range:
                date_str = date.strftime("%Y-%m-%d")
                logger.info(f"å¤„ç†æ—¥æœŸ: {date_str}")

                # ä¸‹è½½æŒ‡æ ‡æ•°æ®ï¼ˆæ‰€æœ‰ç±»å‹éƒ½åœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­ï¼‰
                self._download_metrics_from_vision(symbols, date_str, request_delay)

                # è¯·æ±‚å»¶è¿Ÿ
                if request_delay > 0:
                    time.sleep(request_delay)

            logger.info("âœ… Binance Vision æŒ‡æ ‡æ•°æ®ä¸‹è½½å®Œæˆ")

        except Exception as e:
            logger.error(f"ä» Binance Vision ä¸‹è½½æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
            raise

    def _download_metrics_from_vision(
        self,
        symbols: list[str],
        date: str,
        request_delay: float = 1.0,
    ) -> None:
        """ä» Binance Vision ä¸‹è½½æŒ‡æ ‡æ•°æ®ï¼ˆæŒä»“é‡å’Œå¤šç©ºæ¯”ä¾‹ï¼‰ã€‚

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            date: æ—¥æœŸ (YYYY-MM-DD)
            request_delay: è¯·æ±‚å»¶è¿Ÿï¼ˆç§’ï¼‰
        """
        try:
            date_obj = datetime.strptime(date, "%Y-%m-%d")
            date_str = date_obj.strftime("%Y-%m-%d")

            # Binance Vision S3 URL æ ¼å¼
            base_url = "https://s3-ap-northeast-1.amazonaws.com/data.binance.vision/data/futures/um/daily/metrics"

            for symbol in symbols:
                try:
                    # æ„å»º URL - æ‰€æœ‰æŒ‡æ ‡æ•°æ®åœ¨åŒä¸€ä¸ªæ–‡ä»¶ä¸­
                    url = f"{base_url}/{symbol}/{symbol}-metrics-{date_str}.zip"

                    logger.debug(f"ä¸‹è½½ {symbol} æŒ‡æ ‡æ•°æ®: {url}")

                    # ä¸‹è½½å¹¶è§£ææ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
                    retry_config = RetryConfig(max_retries=3, base_delay=2.0)
                    metrics_data = self._download_and_parse_metrics_csv(url, symbol, retry_config)

                    if metrics_data and self.db:
                        # å­˜å‚¨æŒä»“é‡æ•°æ®
                        if metrics_data.get("open_interest"):
                            self.db.store_open_interest(metrics_data["open_interest"])
                            logger.info(
                                f"âœ… {symbol}: å­˜å‚¨äº† {date_str} {len(metrics_data['open_interest'])} æ¡æŒä»“é‡è®°å½•"
                            )

                        # å­˜å‚¨å¤šç©ºæ¯”ä¾‹æ•°æ®
                        if metrics_data.get("long_short_ratio"):
                            self.db.store_long_short_ratio(metrics_data["long_short_ratio"])
                            logger.info(
                                f"âœ… {symbol}: å­˜å‚¨äº† {date_str} {len(metrics_data['long_short_ratio'])} æ¡å¤šç©ºæ¯”ä¾‹è®°å½•"
                            )
                    else:
                        logger.warning(f"âš ï¸ {symbol}: æ— æ³•è·å–æŒ‡æ ‡æ•°æ®")

                except Exception as e:
                    logger.warning(f"ä¸‹è½½ {symbol} æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
                    # è®°å½•å¤±è´¥çš„ä¸‹è½½
                    self._record_failed_download(symbol, url, str(e), date_str)
                    continue

                # è¯·æ±‚å»¶è¿Ÿ
                if request_delay > 0:
                    time.sleep(request_delay)

        except Exception as e:
            logger.error(f"ä» Binance Vision ä¸‹è½½æŒ‡æ ‡æ•°æ®å¤±è´¥: {e}")
            raise

    def _create_enhanced_session(self) -> requests.Session:
        """åˆ›å»ºå¢å¼ºçš„ç½‘ç»œè¯·æ±‚ä¼šè¯ï¼Œå…·æœ‰æ›´å¥½çš„SSLé…ç½®å’Œè¿æ¥æ± è®¾ç½®ã€‚"""
        session = requests.Session()

        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"],
        )

        # åˆ›å»ºè‡ªå®šä¹‰çš„ HTTPAdapter
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=10,
            pool_maxsize=20,
            pool_block=False,
        )

        # æŒ‚è½½é€‚é…å™¨åˆ°ä¼šè¯
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # è®¾ç½®é»˜è®¤å¤´éƒ¨
        session.headers.update(
            {
                "User-Agent": (
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/91.0.4472.124 Safari/537.36"
                ),
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
        )

        return session

    def _record_failed_download(self, symbol: str, url: str, error: str, date: str) -> None:
        """è®°å½•å¤±è´¥çš„ä¸‹è½½ã€‚

        Args:
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            url: ä¸‹è½½URL
            error: é”™è¯¯ä¿¡æ¯
            date: æ—¥æœŸ
        """
        if symbol not in self.failed_downloads:
            self.failed_downloads[symbol] = []

        self.failed_downloads[symbol].append(
            {
                "url": url,
                "error": error,
                "date": date,
                "timestamp": datetime.now().isoformat(),
                "retry_count": 0,
            }
        )

    def get_failed_downloads(self) -> dict[str, list[dict]]:
        """è·å–å¤±è´¥çš„ä¸‹è½½è®°å½•ã€‚

        Returns:
            å¤±è´¥ä¸‹è½½è®°å½•çš„å­—å…¸
        """
        return self.failed_downloads.copy()

    def clear_failed_downloads(self, symbol: str | None = None) -> None:
        """æ¸…é™¤å¤±è´¥çš„ä¸‹è½½è®°å½•ã€‚

        Args:
            symbol: å¯é€‰ï¼ŒæŒ‡å®šè¦æ¸…é™¤çš„äº¤æ˜“å¯¹ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™æ¸…é™¤æ‰€æœ‰
        """
        if symbol:
            self.failed_downloads.pop(symbol, None)
        else:
            self.failed_downloads.clear()

    def retry_failed_downloads(self, symbol: str | None = None, max_retries: int = 3) -> dict[str, Any]:
        """é‡è¯•å¤±è´¥çš„ä¸‹è½½ã€‚

        Args:
            symbol: å¯é€‰ï¼ŒæŒ‡å®šè¦é‡è¯•çš„äº¤æ˜“å¯¹ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™é‡è¯•æ‰€æœ‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°

        Returns:
            é‡è¯•ç»“æœç»Ÿè®¡
        """
        if not self.failed_downloads:
            logger.info("ğŸ“‹ æ²¡æœ‰å¤±è´¥çš„ä¸‹è½½è®°å½•")
            return {"total": 0, "success": 0, "failed": 0}

        symbols_to_retry = [symbol] if symbol else list(self.failed_downloads.keys())
        total_attempts = 0
        success_count = 0
        failed_count = 0

        for retry_symbol in symbols_to_retry:
            if retry_symbol not in self.failed_downloads:
                continue

            failures = self.failed_downloads[retry_symbol].copy()

            for failure in failures:
                if failure["retry_count"] >= max_retries:
                    logger.debug(f"â­ï¸ {retry_symbol}: è·³è¿‡ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")
                    continue

                total_attempts += 1

                try:
                    logger.info(f"ğŸ”„ é‡è¯•ä¸‹è½½ {retry_symbol}: {failure['date']}")

                    # å°è¯•é‡æ–°ä¸‹è½½
                    retry_config = RetryConfig(max_retries=2, base_delay=3.0)
                    metrics_data = self._download_and_parse_metrics_csv(failure["url"], retry_symbol, retry_config)

                    if metrics_data and self.db:
                        # å­˜å‚¨æ•°æ®
                        if metrics_data.get("open_interest"):
                            self.db.store_open_interest(metrics_data["open_interest"])
                        if metrics_data.get("long_short_ratio"):
                            self.db.store_long_short_ratio(metrics_data["long_short_ratio"])

                        # ä»å¤±è´¥åˆ—è¡¨ä¸­ç§»é™¤
                        self.failed_downloads[retry_symbol].remove(failure)
                        if not self.failed_downloads[retry_symbol]:
                            del self.failed_downloads[retry_symbol]

                        success_count += 1
                        logger.info(f"âœ… {retry_symbol}: é‡è¯•æˆåŠŸ")

                    else:
                        failure["retry_count"] += 1
                        failed_count += 1
                        logger.warning(f"âŒ {retry_symbol}: é‡è¯•å¤±è´¥")

                except Exception as e:
                    failure["retry_count"] += 1
                    failed_count += 1
                    logger.warning(f"âŒ {retry_symbol}: é‡è¯•å¼‚å¸¸ - {e}")

                # é¿å…è¿‡äºé¢‘ç¹çš„é‡è¯•
                time.sleep(1.0)

        result: dict[str, Any] = {
            "total": total_attempts,
            "success": success_count,
            "failed": failed_count,
        }

        logger.info(f"ğŸ“Š é‡è¯•ç»Ÿè®¡: æ€»è®¡ {total_attempts}, æˆåŠŸ {success_count}, å¤±è´¥ {failed_count}")
        return result

    def _validate_metrics_data(self, data: dict[str, list], symbol: str, url: str) -> dict[str, list] | None:
        """éªŒè¯ metrics æ•°æ®çš„å®Œæ•´æ€§å’Œè´¨é‡ã€‚

        Args:
            data: åŒ…å« metrics æ•°æ®çš„å­—å…¸
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            url: æ•°æ®æºURL

        Returns:
            éªŒè¯åçš„æ•°æ®å­—å…¸ï¼Œå¦‚æœæ•°æ®ä¸åˆæ ¼åˆ™è¿”å›None
        """
        try:
            issues = []
            validated_data: dict[str, list] = {
                "open_interest": [],
                "long_short_ratio": [],
            }

            # éªŒè¯æŒä»“é‡æ•°æ®
            if data.get("open_interest"):
                oi_data = data["open_interest"]
                valid_oi = []

                for i, oi in enumerate(oi_data):
                    try:
                        # æ£€æŸ¥å¿…è¦å­—æ®µ
                        if not hasattr(oi, "symbol") or not hasattr(oi, "open_interest") or not hasattr(oi, "time"):
                            issues.append(f"æŒä»“é‡è®°å½• {i}: ç¼ºå°‘å¿…è¦å­—æ®µ")
                            continue

                        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                        if oi.open_interest < 0:
                            issues.append(f"æŒä»“é‡è®°å½• {i}: æŒä»“é‡ä¸ºè´Ÿæ•°")
                            continue

                        # æ£€æŸ¥æ—¶é—´æˆ³æœ‰æ•ˆæ€§
                        if oi.time <= 0:
                            issues.append(f"æŒä»“é‡è®°å½• {i}: æ—¶é—´æˆ³æ— æ•ˆ")
                            continue

                        valid_oi.append(oi)

                    except Exception as e:
                        issues.append(f"æŒä»“é‡è®°å½• {i}: éªŒè¯å¤±è´¥ - {e}")
                        continue

                validated_data["open_interest"] = valid_oi

                # è´¨é‡æ£€æŸ¥
                if len(valid_oi) < len(oi_data) * 0.5:
                    logger.warning(f"âš ï¸ {symbol}: æŒä»“é‡æ•°æ®è´¨é‡è¾ƒä½ï¼Œæœ‰æ•ˆè®°å½• {len(valid_oi)}/{len(oi_data)}")

            # éªŒè¯å¤šç©ºæ¯”ä¾‹æ•°æ®
            if data.get("long_short_ratio"):
                lsr_data = data["long_short_ratio"]
                valid_lsr = []

                for i, lsr in enumerate(lsr_data):
                    try:
                        # æ£€æŸ¥å¿…è¦å­—æ®µ
                        if (
                            not hasattr(lsr, "symbol")
                            or not hasattr(lsr, "long_short_ratio")
                            or not hasattr(lsr, "time")
                        ):
                            issues.append(f"å¤šç©ºæ¯”ä¾‹è®°å½• {i}: ç¼ºå°‘å¿…è¦å­—æ®µ")
                            continue

                        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                        if lsr.long_short_ratio < 0:
                            issues.append(f"å¤šç©ºæ¯”ä¾‹è®°å½• {i}: æ¯”ä¾‹ä¸ºè´Ÿæ•°")
                            continue

                        # æ£€æŸ¥æ—¶é—´æˆ³æœ‰æ•ˆæ€§
                        if lsr.time <= 0:
                            issues.append(f"å¤šç©ºæ¯”ä¾‹è®°å½• {i}: æ—¶é—´æˆ³æ— æ•ˆ")
                            continue

                        valid_lsr.append(lsr)

                    except Exception as e:
                        issues.append(f"å¤šç©ºæ¯”ä¾‹è®°å½• {i}: éªŒè¯å¤±è´¥ - {e}")
                        continue

                validated_data["long_short_ratio"] = valid_lsr

                # è´¨é‡æ£€æŸ¥
                if len(valid_lsr) < len(lsr_data) * 0.5:
                    logger.warning(f"âš ï¸ {symbol}: å¤šç©ºæ¯”ä¾‹æ•°æ®è´¨é‡è¾ƒä½ï¼Œæœ‰æ•ˆè®°å½• {len(valid_lsr)}/{len(lsr_data)}")

            # è®°å½•éªŒè¯ç»“æœ
            if issues:
                logger.debug(f"ğŸ“‹ {symbol}: æ•°æ®éªŒè¯å‘ç° {len(issues)} ä¸ªé—®é¢˜")
                if len(issues) <= 3:
                    for issue in issues:
                        logger.debug(f"  - {issue}")
                else:
                    for issue in issues[:3]:
                        logger.debug(f"  - {issue}")
                    logger.debug(f"  - ... è¿˜æœ‰ {len(issues) - 3} ä¸ªé—®é¢˜")

            # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
            if not validated_data["open_interest"] and not validated_data["long_short_ratio"]:
                logger.warning(f"âš ï¸ {symbol}: æ²¡æœ‰æœ‰æ•ˆçš„metricsæ•°æ®")
                return None

            logger.debug(
                f"âœ… {symbol}: æ•°æ®éªŒè¯é€šè¿‡ - "
                f"æŒä»“é‡: {len(validated_data['open_interest'])}, "
                f"å¤šç©ºæ¯”ä¾‹: {len(validated_data['long_short_ratio'])}"
            )
            return validated_data

        except Exception as e:
            logger.warning(f"âŒ {symbol}: æ•°æ®éªŒè¯å¤±è´¥ - {e}")
            return data  # éªŒè¯å¤±è´¥æ—¶è¿”å›åŸå§‹æ•°æ®

    def _download_and_parse_metrics_csv(
        self,
        url: str,
        symbol: str,
        retry_config: Optional[RetryConfig] = None,
    ) -> dict[str, list] | None:
        """ä¸‹è½½å¹¶è§£æ Binance Vision æŒ‡æ ‡ CSV æ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰ã€‚

        Args:
            url: ä¸‹è½½ URL
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            retry_config: é‡è¯•é…ç½®

        Returns:
            åŒ…å«ä¸åŒæŒ‡æ ‡æ•°æ®çš„å­—å…¸
        """
        if retry_config is None:
            retry_config = RetryConfig(max_retries=3, base_delay=2.0)

        backoff = ExponentialBackoff(retry_config)
        error_handler = EnhancedErrorHandler()

        while True:
            try:
                # ä½¿ç”¨å¢å¼ºçš„ä¼šè¯ä¸‹è½½ ZIP æ–‡ä»¶
                session = self._create_enhanced_session()
                response = session.get(url, timeout=30)
                response.raise_for_status()

                # è§£å‹ ZIP æ–‡ä»¶
                with zipfile.ZipFile(BytesIO(response.content)) as zip_file:
                    # åœ¨ ZIP æ–‡ä»¶ä¸­æŸ¥æ‰¾ CSV æ–‡ä»¶
                    csv_files = [f for f in zip_file.namelist() if f.endswith(".csv")]

                    if not csv_files:
                        logger.warning(f"ZIP æ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ° CSV æ–‡ä»¶: {url}")
                        return None

                    result: dict[str, list] = {
                        "open_interest": [],
                        "long_short_ratio": [],
                    }

                    # å¤„ç†æ¯ä¸ª CSV æ–‡ä»¶
                    for csv_file in csv_files:
                        try:
                            with zip_file.open(csv_file) as f:
                                content = f.read().decode("utf-8")

                            # è§£æ CSV å†…å®¹
                            csv_reader = csv.DictReader(content.splitlines())
                            rows = list(csv_reader)

                            if not rows:
                                logger.warning(f"CSV æ–‡ä»¶ {csv_file} ä¸ºç©º")
                                continue

                            # æ£€æŸ¥æ•°æ®ç»“æ„ï¼Œæ‰€æœ‰æŒ‡æ ‡æ•°æ®éƒ½åœ¨åŒä¸€ä¸ª CSV æ–‡ä»¶ä¸­
                            first_row = rows[0]

                            # å¦‚æœåŒ…å«æŒä»“é‡å­—æ®µï¼Œè§£ææŒä»“é‡æ•°æ®
                            if "sum_open_interest" in first_row:
                                oi_data = self._parse_oi_data(rows, symbol)
                                result["open_interest"].extend(oi_data)

                            # å¦‚æœåŒ…å«å¤šç©ºæ¯”ä¾‹å­—æ®µï¼Œè§£æå¤šç©ºæ¯”ä¾‹æ•°æ®
                            if any(
                                field in first_row
                                for field in [
                                    "sum_toptrader_long_short_ratio",
                                    "count_long_short_ratio",
                                    "sum_taker_long_short_vol_ratio",
                                ]
                            ):
                                lsr_data = self._parse_lsr_data(rows, symbol, csv_file)
                                result["long_short_ratio"].extend(lsr_data)

                        except Exception as e:
                            logger.warning(f"è§£æ CSV æ–‡ä»¶ {csv_file} æ—¶å‡ºé”™: {e}")
                            continue

                    # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
                    if result["open_interest"] or result["long_short_ratio"]:
                        validated_result = self._validate_metrics_data(result, symbol, url)
                        return validated_result
                    else:
                        return None

            except Exception as e:
                severity = error_handler.classify_error(e)

                # å¤„ç†ä¸å¯é‡è¯•çš„é”™è¯¯
                if severity == ErrorSeverity.CRITICAL:
                    logger.error(f"âŒ è‡´å‘½é”™è¯¯ - {symbol}: {e}")
                    logger.error(f"å»ºè®®: {error_handler.get_recommended_action(e)}")
                    return None

                # åˆ¤æ–­æ˜¯å¦é‡è¯•
                if not error_handler.should_retry(e, backoff.attempt, retry_config.max_retries):
                    logger.warning(f"âŒ é‡è¯•å¤±è´¥ - {symbol}: {e}")
                    if severity == ErrorSeverity.LOW:
                        # å¯¹äºä½ä¸¥é‡æ€§é”™è¯¯ï¼Œè¿”å›Noneè€Œä¸è®°å½•é”™è¯¯
                        return None
                    logger.warning(f"ğŸ”— URL: {url}")
                    logger.warning(f"ğŸ’¡ å»ºè®®: {error_handler.get_recommended_action(e)}")
                    return None

                # æ‰§è¡Œé‡è¯•
                logger.warning(f"ğŸ”„ é‡è¯• {backoff.attempt + 1}/{retry_config.max_retries} - {symbol}: {e}")
                logger.info(f"ğŸ’¡ å»ºè®®: {error_handler.get_recommended_action(e)}")

                try:
                    backoff.wait()
                except Exception:
                    logger.warning(f"âŒ è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•° - {symbol}")
                    return None

    def _parse_oi_data(self, raw_data: list[dict], symbol: str) -> list[OpenInterest]:
        """è§£ææŒä»“é‡æ•°æ®ã€‚

        Args:
            raw_data: åŸå§‹ CSV æ•°æ®
            symbol: äº¤æ˜“å¯¹ç¬¦å·

        Returns:
            OpenInterest å¯¹è±¡åˆ—è¡¨
        """
        open_interests = []

        for row in raw_data:
            try:
                # è§£ææ—¶é—´å­—æ®µ (create_time æ ¼å¼: YYYY-MM-DD HH:MM:SS)
                create_time = row["create_time"]
                timestamp = int(datetime.strptime(create_time, "%Y-%m-%d %H:%M:%S").timestamp() * 1000)

                # Binance Vision æŒä»“é‡æ•°æ®æ ¼å¼
                open_interest = OpenInterest(
                    symbol=symbol,
                    open_interest=Decimal(str(row["sum_open_interest"])),
                    time=timestamp,
                    open_interest_value=(
                        Decimal(str(row["sum_open_interest_value"])) if row.get("sum_open_interest_value") else None
                    ),
                )
                open_interests.append(open_interest)

            except (ValueError, KeyError) as e:
                logger.warning(f"è§£ææŒä»“é‡æ•°æ®è¡Œæ—¶å‡ºé”™: {e}, è¡Œæ•°æ®: {row}")
                continue

        return open_interests

    def _parse_lsr_data(self, raw_data: list[dict], symbol: str, file_name: str) -> list[LongShortRatio]:
        """è§£æå¤šç©ºæ¯”ä¾‹æ•°æ®ã€‚

        Args:
            raw_data: åŸå§‹ CSV æ•°æ®
            symbol: äº¤æ˜“å¯¹ç¬¦å·
            file_name: CSV æ–‡ä»¶åï¼ˆç”¨äºåˆ¤æ–­æ¯”ä¾‹ç±»å‹ï¼‰

        Returns:
            LongShortRatio å¯¹è±¡åˆ—è¡¨
        """
        long_short_ratios = []

        for row in raw_data:
            try:
                # è§£ææ—¶é—´å­—æ®µ (create_time æ ¼å¼: YYYY-MM-DD HH:MM:SS)
                create_time = row["create_time"]
                timestamp = int(datetime.strptime(create_time, "%Y-%m-%d %H:%M:%S").timestamp() * 1000)

                # å¤„ç†é¡¶çº§äº¤æ˜“è€…æ•°æ® (å¦‚æœå­˜åœ¨)
                if "sum_toptrader_long_short_ratio" in row:
                    ratio_value = Decimal(str(row["sum_toptrader_long_short_ratio"]))

                    # è®¡ç®—å¹³å‡æ¯”ä¾‹ (ä½¿ç”¨è®¡æ•°æ¥å¹³å‡)
                    if "count_toptrader_long_short_ratio" in row:
                        count = Decimal(str(row["count_toptrader_long_short_ratio"]))
                        if count > 0:
                            ratio_value = ratio_value / count

                    # ä»æ¯”ä¾‹è®¡ç®—å¤šç©ºè´¦æˆ·æ¯”ä¾‹ (å‡è®¾æ¯”ä¾‹æ˜¯long/short)
                    if ratio_value > 0:
                        total = ratio_value + 1  # long + short
                        long_account = ratio_value / total
                        short_account = Decimal("1") / total
                    else:
                        long_account = Decimal("0.5")
                        short_account = Decimal("0.5")

                    long_short_ratios.append(
                        LongShortRatio(
                            symbol=symbol,
                            long_short_ratio=ratio_value,
                            long_account=long_account,
                            short_account=short_account,
                            timestamp=timestamp,
                            ratio_type="account",
                        )
                    )

                # å¤„ç† Taker æ•°æ® (å¦‚æœå­˜åœ¨)
                if "sum_taker_long_short_vol_ratio" in row:
                    taker_ratio = Decimal(str(row["sum_taker_long_short_vol_ratio"]))

                    # ä»æ¯”ä¾‹è®¡ç®—å¤šç©ºæˆäº¤é‡æ¯”ä¾‹
                    if taker_ratio > 0:
                        total = taker_ratio + 1
                        long_vol = taker_ratio / total
                        short_vol = Decimal("1") / total
                    else:
                        long_vol = Decimal("0.5")
                        short_vol = Decimal("0.5")

                    long_short_ratios.append(
                        LongShortRatio(
                            symbol=symbol,
                            long_short_ratio=taker_ratio,
                            long_account=long_vol,
                            short_account=short_vol,
                            timestamp=timestamp,
                            ratio_type="taker",
                        )
                    )

            except (ValueError, KeyError) as e:
                logger.warning(f"è§£æå¤šç©ºæ¯”ä¾‹æ•°æ®è¡Œæ—¶å‡ºé”™: {e}, è¡Œæ•°æ®: {row}")
                continue

        return long_short_ratios

    def _resample_metrics_data(
        self,
        metrics_data: dict[str, list],
        target_freq: Freq,
        source_freq: Freq = Freq.m5,
    ) -> dict[str, list]:
        """å¯¹ metrics æ•°æ®è¿›è¡Œé¢‘ç‡è½¬æ¢ã€‚

        Args:
            metrics_data: åŒ…å« open_interest å’Œ long_short_ratio çš„æ•°æ®å­—å…¸
            target_freq: ç›®æ ‡é¢‘ç‡
            source_freq: æºæ•°æ®é¢‘ç‡ï¼Œé»˜è®¤ä¸º 5 åˆ†é’Ÿ

        Returns:
            é¢‘ç‡è½¬æ¢åçš„æ•°æ®å­—å…¸
        """
        try:
            # å¦‚æœç›®æ ‡é¢‘ç‡ä¸æºé¢‘ç‡ç›¸åŒï¼Œç›´æ¥è¿”å›
            if target_freq == source_freq:
                return metrics_data

            result: dict[str, list] = {"open_interest": [], "long_short_ratio": []}

            # å¤„ç†æŒä»“é‡æ•°æ®
            if metrics_data.get("open_interest"):
                result["open_interest"] = self._resample_open_interest_data(
                    metrics_data["open_interest"], target_freq, source_freq
                )

            # å¤„ç†å¤šç©ºæ¯”ä¾‹æ•°æ®
            if metrics_data.get("long_short_ratio"):
                result["long_short_ratio"] = self._resample_long_short_ratio_data(
                    metrics_data["long_short_ratio"], target_freq, source_freq
                )

            return result

        except Exception as e:
            logger.warning(f"é¢‘ç‡è½¬æ¢å¤±è´¥: {e}")
            return metrics_data  # è¿”å›åŸå§‹æ•°æ®

    def _resample_open_interest_data(
        self,
        oi_data: list[OpenInterest],
        target_freq: Freq,
        source_freq: Freq = Freq.m5,
    ) -> list[OpenInterest]:
        """å¯¹æŒä»“é‡æ•°æ®è¿›è¡Œé¢‘ç‡è½¬æ¢ã€‚

        Args:
            oi_data: æŒä»“é‡æ•°æ®åˆ—è¡¨
            target_freq: ç›®æ ‡é¢‘ç‡
            source_freq: æºæ•°æ®é¢‘ç‡

        Returns:
            é¢‘ç‡è½¬æ¢åçš„æŒä»“é‡æ•°æ®åˆ—è¡¨
        """
        if not oi_data:
            return []

        # æŒ‰symbolåˆ†ç»„å¤„ç†
        symbol_groups: dict[str, list] = {}
        for item in oi_data:
            if item.symbol not in symbol_groups:
                symbol_groups[item.symbol] = []
            symbol_groups[item.symbol].append(item)

        result = []
        for symbol, symbol_data in symbol_groups.items():
            # æŒ‰æ—¶é—´æ’åº
            symbol_data.sort(key=lambda x: x.time)

            # è½¬æ¢ä¸ºDataFrameè¿›è¡Œé‡é‡‡æ ·
            df = pd.DataFrame(
                [
                    {
                        "timestamp": pd.to_datetime(item.time, unit="ms"),
                        "open_interest": float(item.open_interest),
                        "open_interest_value": (float(item.open_interest_value) if item.open_interest_value else None),
                    }
                    for item in symbol_data
                ]
            )

            if df.empty:
                continue

            df.set_index("timestamp", inplace=True)

            # æ ¹æ®ç›®æ ‡é¢‘ç‡è¿›è¡Œé‡é‡‡æ ·
            freq_str = self._freq_to_pandas_freq(target_freq)

            if self._is_upsampling(source_freq, target_freq):
                # ä¸Šé‡‡æ ·ï¼šä½¿ç”¨å‰å‘å¡«å……
                resampled = df.resample(freq_str).ffill()
            else:
                # ä¸‹é‡‡æ ·ï¼šä½¿ç”¨å¹³å‡å€¼
                resampled = df.resample(freq_str).agg({"open_interest": "mean", "open_interest_value": "mean"})

            # è½¬æ¢å› OpenInterest å¯¹è±¡
            for timestamp, row in resampled.iterrows():
                if not pd.isna(row["open_interest"]):
                    # è½¬æ¢æ—¶é—´æˆ³ä¸ºæ¯«ç§’
                    result.append(
                        OpenInterest(
                            symbol=symbol,
                            open_interest=Decimal(str(row["open_interest"])),
                            time=int(cast(pd.Timestamp, timestamp).timestamp() * 1000),
                            open_interest_value=(
                                Decimal(str(row["open_interest_value"]))
                                if not pd.isna(row["open_interest_value"])
                                else None
                            ),
                        )
                    )

        return result

    def _resample_long_short_ratio_data(
        self,
        lsr_data: list[LongShortRatio],
        target_freq: Freq,
        source_freq: Freq = Freq.m5,
    ) -> list[LongShortRatio]:
        """å¯¹å¤šç©ºæ¯”ä¾‹æ•°æ®è¿›è¡Œé¢‘ç‡è½¬æ¢ã€‚

        Args:
            lsr_data: å¤šç©ºæ¯”ä¾‹æ•°æ®åˆ—è¡¨
            target_freq: ç›®æ ‡é¢‘ç‡
            source_freq: æºæ•°æ®é¢‘ç‡

        Returns:
            é¢‘ç‡è½¬æ¢åçš„å¤šç©ºæ¯”ä¾‹æ•°æ®åˆ—è¡¨
        """
        if not lsr_data:
            return []

        # æŒ‰symbolå’Œratio_typeåˆ†ç»„å¤„ç†
        groups: dict[tuple[str, str], list] = {}
        for item in lsr_data:
            key = (item.symbol, item.ratio_type)
            if key not in groups:
                groups[key] = []
            groups[key].append(item)

        result = []
        for (symbol, ratio_type), group_data in groups.items():
            # æŒ‰æ—¶é—´æ’åº
            group_data.sort(key=lambda x: x.timestamp)

            # è½¬æ¢ä¸ºDataFrameè¿›è¡Œé‡é‡‡æ ·
            df = pd.DataFrame(
                [
                    {
                        "timestamp": pd.to_datetime(item.timestamp, unit="ms"),
                        "long_short_ratio": float(item.long_short_ratio),
                        "long_account": (float(item.long_account) if item.long_account else None),
                        "short_account": (float(item.short_account) if item.short_account else None),
                    }
                    for item in group_data
                ]
            )

            if df.empty:
                continue

            df.set_index("timestamp", inplace=True)

            # æ ¹æ®ç›®æ ‡é¢‘ç‡è¿›è¡Œé‡é‡‡æ ·
            freq_str = self._freq_to_pandas_freq(target_freq)

            if self._is_upsampling(source_freq, target_freq):
                # ä¸Šé‡‡æ ·ï¼šä½¿ç”¨å‰å‘å¡«å……
                resampled = df.resample(freq_str).ffill()
            else:
                # ä¸‹é‡‡æ ·ï¼šä½¿ç”¨åŠ æƒå¹³å‡ï¼ˆå¯¹äºæ¯”ä¾‹æ•°æ®ï¼‰
                resampled = df.resample(freq_str).agg(
                    {
                        "long_short_ratio": "mean",
                        "long_account": "mean",
                        "short_account": "mean",
                    }
                )

            # è½¬æ¢å› LongShortRatio å¯¹è±¡
            for timestamp, row in resampled.iterrows():
                if not pd.isna(row["long_short_ratio"]):
                    result.append(
                        LongShortRatio(
                            symbol=symbol,
                            long_short_ratio=Decimal(str(row["long_short_ratio"])),
                            long_account=(
                                Decimal(str(row["long_account"])) if not pd.isna(row["long_account"]) else Decimal("0")
                            ),
                            short_account=(
                                Decimal(str(row["short_account"]))
                                if not pd.isna(row["short_account"])
                                else Decimal("0")
                            ),
                            timestamp=int(cast(pd.Timestamp, timestamp).timestamp() * 1000),
                            ratio_type=ratio_type,
                        )
                    )

        return result

    def _freq_to_pandas_freq(self, freq: Freq) -> str:
        """å°† Freq æšä¸¾è½¬æ¢ä¸º pandas é¢‘ç‡å­—ç¬¦ä¸²ã€‚

        Args:
            freq: é¢‘ç‡æšä¸¾

        Returns:
            pandas é¢‘ç‡å­—ç¬¦ä¸²
        """
        freq_map = {
            Freq.m1: "1min",
            Freq.m3: "3min",
            Freq.m5: "5min",
            Freq.m15: "15min",
            Freq.m30: "30min",
            Freq.h1: "1h",
            Freq.h2: "2h",
            Freq.h4: "4h",
            Freq.h6: "6h",
            Freq.h8: "8h",
            Freq.h12: "12h",
            Freq.d1: "1D",
            Freq.w1: "1W",
            Freq.M1: "1M",
        }
        return freq_map.get(freq, "5min")

    def _is_upsampling(self, source_freq: Freq, target_freq: Freq) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸Šé‡‡æ ·ï¼ˆç›®æ ‡é¢‘ç‡æ›´é«˜ï¼‰ã€‚

        Args:
            source_freq: æºé¢‘ç‡
            target_freq: ç›®æ ‡é¢‘ç‡

        Returns:
            å¦‚æœæ˜¯ä¸Šé‡‡æ ·è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
        """
        # å®šä¹‰é¢‘ç‡çš„åˆ†é’Ÿæ•°
        freq_minutes = {
            Freq.m1: 1,
            Freq.m3: 3,
            Freq.m5: 5,
            Freq.m15: 15,
            Freq.m30: 30,
            Freq.h1: 60,
            Freq.h2: 120,
            Freq.h4: 240,
            Freq.h6: 360,
            Freq.h8: 480,
            Freq.h12: 720,
            Freq.d1: 1440,
            Freq.w1: 10080,
            Freq.M1: 43200,  # çº¦30å¤©
        }

        source_minutes = freq_minutes.get(source_freq, 5)
        target_minutes = freq_minutes.get(target_freq, 5)

        return target_minutes < source_minutes

    @staticmethod
    def get_symbol_categories() -> dict[str, list[str]]:
        """è·å–å½“å‰æ‰€æœ‰äº¤æ˜“å¯¹çš„åˆ†ç±»ä¿¡æ¯ã€‚

        Returns:
            å­—å…¸ï¼Œkeyä¸ºäº¤æ˜“å¯¹symbolï¼Œvalueä¸ºåˆ†ç±»æ ‡ç­¾åˆ—è¡¨
        """
        try:
            logger.info("è·å– Binance äº¤æ˜“å¯¹åˆ†ç±»ä¿¡æ¯...")

            # è°ƒç”¨ Binance åˆ†ç±» API
            url = "https://www.binance.com/bapi/composite/v1/public/marketing/symbol/list"
            response = requests.get(url, timeout=30)
            response.raise_for_status()

            data = response.json()

            if data.get("code") != "000000":
                raise ValueError(f"API è¿”å›é”™è¯¯: {data.get('message', 'Unknown error')}")

            # æå– symbol å’Œ tags çš„æ˜ å°„å…³ç³»
            symbol_categories = {}
            for item in data.get("data", []):
                symbol = item.get("symbol", "")
                tags = item.get("tags", [])

                # åªä¿ç•™ USDT äº¤æ˜“å¯¹
                if symbol.endswith("USDT"):
                    symbol_categories[symbol] = sorted(tags)  # å¯¹æ ‡ç­¾è¿›è¡Œæ’åº

            logger.info(f"æˆåŠŸè·å– {len(symbol_categories)} ä¸ªäº¤æ˜“å¯¹çš„åˆ†ç±»ä¿¡æ¯")
            return symbol_categories

        except Exception as e:
            logger.error(f"è·å–äº¤æ˜“å¯¹åˆ†ç±»ä¿¡æ¯å¤±è´¥: {e}")
            raise

    @staticmethod
    def get_all_categories() -> list[str]:
        """è·å–æ‰€æœ‰å¯èƒ½çš„åˆ†ç±»æ ‡ç­¾ã€‚

        Returns:
            æŒ‰å­—æ¯æ’åºçš„åˆ†ç±»æ ‡ç­¾åˆ—è¡¨
        """
        try:
            symbol_categories = MarketDataService.get_symbol_categories()

            # æ”¶é›†æ‰€æœ‰æ ‡ç­¾
            all_tags = set()
            for tags in symbol_categories.values():
                all_tags.update(tags)

            # æŒ‰å­—æ¯æ’åº
            return sorted(list(all_tags))

        except Exception as e:
            logger.error(f"è·å–åˆ†ç±»æ ‡ç­¾å¤±è´¥: {e}")
            raise

    @staticmethod
    def create_category_matrix(
        symbols: list[str], categories: list[str] | None = None
    ) -> tuple[list[str], list[str], list[list[int]]]:
        """åˆ›å»º symbols å’Œ categories çš„å¯¹åº”çŸ©é˜µã€‚

        Args:
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            categories: åˆ†ç±»åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨è·å–æ‰€æœ‰åˆ†ç±»

        Returns:
            å…ƒç»„ (symbols, categories, matrix)
            - symbols: æ’åºåçš„äº¤æ˜“å¯¹åˆ—è¡¨
            - categories: æ’åºåçš„åˆ†ç±»åˆ—è¡¨
            - matrix: äºŒç»´çŸ©é˜µï¼Œmatrix[i][j] = 1 è¡¨ç¤º symbols[i] å±äº categories[j]
        """
        try:
            # è·å–å½“å‰åˆ†ç±»ä¿¡æ¯
            symbol_categories = MarketDataService.get_symbol_categories()

            # å¦‚æœæ²¡æœ‰æŒ‡å®šåˆ†ç±»ï¼Œè·å–æ‰€æœ‰åˆ†ç±»
            if categories is None:
                categories = MarketDataService.get_all_categories()
            else:
                categories = sorted(categories)

            # è¿‡æ»¤å¹¶æ’åºsymbolsï¼ˆåªä¿ç•™æœ‰åˆ†ç±»ä¿¡æ¯çš„ï¼‰
            valid_symbols = [s for s in symbols if s in symbol_categories]
            valid_symbols.sort()

            # åˆ›å»ºçŸ©é˜µ
            matrix = []
            for symbol in valid_symbols:
                symbol_tags = symbol_categories.get(symbol, [])
                row = [1 if category in symbol_tags else 0 for category in categories]
                matrix.append(row)

            logger.info(f"åˆ›å»ºåˆ†ç±»çŸ©é˜µ: {len(valid_symbols)} symbols Ã— {len(categories)} categories")

            return valid_symbols, categories, matrix

        except Exception as e:
            logger.error(f"åˆ›å»ºåˆ†ç±»çŸ©é˜µå¤±è´¥: {e}")
            raise

    @staticmethod
    def save_category_matrix_csv(
        output_path: Path | str,
        symbols: list[str],
        date_str: str | None = None,
        categories: list[str] | None = None,
    ) -> None:
        """å°†åˆ†ç±»çŸ©é˜µä¿å­˜ä¸º CSV æ–‡ä»¶ã€‚

        Args:
            output_path: è¾“å‡ºç›®å½•è·¯å¾„
            symbols: äº¤æ˜“å¯¹åˆ—è¡¨
            date_str: æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)ï¼ŒNone è¡¨ç¤ºä½¿ç”¨å½“å‰æ—¥æœŸ
            categories: åˆ†ç±»åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨è·å–æ‰€æœ‰åˆ†ç±»
        """
        try:
            import csv
            from datetime import datetime

            output_path = Path(output_path)
            output_path.mkdir(parents=True, exist_ok=True)

            # å¦‚æœæ²¡æœ‰æŒ‡å®šæ—¥æœŸï¼Œä½¿ç”¨å½“å‰æ—¥æœŸ
            if date_str is None:
                date_str = datetime.now().strftime("%Y-%m-%d")

            # åˆ›å»ºåˆ†ç±»çŸ©é˜µ
            valid_symbols, sorted_categories, matrix = MarketDataService.create_category_matrix(symbols, categories)

            # æ–‡ä»¶åæ ¼å¼: categories_YYYY-MM-DD.csv
            filename = f"categories_{date_str}.csv"
            file_path = output_path / filename

            # å†™å…¥ CSV æ–‡ä»¶
            with open(file_path, "w", newline="", encoding="utf-8") as csvfile:
                writer = csv.writer(csvfile)

                # å†™å…¥è¡¨å¤´ (symbol, category1, category2, ...)
                header = ["symbol"] + sorted_categories
                writer.writerow(header)

                # å†™å…¥æ•°æ®è¡Œ
                for i, symbol in enumerate(valid_symbols):
                    row = [symbol] + matrix[i]
                    writer.writerow(row)

            logger.info(f"æˆåŠŸä¿å­˜åˆ†ç±»çŸ©é˜µåˆ°: {file_path}")
            logger.info(f"çŸ©é˜µå¤§å°: {len(valid_symbols)} symbols Ã— {len(sorted_categories)} categories")

        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†ç±»çŸ©é˜µå¤±è´¥: {e}")
            raise

    @staticmethod
    def download_and_save_categories_for_universe(
        universe_file: Path | str,
        output_path: Path | str,
        categories: list[str] | None = None,
    ) -> None:
        """ä¸º universe ä¸­çš„æ‰€æœ‰äº¤æ˜“å¯¹ä¸‹è½½å¹¶ä¿å­˜åˆ†ç±»ä¿¡æ¯ã€‚

        Args:
            universe_file: universe å®šä¹‰æ–‡ä»¶
            output_path: è¾“å‡ºç›®å½•
            categories: åˆ†ç±»åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨è·å–æ‰€æœ‰åˆ†ç±»
        """
        try:
            from datetime import datetime

            # éªŒè¯è·¯å¾„
            universe_file_obj = MarketDataService._validate_and_prepare_path(universe_file, is_file=True)
            output_path_obj = MarketDataService._validate_and_prepare_path(output_path, is_file=False)

            # æ£€æŸ¥universeæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not universe_file_obj.exists():
                raise FileNotFoundError(f"Universeæ–‡ä»¶ä¸å­˜åœ¨: {universe_file_obj}")

            # åŠ è½½universeå®šä¹‰
            universe_def = UniverseDefinition.load_from_file(universe_file_obj)

            logger.info("ğŸ·ï¸ å¼€å§‹ä¸º universe ä¸‹è½½åˆ†ç±»ä¿¡æ¯:")
            logger.info(f"   - Universeå¿«ç…§æ•°: {len(universe_def.snapshots)}")
            logger.info(f"   - è¾“å‡ºç›®å½•: {output_path_obj}")

            # æ”¶é›†æ‰€æœ‰äº¤æ˜“å¯¹
            all_symbols = set()
            for snapshot in universe_def.snapshots:
                all_symbols.update(snapshot.symbols)

            all_symbols_list = sorted(list(all_symbols))
            logger.info(f"   - æ€»äº¤æ˜“å¯¹æ•°: {len(all_symbols_list)}")

            # è·å–å½“å‰åˆ†ç±»ä¿¡æ¯ï¼ˆç”¨äºæ‰€æœ‰å†å²æ•°æ®ï¼‰
            current_date = datetime.now().strftime("%Y-%m-%d")
            logger.info(f"   ğŸ“… è·å– {current_date} çš„åˆ†ç±»ä¿¡æ¯ï¼ˆç”¨äºå¡«å……å†å²æ•°æ®ï¼‰")

            # ä¸ºæ¯ä¸ªå¿«ç…§æ—¥æœŸä¿å­˜åˆ†ç±»çŸ©é˜µ
            for i, snapshot in enumerate(universe_def.snapshots):
                logger.info(f"   ğŸ“… å¤„ç†å¿«ç…§ {i + 1}/{len(universe_def.snapshots)}: {snapshot.effective_date}")

                # ä½¿ç”¨å¿«ç…§çš„æœ‰æ•ˆæ—¥æœŸ
                snapshot_date = snapshot.effective_date

                # ä¿å­˜è¯¥å¿«ç…§çš„åˆ†ç±»çŸ©é˜µ
                MarketDataService.save_category_matrix_csv(
                    output_path=output_path_obj,
                    symbols=snapshot.symbols,
                    date_str=snapshot_date,
                    categories=categories,
                )

                logger.info(f"       âœ… ä¿å­˜äº† {len(snapshot.symbols)} ä¸ªäº¤æ˜“å¯¹çš„åˆ†ç±»ä¿¡æ¯")

            # ä¹Ÿä¿å­˜ä¸€ä¸ªå½“å‰æ—¥æœŸçš„å®Œæ•´çŸ©é˜µï¼ˆåŒ…å«æ‰€æœ‰äº¤æ˜“å¯¹ï¼‰
            logger.info(f"   ğŸ“… ä¿å­˜å½“å‰æ—¥æœŸ ({current_date}) çš„å®Œæ•´åˆ†ç±»çŸ©é˜µ")
            MarketDataService.save_category_matrix_csv(
                output_path=output_path_obj,
                symbols=all_symbols_list,
                date_str=current_date,
                categories=categories,
            )

            logger.info("âœ… æ‰€æœ‰åˆ†ç±»ä¿¡æ¯ä¸‹è½½å’Œä¿å­˜å®Œæˆ")

        except Exception as e:
            logger.error(f"ä¸º universe ä¸‹è½½åˆ†ç±»ä¿¡æ¯å¤±è´¥: {e}")
            raise
