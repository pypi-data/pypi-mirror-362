from typing import Any, Dict, List, Optional
from tqdm import tqdm


try:
    import ollama
except ImportError:
    raise ImportError("ollama is required for OllamaBackend. Install with: pip install minieval[ollama]")

from ..datatypes import LMOutput, LMRequest, SamplingParams


class OllamaBackend:
    def __init__(self, model_path: str, **kwargs):
        self.model_path = model_path
        self.client = ollama.Client(**kwargs)
        
    def generate(
        self, requests: List[LMRequest], sampling_params: SamplingParams = None
    ) -> List[List[LMOutput]]:
        # Map SamplingParams to Ollama format
        options = {}
        repeats = 1
        logprobs_enabled = False
        
        if sampling_params:
            if sampling_params.temperature is not None:
                options["temperature"] = sampling_params.temperature
            if sampling_params.top_p is not None:
                options["top_p"] = sampling_params.top_p
            if sampling_params.top_k is not None:
                options["top_k"] = sampling_params.top_k
            if sampling_params.max_gen_toks is not None:
                options["num_predict"] = sampling_params.max_gen_toks
            if sampling_params.stop_sequences is not None:
                options["stop"] = sampling_params.stop_sequences
            
            # Handle repeats (number of completions)
            repeats = sampling_params.repeats if sampling_params.repeats is not None else 1
            logprobs_enabled = sampling_params.logprobs if sampling_params.logprobs is not None else False
        
        results = []
        for request in tqdm(requests, desc="Generating responses"):
            request_results = []
            for _ in range(repeats):
                try:
                    response = self.client.generate(
                        model=self.model_path,
                        prompt=request.context,
                        options=options,
                        stream=False
                    )
                    
                    # Extract text and logprobs if available
                    text = response.get("response", "")
                    logprobs = None
                    
                    # Ollama doesn't provide detailed logprobs in the same format as vLLM
                    # We'll create a simple placeholder structure
                    if logprobs_enabled:
                        logprobs = [{"token": token, "logprob": 0.0} for token in text.split()]
                    
                    request_results.append(LMOutput(text=text, logprobs=logprobs))
                    
                except Exception as e:
                    # Handle errors gracefully
                    request_results.append(LMOutput(text=f"Error: {str(e)}", logprobs=None))
            
            results.append(request_results)
        
        return results

    def logprobs(
        self,
        requests: List[LMRequest],
        sampling_params: SamplingParams = None,
    ) -> List[List[LMOutput]]:
        raise NotImplementedError("Ollama does not support logprobs!")