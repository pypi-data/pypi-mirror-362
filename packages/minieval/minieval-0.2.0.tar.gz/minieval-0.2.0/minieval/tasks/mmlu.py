from datasets import load_dataset

from minieval.datatypes import Instance, LMOutput, Task, TaskConfig
from minieval.extract import qa


class MMLU(Task):
    # fmt: off
    subsets = [
        "abstract_algebra", "anatomy", "astronomy", "business_ethics", "clinical_knowledge", "college_biology", 
        "college_chemistry", "college_computer_science", "college_mathematics", "college_medicine", "college_physics", 
        "computer_security", "conceptual_physics", "econometrics", "electrical_engineering", "elementary_mathematics", 
        "formal_logic", "global_facts", "high_school_biology", "high_school_chemistry", "high_school_computer_science", "high_school_european_history", "high_school_geography", "high_school_government_and_politics", 
        "high_school_macroeconomics", "high_school_mathematics", "high_school_microeconomics", "high_school_physics", 
        "high_school_psychology", "high_school_statistics", "high_school_us_history", "high_school_world_history", "human_aging", 
        "human_sexuality", "international_law", "jurisprudence", "logical_fallacies", "machine_learning", "management", "marketing", 
        "medical_genetics", "miscellaneous", "moral_disputes", "moral_scenarios", "nutrition", "philosophy", "prehistory", 
        "professional_accounting", "professional_law", "professional_medicine", "professional_psychology", "public_relations", 
        "security_studies", "sociology", "us_foreign_policy", "virology", "world_religions"
    ]
    # fmt: on
    hf_path = "cais/mmlu"

    def __init__(self, config: TaskConfig):
        self.config = config
        self._requests = None

    @property
    def requests(self):
        if self._requests is None:
            dataset = load_dataset(path=self.hf_path, name=self.config.subset, split="test")
            self._requests = list(map(self._process_instance, dataset))
        return self._requests

    def _process_instance(self, doc):
        gold_idx = doc["answer"]
        choices = doc["choices"]

        return Instance(
            question=doc["question"],
            gold_completion=choices[gold_idx],
            choices=choices,
            solution=gold_idx,
        )

    @classmethod
    def extract_answer(self, generation: LMOutput) -> str:
        answer = qa.extract_mcqa_answer(
            generation.text, answer_regexes=["\\(?([A-D])\\)?"]  # both "(A)" and "A"
        )
        if answer in ["A", "B", "C", "D"]:
            return answer
        return None

    def _construct_few_shot(self):
        # Sample few shot examples from the "dev" set, following
        # https://github.com/hendrycks/test/blob/master/evaluate.py#L28
        return load_dataset(path=self.hf_path, name=self.config.subset, split="dev")


class MMLUPro(Task):
    # fmt: off
    subsets = [
        "math", "health", "physics", "business", "biology", "chemistry", "computer science", 
        "economics", "engineering", "philosophy", "other", "history", "psychology", "law"
    ]
    # fmt: on
    hf_path = "TIGER-Lab/MMLU-Pro"

    def __init__(self, config):
        self.config = config
        self._requests = None

    @property
    def requests(self):
        if self._requests is None:
            dataset = load_dataset(path=self.hf_path, split="test")
            self._requests = list(map(self._process_instance, dataset))
        return self._requests

    def _process_instance(self, doc):
        gold_idx = doc["answer_index"]
        choices = doc["options"]

        return Instance(
            question=doc["question"],
            gold_completion=choices[gold_idx],
            choices=choices,
            solution=gold_idx,
            metadata={"id": doc["question_id"], "src": doc["src"], "subset": doc["category"]},
        )

    @classmethod
    def extract_answer(self, generation: LMOutput) -> str:
        answer = qa.extract_mcqa_answer(
            generation.text, answer_regexes=["\\(?([A-J])\\)?"]  # both "(A)" and "A"
        )
        if answer in ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J"]:
            return answer
        return None

    def _construct_few_shot(self):
        return load_dataset(path=self.hf_path, name=self.config.subset, split="validation")


# TODO: Grab prompts from https://github.com/openai/simple-evals/blob/main/mgsm_eval.py
class MultilingualMMLU(Task):
    hf_path = "openai/MMMLU"
    # fmt: off
    subsets = [
        "AR-XY", "BN-BD", "DE-DE", "ES-LA", "FR-FR", "HI-IN", "ID-ID", 
        "IT-IT", "JA-JP", "KO-KR", "PT-BR", "SW-KE", "YO-NG", "ZH-CN",
    ]
    # fmt: on

    def __init__(self, config: TaskConfig):
        self.config = config
        self._requests = None

    @property
    def requests(self):
        if self._requests is None:
            dataset = load_dataset(path=self.hf_path, split="validation")
            self._requests = list(map(self._process_instance, dataset, self.config.subset))
        return self._requests

    def _process_instance(self, doc):
        gold_idx = ord(doc["Answer"]) - ord("A")
        choices = [doc["A"], doc["B"], doc["C"], doc["D"]]

        return Instance(
            question=doc["Question"],
            gold_completion=choices[gold_idx],
            choices=choices,
            solution=gold_idx,
            metadata={"id": doc["Unnamed: 0"], "src": doc["src"], "mmlu_subset": doc["Subject"]},
        )
