import logging
import os
import sys
from typing import List, Optional
import requests
from rich.console import Console
from pprint import pprint
import json

logger = logging.getLogger(__name__)

console = Console()

def search_datalake_hashes(model_hash: str, tasks_hash_liststr: str, base_url: str = "https://oe-eval-datalake.allen.ai") -> List[dict]:
    """Search for existing evaluations in the datalake"""
    url = f"{base_url}/bluelake/get-model-task-eval/"
    params = {
        "model_hash": model_hash,
        "task_hash": tasks_hash_liststr,
        "return_fields": "task_idx,num_instances,eval_sha,run_date",
    }
    try:
        response = requests.get(url=url, params=params)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.HTTPError as e:
        raise SystemExit(e)


def download_datalake_files(
    experiment_id: str,
    task_idx: int,
    output_dir: str,
    task_name: str,
    base_url: str = "https://oe-eval-datalake.allen.ai",
    file_types: Optional[List[str]] = None
):
    """Download files from the datalake
    
    Args:
        experiment_id: The experiment ID to download from
        task_idx: The task index
        output_dir: Directory to save files to
        task_name: Name of the task
        base_url: Base URL for the datalake API
        file_types: List of file types to download. Options: 'PREDICTIONS', 'METRICS', 'INPUTS', 'REQUESTS'.
                   If None, downloads all file types.
    """
    
    # Default to all file types if not specified
    if file_types is None:
        file_types = ["PREDICTIONS", "METRICS", "INPUTS"]
    
    # Validate file_types - REQUESTS is not supported by the datalake API
    valid_types = {"PREDICTIONS", "METRICS", "INPUTS", "ALL_METRICS", "METADATA"}
    invalid_types = set(file_types) - valid_types
    if invalid_types:
        raise ValueError(f"Invalid file types: {invalid_types}. Valid types are: {valid_types}")
    
    # Warn about REQUESTS not being supported
    if "REQUESTS" in file_types:
        logger.warning("REQUESTS file type is not supported by the datalake API. Skipping.")
        file_types = [ft for ft in file_types if ft != "REQUESTS"]
    
    # First check if the experiment has any files
    inspect_url = f"{base_url}/greenlake/inspect/{experiment_id}"
    try:
        inspect_response = requests.get(inspect_url)
        if inspect_response.status_code == 200:
            file_counts = inspect_response.json()
            total_files = sum([
                file_counts.get("ALL_METRICS_files", 0),
                file_counts.get("METRICS_files", 0),
                file_counts.get("PREDICTIONS_files", 0),
                file_counts.get("REQUESTS_files", 0),
                file_counts.get("INPUTS_files", 0),
                file_counts.get("METADATA_files", 0)
            ])
            
            if total_files == 0:
                logger.error(f"No files found in datalake for experiment {experiment_id}")
                logger.error("This could mean:")
                logger.error("1. The experiment hasn't been pushed to the datalake yet")
                logger.error("2. The experiment was run locally and files weren't uploaded to Beaker")
                logger.error("3. The experiment ID is incorrect")
                logger.error(f"You can check the experiment status at: {inspect_url}")
                raise Exception(f"No files found in datalake for experiment {experiment_id}")
    except requests.exceptions.RequestException as e:
        logger.warning(f"Could not check experiment status: {e}")
    
    url = f"{base_url}/greenlake/download-result/{experiment_id}"
    params = {"task_idx": task_idx}
    
    output_filenames = {
        "PREDICTIONS": f"{task_name}_predictions.jsonl",
        "METRICS": f"{task_name}_metrics.json",
        "INPUTS": f"{task_name}_recorded_inputs.jsonl",
        "REQUESTS": f"{task_name}_requests.jsonl"
    }
    
    os.makedirs(output_dir, exist_ok=True)
    
    downloaded_files = []
    failed_downloads = []
    
    # Only download the requested file types
    for result_type in file_types:
        filename = output_filenames[result_type]
        output_file = os.path.join(output_dir, filename)
        params["resulttype"] = result_type
        
        logger.info(f"Fetching {result_type} for {output_file}")
        logger.debug(f"Request URL: {url}")
        logger.debug(f"Request params: {params}")
        
        try:
            # Add headers to handle gzip compression properly
            headers = {
                'Accept-Encoding': 'gzip, deflate',
                'User-Agent': 'minieval-datalake-client/1.0'
            }
            
            response = requests.get(url=url, params=params, timeout=30, headers=headers)
            logger.debug(f"Response status: {response.status_code}")
            logger.debug(f"Response headers: {dict(response.headers)}")
            
            if response.status_code == 200:
                try:
                    # Get the content directly - requests handles gzip decompression automatically
                    content = response.content
                    content_length = len(content)
                    logger.debug(f"Response content length: {content_length}")
                    
                    if content_length > 0:
                        with open(output_file, "wb") as f:
                            f.write(content)
                        logger.info(f"Downloaded {output_file}")
                        downloaded_files.append(filename)
                    else:
                        logger.warning(f"Empty response for {result_type}")
                        failed_downloads.append(f"{result_type} (empty response)")
                        
                except requests.exceptions.ChunkedEncodingError as e:
                    logger.error(f"Chunked encoding error for {result_type}: {e}")
                    # Try streaming approach as fallback
                    try:
                        logger.info(f"Retrying with streaming for {result_type}")
                        response = requests.get(url=url, params=params, timeout=30, headers=headers, stream=True)
                        
                        content = b""
                        for chunk in response.iter_content(chunk_size=1024, decode_unicode=False):
                            if chunk:
                                content += chunk
                        
                        if content:
                            with open(output_file, "wb") as f:
                                f.write(content)
                            logger.info(f"Downloaded {output_file} (streaming method)")
                            downloaded_files.append(filename)
                        else:
                            failed_downloads.append(f"{result_type} (chunked encoding error)")
                    except Exception as fallback_e:
                        logger.error(f"Streaming fallback failed for {result_type}: {fallback_e}")
                        failed_downloads.append(f"{result_type} (chunked encoding error)")
                        
            else:
                logger.warning(f"No data found for {result_type} (HTTP {response.status_code})")
                if response.text:
                    logger.debug(f"Response text: {response.text[:500]}...")
                failed_downloads.append(f"{result_type} (HTTP {response.status_code})")
                
        except requests.exceptions.HTTPError as e:
            logger.error(f"Failed to download {result_type}: {e}")
            failed_downloads.append(f"{result_type} ({e})")
        except requests.exceptions.RequestException as e:
            logger.error(f"Request error for {result_type}: {e}")
            failed_downloads.append(f"{result_type} ({e})")
    
    if downloaded_files:
        logger.info(f"Successfully downloaded {len(downloaded_files)} files: {', '.join(downloaded_files)}")
    
    if failed_downloads:
        logger.error(f"Failed to download {len(failed_downloads)} files: {', '.join(failed_downloads)}")
        if not downloaded_files:
            raise Exception(f"Failed to download any files for experiment {experiment_id}, task_idx {task_idx}")
    
    return downloaded_files


def fetch_eval_data(
    eval_sha: str,
    output_dir: Optional[str] = None,
    metrics: bool = False,
    predictions: bool = False,
    requests: bool = False,
    base_url: str = "https://oe-eval-datalake.allen.ai"
):
    """Fetch evaluation data from datalake by eval_sha"""
    if not any([metrics, predictions, requests]):
        raise ValueError("Must specify at least one data type to fetch: metrics, predictions, or requests")
    
    # This is a simplified version - in practice you'd need to implement
    # the full datalake reader logic from the legacy code
    logger.info(f"Fetching eval data for SHA: {eval_sha}")
    
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"Results will be saved to: {output_dir}")
    else:
        logger.info("No output directory specified - will preview first row")
    
    # Implementation would go here based on the legacy fetch.py logic
    # This is a placeholder for the actual implementation
    raise NotImplementedError("Full fetch implementation needs to be completed based on datalake_reader") 


def push_to_datalake(experiment_id: str, tags: str, save_path: str):
    """Push evaluation results to the datalake"""
    from minieval.writers.datalake import DatalakeConfig, DatalakeWriter
    
    config = DatalakeConfig(save_path=save_path, experiment_id=experiment_id, tags=tags)
    writer = DatalakeWriter(config)
    
    try:
        result = writer.push_to_datalake(experiment_id, tags)
        console.print(f"[bold green]Successfully pushed to datalake:[/bold green]")
        pprint(result)
        
        # Check if there's a warning about no files found
        if result.get("warning"):
            console.print(f"\n[bold yellow]Note:[/bold yellow] {result['warning']}")
            console.print("[dim]To test the full workflow, you would need to run evaluations within Beaker experiments.[/dim]")
            console.print(f"[dim]You can inspect the experiment at: https://oe-eval-datalake.allen.ai/greenlake/inspect/{experiment_id}[/dim]")
        
    except Exception as e:
        console.print(f"[bold red]Failed to push to datalake:[/bold red] {e}")
        sys.exit(1)


def pull_from_datalake(experiment_id: str, task_idx: int, output_dir: str, task_name: str, file_types: list = None):
    """Pull evaluation results from the datalake"""    
    # Strip formatting suffix from task name (e.g., arc_easy:mc -> arc_easy)
    clean_task_name = task_name.split(':')[0]
    
    try:
        downloaded_files = download_datalake_files(experiment_id, task_idx, output_dir, clean_task_name, file_types=file_types)
        if downloaded_files:
            console.print(f"[bold green]Successfully downloaded {len(downloaded_files)} files to:[/bold green] {output_dir}")
            for filename in downloaded_files:
                console.print(f"  - {filename}")
        else:
            console.print(f"[bold yellow]No files were downloaded for experiment {experiment_id}[/bold yellow]")
    except Exception as e:
        console.print(f"[bold red]Failed to pull from datalake:[/bold red] {e}")
        sys.exit(1)


def consolidate_metrics(save_path: str, model_config_file: str = None):
    """Consolidate individual metrics files into summary files"""
    from minieval.writers.datalake import DatalakeWriter, DatalakeConfig
    
    # Load model config if provided
    model_config = {}
    if model_config_file and os.path.exists(model_config_file):
        try:
            with open(model_config_file, "r") as f:
                model_config = json.load(f)
        except Exception as e:
            console.print(f"[bold yellow]Warning: Could not load model config from {model_config_file}: {e}[/bold yellow]")
    
    # Create DatalakeWriter instance
    config = DatalakeConfig(save_path=save_path)
    writer = DatalakeWriter(config)
    
    try:
        writer.finalize_metrics(model_config)
        console.print(f"[bold green]Successfully consolidated metrics in:[/bold green] {save_path}")
        console.print("  - metrics-all.jsonl")
        console.print("  - metrics.json")
    except Exception as e:
        console.print(f"[bold red]Failed to consolidate metrics:[/bold red] {e}")
        sys.exit(1)


def fetch_eval_data(eval_sha: str, output_dir: str = None, metrics: bool = False, predictions: bool = False, requests: bool = False):
    """Fetch evaluation data by SHA"""
    from minieval.writers.datalake import fetch_eval_data as fetch_func
    
    try:
        fetch_func(eval_sha, output_dir, metrics, predictions, requests)
        console.print(f"[bold green]Successfully fetched data for SHA:[/bold green] {eval_sha}")
    except Exception as e:
        console.print(f"[bold red]Failed to fetch data:[/bold red] {e}")
        sys.exit(1)

def add_datalake_args(subparsers):
    """Add datalake-related arguments to the argument parser"""
    # Push command
    push_parser = subparsers.add_parser('push', help='Push results to datalake')
    push_parser.add_argument("--experiment-id", required=True, help="Beaker experiment ID")
    push_parser.add_argument("--tags", help="Comma-delimited tags for datalake")
    push_parser.add_argument("--save-path", help="Path to results files")
    
    # Pull command
    pull_parser = subparsers.add_parser('pull', help='Pull results from datalake')
    pull_parser.add_argument("--experiment-id", required=True, help="Beaker experiment ID")
    pull_parser.add_argument("--task-idx", type=int, required=True, help="Task index")
    pull_parser.add_argument("--output-dir", required=True, help="Output directory")
    pull_parser.add_argument("--task-name", required=True, help="Task name")
    pull_parser.add_argument("--file-types", nargs='+', choices=['predictions', 'metrics', 'inputs', 'all_metrics', 'metadata'], 
                            help="File types to download. Options: predictions, metrics, inputs, all_metrics, metadata. Default: predictions, metrics, inputs")
    
    # Consolidate command
    consolidate_parser = subparsers.add_parser('consolidate', help='Consolidate individual metrics files')
    consolidate_parser.add_argument("--save-path", required=True, help="Path to directory containing metrics files")
    consolidate_parser.add_argument("--model-config", help="Path to model config JSON file (optional)")
    
    # Fetch command
    fetch_parser = subparsers.add_parser('fetch', help='Fetch data by eval SHA')
    fetch_parser.add_argument("--sha", required=True, help="Evaluation SHA")
    fetch_parser.add_argument("--output-dir", help="Output directory (optional)")
    fetch_parser.add_argument("--metrics", action="store_true", help="Fetch metrics")
    fetch_parser.add_argument("--predictions", action="store_true", help="Fetch predictions")
    fetch_parser.add_argument("--requests", action="store_true", help="Fetch requests")


def handle_datalake_command(args):
    """Handle datalake-related commands"""
    if args.command == 'push':
        push_to_datalake(args.experiment_id, args.tags, args.save_path)
    elif args.command == 'pull':
        # Convert file types to uppercase if provided
        file_types = None
        if hasattr(args, 'file_types') and args.file_types:
            file_types = [ft.upper() for ft in args.file_types]
        pull_from_datalake(args.experiment_id, args.task_idx, args.output_dir, args.task_name, file_types)
    elif args.command == 'consolidate':
        consolidate_metrics(args.save_path, getattr(args, 'model_config', None))
    elif args.command == 'fetch':
        fetch_eval_data(args.sha, args.output_dir, args.metrics, args.predictions, args.requests)


def main():
    """Main entrypoint for datalake CLI"""
    import argparse
    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers(dest='command', required=True)
    
    add_datalake_args(subparsers)
    
    args = parser.parse_args()
    handle_datalake_command(args)


if __name__ == '__main__':
    main()