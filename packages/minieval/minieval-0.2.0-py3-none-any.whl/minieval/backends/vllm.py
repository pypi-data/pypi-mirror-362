import os
from typing import Any, Dict, List, Optional

def quiet_vllm_logger():
    os.environ["VLLM_LOGGING_LEVEL"] = "WARNING"
    os.environ["VLLM_LOG_LEVEL"] = "WARNING"

try:
    quiet_vllm_logger()
    from vllm import LLM, SamplingParams as VLLMSamplingParams
    from vllm.outputs import CompletionOutput, RequestOutput
except ImportError:
    raise ImportError("vLLM is required for VLLMBackend. Install with: pip install minieval[vllm]")

from ..datatypes import LMOutput, LMRequest, SamplingParams


class VLLMBackend:
    def __init__(self, model_path: str, **kwargs):
        self.model_path = model_path
        self.llm = LLM(model=model_path, **kwargs)

    def generate(
        self, requests: List[LMRequest], sampling_params: SamplingParams = None
    ) -> List[List[LMOutput]]:
        # Build vLLM sampling params from our SamplingParams
        vllm_params = {}
        
        # Set default values
        vllm_params["logprobs"] = sampling_params.logprobs if sampling_params.logprobs is not None else 5
        
        # Map parameter names to vLLM format
        if sampling_params.max_gen_toks is not None:
            vllm_params["max_tokens"] = sampling_params.max_gen_toks
        
        if sampling_params.temperature is not None:
            vllm_params["temperature"] = sampling_params.temperature
            
        if sampling_params.top_p is not None:
            vllm_params["top_p"] = sampling_params.top_p
            
        if sampling_params.top_k is not None:
            vllm_params["top_k"] = sampling_params.top_k
        
        # Handle repeats parameter
        if sampling_params.repeats is not None:
            vllm_params["n"] = sampling_params.repeats
        
        # Handle stop sequences
        if sampling_params.stop_sequences is not None:
            vllm_params["stop"] = sampling_params.stop_sequences

        vllm_sampling_params = VLLMSamplingParams(**vllm_params)
        
        contexts: list[str] = [req.context for req in requests]
        outputs: list[RequestOutput] = self.llm.generate(contexts, vllm_sampling_params)

        results = []
        for output in outputs:
            output_results = []
            completions: list[CompletionOutput] = output.outputs
            for completion in completions:
                output_results.append(LMOutput(text=completion.text, logprobs=completion.logprobs))
            results.append(output_results)

        return results

    def logprobs(
        self,
        requests: List[LMRequest],
    ) -> List[List[LMOutput]]:
        # Build vLLM sampling params from our SamplingParams
        vllm_params = {
            "logprobs": 5,
            "max_tokens": 1
        }

        # Create prompts for each request and continuation pair
        full_prompts: list[str] = []
        for req in requests:
            for cont in req.continuation:
                full_prompts.append(req.context + cont)

        vllm_sampling_params = VLLMSamplingParams(**vllm_params)
        outputs: list[RequestOutput] = self.llm.generate(full_prompts, vllm_sampling_params)

        # Group results by request
        results = []
        idx = 0
        for req in requests:
            output_results = []
            # Process each continuation for this request
            for _ in req.continuation:
                output = outputs[idx]
                completions: list[CompletionOutput] = output.outputs
                for completion in completions:
                    output_results.append(
                        LMOutput(text=completion.text, logprobs=completion.logprobs)
                    )
                idx += 1
            results.append(output_results)

        return results
