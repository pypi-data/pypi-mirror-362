import os
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Dict, List, Optional

from tqdm import tqdm

try:
    import litellm
except ImportError:
    raise ImportError(
        "litellm is required for LiteLLMBackend. Install with: pip install minieval[litellm]"
    )

from ..datatypes import LMOutput, LMRequest, SamplingParams


class LiteLLMBackend:
    API_KEY_MAPPINGS = {
        "OPENAI_API_KEY": "openai_api_key",
        "ANTHROPIC_API_KEY": "anthropic_api_key",
        "COHERE_API_KEY": "cohere_api_key",
        "REPLICATE_API_TOKEN": "replicate_api_token",
        "HUGGINGFACE_API_KEY": "huggingface_api_key",
        "TOGETHER_API_KEY": "together_api_key",
        "AZURE_API_KEY": "azure_api_key",
        "AZURE_API_BASE": "azure_api_base",
        "AZURE_API_VERSION": "azure_api_version",
        "BEDROCK_AWS_ACCESS_KEY_ID": "aws_access_key_id",
        "BEDROCK_AWS_SECRET_ACCESS_KEY": "aws_secret_access_key",
        "BEDROCK_AWS_REGION_NAME": "aws_region_name",
    }

    def __init__(self, model_name: str, **kwargs):
        self.model_name = model_name
        self.kwargs = kwargs
        self._setup_api_keys()

    def _setup_api_keys(self):
        for env_var, litellm_key in self.API_KEY_MAPPINGS.items():
            value = os.getenv(env_var)
            if value:
                setattr(litellm, litellm_key, value)

    def generate(
        self, requests: List[LMRequest], sampling_params: SamplingParams = None
    ) -> List[List[LMOutput]]:
        # Extract parameters from SamplingParams
        if sampling_params:
            n = sampling_params.repeats if sampling_params.repeats is not None else 1
            max_completion_tokens = sampling_params.max_gen_toks
            temperature = sampling_params.temperature
            stop_sequences = sampling_params.stop_sequences
            logprobs = sampling_params.logprobs if sampling_params.logprobs is not None else True
        else:
            n = 1
            max_completion_tokens = None
            temperature = None
            stop_sequences = None
            logprobs = True

        def _generate_single(request):
            try:
                # Build messages from request context
                messages = [{"role": "user", "content": request.context}]
                
                response = litellm.completion(
                    model=self.model_name,
                    messages=messages,
                    n=n,
                    max_completion_tokens=max_completion_tokens,
                    temperature=temperature,
                    stop=stop_sequences,
                    logprobs=logprobs,
                    **self.kwargs,
                )

                results = []
                for choice in response.choices:
                    text = choice.message.content if choice and choice.message else ""
                    logprobs_data = getattr(choice, "logprobs", None)
                    results.append(LMOutput(text=text or "", logprobs=logprobs_data))
                return results

            except Exception as e:
                return [LMOutput(text=f"Error: {str(e)}", logprobs=None)]

        with ThreadPoolExecutor() as executor:
            results = list(
                tqdm(
                    executor.map(_generate_single, requests),
                    total=len(requests),
                    desc="Generating responses",
                )
            )

        return results

    def logprobs(
        self,
        requests: List[LMRequest],
    ) -> List[List[LMOutput]]:
        # Single pass to estimate logprobs
        n = 1
        max_completion_tokens = 1

        def _logprobs_single(request):
            try:
                results = []
                for continuation in request.continuation:
                    # Build messages with continuation
                    messages = [{"role": "user", "content": request.context + continuation}]

                    response = litellm.completion(
                        model=self.model_name,
                        messages=messages,
                        n=n,
                        max_completion_tokens=max_completion_tokens,
                        **self.kwargs,
                    )

                    for choice in response.choices:
                        logprobs_data = getattr(choice, "logprobs", None)
                        if logprobs_data and hasattr(logprobs_data, "content"):
                            results.append(LMOutput(text=continuation, logprobs=logprobs_data.content if logprobs_data.content else []))
                        else:
                            results.append(LMOutput(text=continuation, logprobs=[]))
                return results

            except Exception as e:
                return [LMOutput(text=f"Error: {str(e)}", logprobs=None)]

        with ThreadPoolExecutor() as executor:
            results = list(
                tqdm(
                    executor.map(_logprobs_single, requests),
                    total=len(requests),
                    desc="Generating logprobs",
                )
            )

        return results
