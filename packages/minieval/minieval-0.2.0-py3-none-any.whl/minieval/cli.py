import os
import sys
from dataclasses import dataclass

from rich.console import Console
from rich.pretty import pprint

from minieval.backends import Backend, init_backend, init_template
from minieval.datatypes import (
    Formatter,
    Instance,
    LauncherConfig,
    LMOutput,
    LMRequest,
    LauncherType,
    Metric,
    ModelConfig,
    RequestType,
    Response,
    Scorer,
    TaskConfig,
)
from minieval.launchers import WriterType, init_writer
from minieval.utils import apply_overrides

console = Console()


@dataclass
class RunnerConfig:
    backend: Backend
    model: ModelConfig
    tasks: list[TaskConfig]
    launcher: LauncherConfig


class EvalRunner:
    def __init__(self, config: RunnerConfig):
        self.config = config

        self.llm = init_backend(backend_type=self.config.backend, model_name=self.config.model.name)

        self.template_func = init_template(
            backend_type=self.config.backend, model_name=self.config.model.name
        )

        # @davidh also need to refactor with new writer
        experiment_id = os.getenv("BEAKER_EXPERIMENT_ID", None)
        if experiment_id:
            writer_type = WriterType.datalake
        else:
            writer_type = WriterType.local

        self.writer = init_writer(config=self.config.launcher, writer_type=writer_type)

    def run(self, task_config: TaskConfig):
        console.rule(f"[bold red]{task_config.alias}")

        from minieval.task_registry import TaskRegistry

        TaskClass = TaskRegistry.get_task(task_config.alias)

        task = TaskClass(config=task_config)

        instances: list[Instance] = task.requests
        _: list[Instance] = task.build_few_shot()

        if task_config.limit:
            assert task_config.limit <= len(
                instances
            ), f"Limit is larger than the dataset! {task_config.limit=}, {len(instances)=}"
            instances = instances[: task_config.limit]

        formatter: Formatter = task_config.formatter
        scorers: list[Scorer] = task_config.scorer
        metrics: list[Metric] = task_config.metric

        messages: list[LMRequest] = formatter.build_messages(instances)

        requests: list[LMRequest] = formatter.build_requests(self.template_func, messages)

        if formatter.REQUEST_TYPE == RequestType.GENERATE:
            generations: list[LMOutput] = self.llm.generate(
                requests, sampling_params=task_config.sampling_params
            )
        elif formatter.REQUEST_TYPE == RequestType.LOGPROBS:
            generations: list[LMOutput] = self.llm.logprobs(requests)

        generations: list[LMOutput] = task.extract_answers(generations)

        responses: list[Response] = []
        for inst, req, gen in zip(instances, requests, generations):
            responses += [Response(input=inst, request=req, output=gen)]

        for score in scorers:
            responses = score.score_responses(responses)

        for metric in metrics:
            responses = metric.compute_metrics(responses)

        pprint(responses[0], max_string=1000)

        dataset_metrics = self.reduce_metrics(responses)

        console.print(f"[dim]─── results ({task_config.alias}) ───[/dim]")

        pprint(dataset_metrics, expand_all=True)

        self.writer.save_responses(task_alias=task_config.alias, responses=responses)

        self.writer.save_metrics(task_alias=task_config.alias, metrics=dataset_metrics)

        return dataset_metrics

    def reduce_metrics(self, responses: list[Response]) -> dict:
        def average_dict(dicts: list[dict]) -> dict:
            """Recursively average entries in a list of dicts"""
            result = {}
            first = dicts[0]

            for key in first:
                if isinstance(first[key], dict):
                    values = [d[key] for d in dicts]
                    result[key] = average_dict(values)
                else:
                    values = [float(d[key]) for d in dicts]
                    result[key] = sum(values) / len(values)

            return result

        all_scores = [response.scores for response in responses]
        
        averaged_metrics = average_dict(all_scores)
        
        # ensure there is a primary_score key
        if "primary_score" not in averaged_metrics:
            averaged_metrics["primary_score"] = None
        
        return averaged_metrics

    def evaluate(self):
        all_metrics = {}
        for task_config in self.config.tasks:
            all_metrics[task_config.alias] = self.run(task_config)
        
        self.writer.write_finalized_metrics(self.config)
        
        return all_metrics


def run_eval(
    aliases: list[str], 
    backend: str, 
    model_name: str, 
    launch_type: LauncherType = LauncherType.LOCAL):
    from minieval.task_registry import TaskRegistry

    TaskRegistry()  # initialize the registry

    # Init task config from registry
    tasks = []
    for alias in aliases:
        config: TaskConfig = TaskRegistry.get_config(alias)
        tasks.append(config)

    # Init model config
    model = ModelConfig(name=model_name)

    # Init launcher config
    match launch_type:
        case LauncherType.LOCAL:
            launcher = LauncherConfig()
        case LauncherType.BEAKER:
            from minieval.launchers.beaker.launcher import BeakerConfig
            launcher = BeakerConfig(
                description=f"{len(aliases)} task(s): {model_name} on {' '.join(aliases)}",
                task_name=f"minieval_{model_name}"
            )

            # @davidh -- Datalake upload will be used if we are on remote. TODO is separate the
            # launcher from the writer
            experiment_id = os.getenv("BEAKER_EXPERIMENT_ID", None)
            if experiment_id:
                from minieval.writers.datalake import DatalakeConfig
                launcher = DatalakeConfig(
                    experiment_id=experiment_id,
                    task_idx=0 # updated during execution
                )

    config = RunnerConfig(
        backend=backend, 
        model=model, 
        tasks=tasks, 
        launcher=launcher
    )

    config: RunnerConfig = apply_overrides(config)

    pprint(config, expand_all=True)

    # If local, exit here with a gantry command
    # If on beaker, we will continue (@davidh might not be the best design if we want
    # to be able to launch jobs inside non-minieval beaker jobs)
    if launch_type == LauncherType.BEAKER and not os.getenv("BEAKER_EXPERIMENT_ID"):
        from minieval.launchers.beaker.launcher import launch_gantry
        launch_gantry(config.launcher)
        return

    runner = EvalRunner(config)

    all_metrics = runner.evaluate()

    console.print("[dim]─── all results ───[/dim]")

    pprint(all_metrics, expand_all=True)

    # Auto-push to datalake if running in Beaker environment
    if experiment_id and os.getenv("BEAKER_EXPERIMENT_ID"):
        console.print("\n[dim]─── pushing to datalake ───[/dim]")
        try:
            from minieval.writers.datalake import DatalakeWriter
            writer = DatalakeWriter(config.launcher)
            result = writer.push_to_datalake(experiment_id, writer.config.tags)
            console.print(f"[bold green]Successfully pushed to datalake:[/bold green]")
            pprint(result)
        except Exception as e:
            console.print(f"[bold red]Failed to push to datalake:[/bold red] {e}")
            # Don't exit here - we want the evaluation results even if push fails


def main():
    import argparse

    parser = argparse.ArgumentParser(description="""minieval - A deviously simple eval library""")
    parser.add_argument("-t", "--tasks", nargs="+", help="Task aliases to evaluate")
    parser.add_argument("-m", "--model", default="mock", help="Model name/path to evaluate")
    parser.add_argument(
        "-b", "--backend", default="mock", help="Backend to use (mock, vllm, litellm, ollama)"
    )
    parser.add_argument(
        "-l", "--launcher", default="local", help="Launcher to use (local, beaker)"
    )
    parser.add_argument("--list", action="store_true", help="List available tasks")

    if len(sys.argv) == 1:
        name = r"""
           _       _                 _   Examples:
          (_)     (_)               | |  -> minieval --list
 _ __ ___  _ _ __  _  _____   ____ _| |  -> minieval -t minerva:cot -m mock
| '_ ` _ \| | '_ \| |/ _ \ \ / / _` | |  -> minieval -t arc_challenge:rc -m allenai/OLMo-2-0425 -b vllm
| | | | | | | | | | |  __/\ V / (_| | |  -> minieval -t mmlu:cot -m Qwen/Qwen3-4B -b vllm -l beaker
|_| |_| |_|_|_| |_|_|\___| \_/ \__,_|_|  -> minieval -t aime:selfc -m gpt-4.1-nano -b litellm

"""
        # don't print beyond terminal width
        term_width = os.get_terminal_size().columns
        lines = name[1:].split('\n')
        truncated = '\n'.join(line[:term_width] for line in lines)
        
        sys.stdout.write('\033[35m' + truncated + '\033[0m')
        parser.print_help()
        sys.exit(1)

    # Allow unknown arguments to be passed as overrides
    args, unknown = parser.parse_known_args()

    if args.list:
        from minieval.task_registry import TaskRegistry

        pprint(TaskRegistry.names())
        return

    if not args.tasks:
        parser.error("the following arguments are required: -t/--tasks")

    # Override defaults with CLI args
    aliases = args.tasks
    backend = args.backend
    model_name = args.model
    launch_type = args.launcher

    # Special alias "all", to run on every task in the registry
    if "all" in aliases:
        from minieval.task_registry import TaskRegistry
        aliases = TaskRegistry.names()

    # Get launcher type
    try:
        launch_type = LauncherType(launch_type.lower())
    except ValueError:
        parser.error(f"invalid launcher type '{launch_type}'. Must be one of: {[t.value for t in LauncherType]}")

    run_eval(
        aliases,
        backend,
        model_name,
        launch_type
    )


if __name__ == "__main__":
    main()