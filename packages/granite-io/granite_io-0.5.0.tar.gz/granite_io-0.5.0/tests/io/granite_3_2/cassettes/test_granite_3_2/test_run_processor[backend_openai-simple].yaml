interactions:
- request:
    body: '{"model":"granite3.2:2b","prompt":"<|start_of_role|>system<|end_of_role|>Knowledge
      Cutoff Date: April 2024.\nToday''s Date: March 11, 2025.\nYou are Granite, developed
      by IBM. You are a helpful AI assistant.<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>Hello,
      how are you?<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>I''m doing
      great. How can I help you today?<|end_of_text|>\n<|start_of_role|>user<|end_of_role|>I''d
      like to show off how chat templating works!<|end_of_text|>\n<|start_of_role|>assistant<|end_of_role|>","best_of":1,"n":1}'
    headers:
      accept:
      - application/json
      accept-encoding:
      - gzip, deflate
      connection:
      - keep-alive
      content-length:
      - '556'
      content-type:
      - application/json
      host:
      - localhost:11434
      rits_api_key:
      - ollama
      user-agent:
      - OpenAI/Python 1.65.5
      x-stainless-arch:
      - arm64
      x-stainless-async:
      - 'false'
      x-stainless-lang:
      - python
      x-stainless-os:
      - MacOS
      x-stainless-package-version:
      - 1.65.5
      x-stainless-read-timeout:
      - '600'
      x-stainless-retry-count:
      - '0'
      x-stainless-runtime:
      - CPython
      x-stainless-runtime-version:
      - 3.12.5
    method: POST
    uri: http://localhost:11434/v1/completions
  response:
    content: "{\"id\":\"cmpl-457\",\"object\":\"text_completion\",\"created\":1741680399,\"model\":\"granite3.2:2b\",\"system_fingerprint\":\"fp_ollama\",\"choices\":[{\"text\":\"Absolutely,
      let's dive into chat templating! Chat templates allow us to create dynamic responses
      based on input or predefined conditions. They're incredibly useful for maintaining
      conversational flow and consistency in our AI-driven conversations. Here\u2019s
      a simple demonstration using a template-based approach:\\n\\n1. **Template Definition:**\\n
      \  Let's define a basic conversation template for responding to questions about
      the weather.\\n\\n   ```\\n   Hello! How can I assist you with the weather?\\n\\n
      \  It appears we're in [Location] at this moment, so let's check that. Here\u2019s
      what I know:\\n   \\n   - **Temperature**: Around [\xB0C] degrees.\\n   - **Condition**:
      Most likely, it\u2019s [Weather Condition], possibly with a chance of [Chance].\\n
      \  \\n   Are there any specific aspects of the weather you're curious about?
      If you wish to ask about another location or time, feel free.\\n   ```\\n\\n2.
      **Input and Rendering:**\\n\\n   Now, we'll simulate this template:\\n\\n   **User
      Input (Question):** \\\"Weather in New York today.\\\"\\n\\n   Applying our
      defined template with this input yields a dynamic response. Here\u2019s how
      it might look:\\n\\n   - `It appears we're in [Location]` -\\u003e Changes to
      `It appears we're in New York.`\\n   \\n   - **`Here's what I know:`**\\n   -
      **Temperature**: Around 25 degrees. (Static temperature data)\\n   - **Condition**:
      Most likely, it\u2019s cloudy.\\n   \\n   - `Are there any specific aspects
      of the weather you're curious about? If you wish to ask about another location
      or time, feel free.`\\n   \\n   This dynamic response maintains a consistent
      structure while adapting real-time data (like temperature and conditions) for
      each inputted query. It's powerful because templates allow for easy customization
      without writing individual responses from scratch every time.\",\"index\":0,\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":142,\"completion_tokens\":389,\"total_tokens\":531}}\n"
    headers:
      Content-Type:
      - application/json
      Date:
      - Tue, 11 Mar 2025 08:06:39 GMT
      Transfer-Encoding:
      - chunked
    http_version: HTTP/1.1
    status_code: 200
version: 1
