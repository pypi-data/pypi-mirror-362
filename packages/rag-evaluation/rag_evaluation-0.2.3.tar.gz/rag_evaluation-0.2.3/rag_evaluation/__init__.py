"""
RAG Evaluation package

Provides tools for evaluating Retrieval-Augmented Generation (RAG) projects
by scoring generated responses across various metrics.
"""

from .config import get_api_key
from .config import set_api_key
from .evaluator import (
    evaluate_all,
    evaluate_response,
    evaluate_openai,
    evaluate_gemini,
)
from .metrics import (
    EVALUATION_PROMPT_TEMPLATE,
    QUERY_RELEVANCE_CRITERIA,
    QUERY_RELEVANCE_STEPS,
    FACTUAL_ACCURACY_CRITERIA,
    FACTUAL_ACCURACY_STEPS,
    COVERAGE_CRITERIA,
    COVERAGE_STEPS,
    COHERENCE_SCORE_CRITERIA,
    COHERENCE_SCORE_STEPS,
    FLUENCY_SCORE_CRITERIA,
    FLUENCY_SCORE_STEPS,
)
from .utils import normalize_score, calculate_weighted_accuracy, generate_report

# define a version string and other metadata.
__version__ = "0.2.3"

__all__ = [
    "get_api_key",
    "set_api_key",
    "evaluate_all",
    "evaluate_response",
    "normalize_score",
    "evaluate_openai",
    "evaluate_gemini",
    "calculate_weighted_accuracy",
    "generate_report",
    "EVALUATION_PROMPT_TEMPLATE",
    "QUERY_RELEVANCE_CRITERIA",
    "QUERY_RELEVANCE_STEPS",
    "FACTUAL_ACCURACY_CRITERIA",
    "FACTUAL_ACCURACY_STEPS",
    "COVERAGE_CRITERIA",
    "COVERAGE_STEPS",
    "COHERENCE_SCORE_CRITERIA",
    "COHERENCE_SCORE_STEPS",
    "FLUENCY_SCORE_CRITERIA",
    "FLUENCY_SCORE_STEPS",
]
