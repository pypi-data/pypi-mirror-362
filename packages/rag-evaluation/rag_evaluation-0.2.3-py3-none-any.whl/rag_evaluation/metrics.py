"""
Module defining evaluation metrics, criteria, steps, and the prompt template.
"""

EVALUATION_PROMPT_TEMPLATE = """
You will be given a user query, a response generated by an LLM, and the knowledge source document that was used to generate the response. Your task is to evaluate the response on one specific metric.

Scoring Scale (1–5):
1: Poor performance (significant issues; criteria and steps not met).
2: Fair (partial alignment with criteria and steps).
3: Good (mostly accurate, with some minor issues in alignment with criteria and steps).
4: Very Good (high alignment with criteria and steps, with only minor inconsistencies).
5: Excellent (flawless performance; all criteria and steps fully met).

Evaluation Criteria:

{criteria}

Evaluation Steps:

{steps}

Example:

User Query:

{query}

Source Document:

{document}

Generated Response:

{response}

Evaluation Form (scores ONLY):

- {metric_name}

Please provide only a single integer between 1 and 5 as your answer based on the evaluation metric's criteria and steps, with no additional text
"""

QUERY_RELEVANCE_CRITERIA = """
Relevance (1-5): Measures how well the generated response addresses the user’s query.
A relevant response answers the query directly and avoids unrelated or redundant information.
"""

QUERY_RELEVANCE_STEPS = """
1.  Read the user query, source document, and generated response.
2.  Identify the main intent of the user query.
3.  Assess whether the generated response adequately addresses the user's question.
4.  Assign a relevance score from 1 to 5.
"""

FACTUAL_ACCURACY_CRITERIA = """
Accuracy (1-5): Determines if the response is factually accurate, without any errors, based on the content in the source document.
"""

FACTUAL_ACCURACY_STEPS = """
1.  Compare the generated response with the source document.
2.  Identify any factual discrepancies, incorrect details, or hallucinations.
3.  Assign an accuracy score from 1 to 5 based on how well the response aligns with the facts in the source.
4.  Penalize if the response contains any facts or references not found in the source document.
"""

COVERAGE_CRITERIA = """
Coverage (1-5): Evaluate whether the response captures all the key points from the source document that are relevant to the query.
Penalize if important details from the source are missing.
"""

COVERAGE_STEPS = """
1.  Read the user query and the source document.
2.  Check if the generated response covers all essential points from the source that answer the query.
3.  Assign a coverage score from 1 to 5.
4.  Review and penalize if essential details from the source are missing.
"""

COHERENCE_SCORE_CRITERIA = """
Coherence (1-5): The response should be well-organized and logically structured, with the ideas flowing naturally.
The integration of information from the source should make sense as a cohesive answer to the query.
"""

COHERENCE_SCORE_STEPS = """
1.  Read the response carefully and assess how well the ideas are structured.
2.  Check whether the information from the source document has been combined into a coherent response.
3.  Assign a coherence score from 1 to 5.
"""

FLUENCY_SCORE_CRITERIA = """
Fluency (1-5): Evaluates the quality of the language used, including grammar, punctuation, and sentence structure.
"""

FLUENCY_SCORE_STEPS = """
1.  Read the response and evaluate its readability, grammatical correctness, and fluency.
2.  Assign a fluency score from 1 to 5.
"""
