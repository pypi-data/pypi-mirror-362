Metadata-Version: 2.4
Name: ocr_translate_ollama
Version: 0.2.0
Summary: Plugin to implement ollama (LLMs) based translations for ocr_translate
Keywords: django,translation
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3.10
License-File: LICENSE.txt
Requires-Dist: requests
Requires-Dist: django-ocr_translate>=0.2.1,<1.0
Requires-Dist: pre-commit~=3.3.3 ; extra == "pre-commit"
Requires-Dist: pylint~=2.17.4 ; extra == "pre-commit"
Requires-Dist: pylint-pytest~=1.1.2 ; extra == "pre-commit"
Requires-Dist: pylint-django~=2.5.3 ; extra == "pre-commit"
Requires-Dist: flit ; extra == "release"
Requires-Dist: pytest ; extra == "tests"
Requires-Dist: pytest-cov ; extra == "tests"
Requires-Dist: pytest-django~=4.5.2 ; extra == "tests"
Project-URL: Source, https://github.com/Crivella/ocr_translate-ollama
Provides-Extra: pre-commit
Provides-Extra: release
Provides-Extra: tests

# Plugin ocr_translate_ollama

This is a plugin for [ocr_translate](https://github.com/Crivella/ocr_translate) that implements translations through [ollama](https://github.com/ollama/ollama) using Large Language Models (LLM)s.

## Usage

For versions of the server `<v0.6`

- Install this by running `pip install ocr_translate_ollama`
- Add `ocr_translate_ollama` to your `INSTALLED_APPS` in `settings.py`
- Run the server with `AUTOCREATE_VALIDATED_MODELS` once

For versions of the server `>=0.6`

- Install through the server plugin manager

## Ollama name prefixes

To avoid cluttering the ollama server (or having to set a dedicated one), the plugin will create new models from the original one (with a dedicated system prompt) adding the prefix `oct_ollama_` to the model names.

This behavior can be changed by setting the environment variable `OCT_OLLAMA_PREFIX` to a different value.

**NOTE**: The final `_` is added by the plugin, so it should not be included in the prefix.

## IMPORTANT

[Ollama](https://github.com/ollama/ollama) needs to be installed separately and reachable from the server (check the link for instructions).
The environment variable `OCT_OLLAMA_ENDPOINT` should be set to the endpoint of the ollama server (including the `/api`).

Example:

```bash
export OLLAMA_ENDPOINT=http://localhost:11434/api
```

Depending on the RAM available on your system (CPU/GPU), you also may need to tune the variables

- `OLLAMA_MAX_LOADED_MODELS`
- `OLLAMA_NUM_PARALLEL`

when running the server.

For more information, check the [ollama FAQ](https://github.com/ollama/ollama/blob/main/docs/faq.md)

