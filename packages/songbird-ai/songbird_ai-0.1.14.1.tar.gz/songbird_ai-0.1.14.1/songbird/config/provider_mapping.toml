# Songbird LiteLLM Provider Mapping Configuration
# This file defines how Songbird's provider names map to LiteLLM model strings

[defaults]
# Default models for each provider - used when no specific model is requested
openai = "openai/gpt-4o"
claude = "anthropic/claude-3-5-sonnet-20241022"
gemini = "gemini/gemini-2.0-flash-001"
ollama = "ollama/qwen2.5-coder:7b"
openrouter = "deepseek/deepseek-chat-v3-0324:free"
copilot = "gpt-4o"

[urls]
# Custom API base URLs for providers that need them
openrouter = "https://openrouter.ai/api/v1"
ollama = "http://localhost:11434"
# Add other providers as needed
# together = "https://api.together.xyz"
# groq = "https://api.groq.com/openai/v1"

# Model mappings per provider - maps Songbird model names to LiteLLM model strings
[models.openai]
"gpt-4o" = "openai/gpt-4o"
"gpt-4o-mini" = "openai/gpt-4o-mini"
"gpt-4-turbo" = "openai/gpt-4-turbo"
"gpt-4" = "openai/gpt-4"
"gpt-3.5-turbo" = "openai/gpt-3.5-turbo"

[models.claude]
"claude-3-5-sonnet-20241022" = "anthropic/claude-3-5-sonnet-20241022"
"claude-3-5-haiku-20241022" = "anthropic/claude-3-5-haiku-20241022"
"claude-3-opus-20240229" = "anthropic/claude-3-opus-20240229"
"claude-3-sonnet-20240229" = "anthropic/claude-3-sonnet-20240229"
"claude-3-haiku-20240307" = "anthropic/claude-3-haiku-20240307"

[models.gemini]
"gemini-2.0-flash-001" = "gemini/gemini-2.0-flash-001"
"gemini-2.0-flash" = "gemini/gemini-2.0-flash"
"gemini-1.5-flash" = "gemini/gemini-1.5-flash"
"gemini-2.0-flash-exp" = "gemini/gemini-2.0-flash-exp"

[models.ollama]
# Note: Ollama models are dynamic and discovered at runtime
# These are common models that users might have
"qwen2.5-coder:7b" = "ollama/qwen2.5-coder:7b"
"qwen2.5-coder:14b" = "ollama/qwen2.5-coder:14b"
"qwen2.5-coder:32b" = "ollama/qwen2.5-coder:32b"
"llama3.2:latest" = "ollama/llama3.2:latest"
"codellama:latest" = "ollama/codellama:latest"
"deepseek-coder:6.7b" = "ollama/deepseek-coder:6.7b"
"phi3:latest" = "ollama/phi3:latest"
"mistral:latest" = "ollama/mistral:latest"

[models.openrouter]
# OpenRouter models with tool calling support (dynamic discovery preferred)
"anthropic/claude-3.5-sonnet" = "anthropic/claude-3.5-sonnet"
"openai/gpt-4o" = "openai/gpt-4o"
"openai/gpt-4o-mini" = "openai/gpt-4o-mini"
"deepseek/deepseek-chat-v3-0324:free" = "deepseek/deepseek-chat-v3-0324:free"
"meta-llama/llama-3.2-90b-vision-instruct" = "meta-llama/llama-3.2-90b-vision-instruct"
"mistralai/mistral-large-2407" = "mistralai/mistral-large-2407"

[models.copilot]
# GitHub Copilot models (using custom provider)
"gpt-4o" = "gpt-4o"
"gpt-4o-mini" = "gpt-4o-mini"
"gpt-4" = "gpt-4"
"gpt-4-turbo" = "gpt-4-turbo"
"claude-3.5-sonnet" = "claude-3.5-sonnet"
"claude-3.5-haiku" = "claude-3.5-haiku"
"claude-3-opus" = "claude-3-opus"
"claude-3-sonnet" = "claude-3-sonnet"
"claude-3-haiku" = "claude-3-haiku"
"gemini-1.5-pro" = "gemini-1.5-pro"
"gemini-1.5-flash" = "gemini-1.5-flash"
"o1-preview" = "o1-preview"
"o1-mini" = "o1-mini"

# Provider-specific configuration
[provider_config.openai]
supports_function_calling = true
supports_streaming = true
requires_api_key = true
api_key_env_var = "OPENAI_API_KEY"

[provider_config.claude]
supports_function_calling = true
supports_streaming = true
requires_api_key = true
api_key_env_var = "ANTHROPIC_API_KEY"

[provider_config.gemini]
supports_function_calling = true
supports_streaming = true
requires_api_key = true
api_key_env_var = "GEMINI_API_KEY"

[provider_config.ollama]
supports_function_calling = true
supports_streaming = true
requires_api_key = false
# api_key_env_var not needed for Ollama

[provider_config.openrouter]
supports_function_calling = true
supports_streaming = true
requires_api_key = true
api_key_env_var = "OPENROUTER_API_KEY"

[provider_config.copilot]
supports_function_calling = true
supports_streaming = true
requires_api_key = true
api_key_env_var = "COPILOT_ACCESS_TOKEN"