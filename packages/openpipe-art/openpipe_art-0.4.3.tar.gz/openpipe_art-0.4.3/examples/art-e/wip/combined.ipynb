{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "Rmx-KPzCdrAn",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "# ü§ñ ART‚Ä¢E - Train an Email Agent that Beats o3!\n",
                "\n",
                "Imagine you're drowning in emails and need an AI assistant that can actually find the information that you're looking for. Today, we're going to build and train one using **reinforcement learning** - the same technique used to train o3 and DeepSeek-R1. Specifically, we'll be using GRPO (group relative policy optimization) to teach Qwen 2.5 7B to retrieve information from a database of emails with a higher degree of accuracy than o3.\n",
                "\n",
                "\n",
                "## üß≠ The Journey Ahead\n",
                "\n",
                "1. **üîß Setup** - Get our tools ready\n",
                "2. **üìö Data** - Create a realistic email database\n",
                "3. **üõ†Ô∏è Tools** - Build search capabilities\n",
                "4. **üéÆ Training** - Teach our agent through trial and error\n",
                "5. **üöÄ Deploy** - Watch it work its magic!\n",
                "\n",
                "If you come across any issues or have questions while following along, please join the Discord and ask away! For feature requests or to leave a star, visit our [GitHub](https://github.com/openpipe/art).\n",
                "\n",
                "\n",
                "<div class=\"align-center\">\n",
                "<a href=\"https://github.com/openpipe/art\"><img src=\"https://github.com/openpipe/art/raw/main/assets/ART_pill.png\" height=\"50\"></a>\n",
                "<a href=\"https://discord.gg/zbBHRUpwf4\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Discord_pill.png\" height=\"50\"></a>\n",
                "<a href=\"https://art.openpipe.ai\"><img src=\"https://github.com/openpipe/art/raw/main/assets/Documentation_pill.png\" height=\"50\"></a>\n",
                "\n",
                "</div>\n",
                "\n",
                " Now let's dive in!\n",
                "\n",
                "---\n",
                "\n",
                "## Step 1: Installing Packages üì¶\n",
                "\n",
                "We'll start by installing the packages we need to build our email search agent:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "t0Ns7jSTdrAo",
                "outputId": "c960fb5e-9b91-490b-bfe8-df0ea4b6fa07"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üì¶ Installing packages...\n",
                        "‚úÖ Successfully installed all packages!\n",
                        "\n",
                        "üéâ Package installation complete!\n"
                    ]
                }
            ],
            "source": [
                "print(\"üì¶ Installing packages...\")\n",
                "\n",
                "# Install packages using pip (uv can be used via command line if preferred)\n",
                "!uv pip install --quiet openpipe-art datasets litellm pydantic python-dotenv langchain-core tenacity weave rich tqdm nbformat\n",
                "\n",
                "print(\"‚úÖ Successfully installed all packages!\")\n",
                "print(\"\\nüéâ Package installation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "OjLv0jOudrAp",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "## üì• Data: Thank you Enron! ü´°\n",
                "\n",
                "Next, we need some realistic email inboxes for our research agent to navigate during training. Fortunately, when notorious energy trader Enron was sued for massive accounting fraud in 2001, 500K of their emails were made public in the litigation. (Pro tip: if you're engaging in massive accounting fraud, maybe don't save all your emails.)\n",
                "\n",
                "These emails are perfect for our project - they're real business correspondence with the kind of questions people actually ask: meeting times, flight confirmations, document locations, and more.\n",
                "\n",
                "In this step, we transform this raw email dump into a searchable database that our agent can learn to navigate. We'll do the following:\n",
                "\n",
                "1. **Download the emails** - Use the `datasets` library to download the emails from the [Enron dataset](https://huggingface.co/datasets/corbt/enron-emails).\n",
                "2. **Create database schema** - Set up SQLite tables for emails and recipients with proper indexes.\n",
                "3. **Build full-text search** - Create SQLite FTS5 indexes and triggers for optimized keyword searches.\n",
                "4. **Optimize for performance** - Add database indexes and configure search capabilities for real-time queries.\n",
                "\n",
                "\n",
                "When we're done, our agent will be able to practice searching through thousands of these real emails to find the exact information users are looking for.\n",
                "\n",
                "*The code below might take a few minutes to run while the email environment is being created. Feel free to expand the cell to see implementation details, or skip and continue on to the next section while it runs.*\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "id": "bi17DArpdrAq",
                "outputId": "329b384c-fb98-4b5e-ff57-c1aae88a26a6"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Database already exists at /root/sky_workdir/examples/art-e/data/enron_emails.db\n",
                        "üìß Sample Email from Database:\n",
                        "==================================================\n",
                        "üìÖ Date: 2044-01-04 22:48:58\n",
                        "üë§ From: cramer@cadvision.com\n",
                        "üìù Subject: trades\n",
                        "üÜî Message ID: <21511287.1075842027020.JavaMail.evans@thyme>\n",
                        "\n",
                        "üìÑ Body:\n",
                        "------------------------------\n",
                        "\n",
                        "Howdy, \n",
                        "bom went out 35 at 35.5 \n",
                        "Feb traded 32.75 and 33 \n",
                        "Mar 33 ,(away) , 33.5, 33.75 , and  34.25. \n",
                        "Q2 was lifted 33 \n",
                        "and Q4 closed at 39 for 25 MW \n",
                        "What day are we going for lunch  next week ? \n",
                        "Have good weekend \n",
                        "Erik \n",
                        "  \n",
                        "==================================================\n"
                    ]
                }
            ],
            "source": [
                "# @title Email Database Initialization {display-mode:\"form\"}\n",
                "# Setup SQLite database with email data from Hugging Face\n",
                "\n",
                "import sqlite3\n",
                "import os\n",
                "from datasets import load_dataset, Features, Value, Sequence\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Configuration\n",
                "BASE_DIR = os.getcwd()\n",
                "DEFAULT_DB_PATH = os.path.join(BASE_DIR, \"data\", \"enron_emails.db\")\n",
                "DEFAULT_REPO_ID = \"corbt/enron-emails\"\n",
                "\n",
                "# Database schema\n",
                "SQL_CREATE_TABLES = \"\"\"\n",
                "DROP TABLE IF EXISTS recipients;\n",
                "DROP TABLE IF EXISTS emails_fts;\n",
                "DROP TABLE IF EXISTS emails;\n",
                "\n",
                "CREATE TABLE emails (\n",
                "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
                "    message_id TEXT UNIQUE,\n",
                "    subject TEXT,\n",
                "    from_address TEXT,\n",
                "    date TEXT,\n",
                "    body TEXT,\n",
                "    file_name TEXT\n",
                ");\n",
                "\n",
                "CREATE TABLE recipients (\n",
                "    email_id INTEGER,\n",
                "    recipient_address TEXT,\n",
                "    recipient_type TEXT,\n",
                "    FOREIGN KEY(email_id) REFERENCES emails(id) ON DELETE CASCADE\n",
                ");\n",
                "\"\"\"\n",
                "\n",
                "SQL_CREATE_INDEXES_TRIGGERS = \"\"\"\n",
                "CREATE INDEX idx_emails_from ON emails(from_address);\n",
                "CREATE INDEX idx_emails_date ON emails(date);\n",
                "CREATE INDEX idx_emails_message_id ON emails(message_id);\n",
                "CREATE INDEX idx_recipients_address ON recipients(recipient_address);\n",
                "CREATE INDEX idx_recipients_type ON recipients(recipient_type);\n",
                "CREATE INDEX idx_recipients_email_id ON recipients(email_id);\n",
                "CREATE INDEX idx_recipients_address_email ON recipients(recipient_address, email_id);\n",
                "\n",
                "CREATE VIRTUAL TABLE emails_fts USING fts5(\n",
                "    subject,\n",
                "    body,\n",
                "    content='emails',\n",
                "    content_rowid='id'\n",
                ");\n",
                "\n",
                "CREATE TRIGGER emails_ai AFTER INSERT ON emails BEGIN\n",
                "    INSERT INTO emails_fts (rowid, subject, body)\n",
                "    VALUES (new.id, new.subject, new.body);\n",
                "END;\n",
                "\n",
                "CREATE TRIGGER emails_ad AFTER DELETE ON emails BEGIN\n",
                "    DELETE FROM emails_fts WHERE rowid=old.id;\n",
                "END;\n",
                "\n",
                "CREATE TRIGGER emails_au AFTER UPDATE ON emails BEGIN\n",
                "    UPDATE emails_fts SET subject=new.subject, body=new.body WHERE rowid=old.id;\n",
                "END;\n",
                "\n",
                "INSERT INTO emails_fts (rowid, subject, body) SELECT id, subject, body FROM emails;\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "def generate_database(overwrite: bool = False):\n",
                "    \"\"\"Generate the email database from Hugging Face dataset.\"\"\"\n",
                "    if os.path.exists(DEFAULT_DB_PATH) and not overwrite:\n",
                "        print(f\"Database already exists at {DEFAULT_DB_PATH}\")\n",
                "        return\n",
                "\n",
                "    os.makedirs(os.path.dirname(DEFAULT_DB_PATH), exist_ok=True)\n",
                "\n",
                "    print(\"üì• Downloading dataset from Hugging Face...\")\n",
                "    # Download dataset\n",
                "    expected_features = Features(\n",
                "        {\n",
                "            \"message_id\": Value(\"string\"),\n",
                "            \"subject\": Value(\"string\"),\n",
                "            \"from\": Value(\"string\"),\n",
                "            \"to\": Sequence(Value(\"string\")),\n",
                "            \"cc\": Sequence(Value(\"string\")),\n",
                "            \"bcc\": Sequence(Value(\"string\")),\n",
                "            \"date\": Value(\"timestamp[us]\"),\n",
                "            \"body\": Value(\"string\"),\n",
                "            \"file_name\": Value(\"string\"),\n",
                "        }\n",
                "    )\n",
                "    dataset = load_dataset(DEFAULT_REPO_ID, features=expected_features, split=\"train\")\n",
                "\n",
                "    print(\"üóÑÔ∏è Creating database and tables...\")\n",
                "    # Create database\n",
                "    conn = sqlite3.connect(DEFAULT_DB_PATH)\n",
                "    cursor = conn.cursor()\n",
                "    cursor.executescript(SQL_CREATE_TABLES)\n",
                "    conn.commit()\n",
                "\n",
                "    print(\"üìù Populating database with email data...\")\n",
                "    # Populate database\n",
                "    conn.execute(\"PRAGMA synchronous = OFF;\")\n",
                "    conn.execute(\"PRAGMA journal_mode = MEMORY;\")\n",
                "    conn.execute(\"BEGIN TRANSACTION;\")\n",
                "\n",
                "    processed_emails = set()\n",
                "\n",
                "    for email_data in tqdm(dataset, desc=\"Inserting emails\"):\n",
                "        message_id = email_data[\"message_id\"]\n",
                "        subject = email_data[\"subject\"]\n",
                "        from_address = email_data[\"from\"]\n",
                "        date_obj = email_data[\"date\"]\n",
                "        body = email_data[\"body\"]\n",
                "        file_name = email_data[\"file_name\"]\n",
                "\n",
                "        # Filter long emails and high recipient counts\n",
                "        if len(body) > 5000:\n",
                "            continue\n",
                "\n",
                "        to_list = [str(addr) for addr in email_data[\"to\"] if addr]\n",
                "        cc_list = [str(addr) for addr in email_data[\"cc\"] if addr]\n",
                "        bcc_list = [str(addr) for addr in email_data[\"bcc\"] if addr]\n",
                "\n",
                "        total_recipients = len(to_list) + len(cc_list) + len(bcc_list)\n",
                "        if total_recipients > 30:\n",
                "            continue\n",
                "\n",
                "        # Deduplicate\n",
                "        email_key = (subject, body, from_address)\n",
                "        if email_key in processed_emails:\n",
                "            continue\n",
                "        processed_emails.add(email_key)\n",
                "\n",
                "        date_str = date_obj.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
                "\n",
                "        cursor.execute(\n",
                "            \"\"\"\n",
                "            INSERT INTO emails (message_id, subject, from_address, date, body, file_name)\n",
                "            VALUES (?, ?, ?, ?, ?, ?)\n",
                "        \"\"\",\n",
                "            (message_id, subject, from_address, date_str, body, file_name),\n",
                "        )\n",
                "\n",
                "        email_pk_id = cursor.lastrowid\n",
                "\n",
                "        # Insert recipients\n",
                "        recipient_data = []\n",
                "        for addr in to_list:\n",
                "            recipient_data.append((email_pk_id, addr, \"to\"))\n",
                "        for addr in cc_list:\n",
                "            recipient_data.append((email_pk_id, addr, \"cc\"))\n",
                "        for addr in bcc_list:\n",
                "            recipient_data.append((email_pk_id, addr, \"bcc\"))\n",
                "\n",
                "        if recipient_data:\n",
                "            cursor.executemany(\n",
                "                \"\"\"\n",
                "                INSERT INTO recipients (email_id, recipient_address, recipient_type)\n",
                "                VALUES (?, ?, ?)\n",
                "            \"\"\",\n",
                "                recipient_data,\n",
                "            )\n",
                "\n",
                "    conn.execute(\"COMMIT;\")\n",
                "    print(\"üîç Creating search indexes and triggers...\")\n",
                "    cursor.executescript(SQL_CREATE_INDEXES_TRIGGERS)\n",
                "    conn.commit()\n",
                "    conn.close()\n",
                "\n",
                "    print(f\"‚úÖ Database successfully created at {DEFAULT_DB_PATH}\")\n",
                "\n",
                "\n",
                "# Initialize the database\n",
                "generate_database()\n",
                "\n",
                "\n",
                "# print first email from database\n",
                "# Load and print first email from database\n",
                "def get_sample_email():\n",
                "    \"\"\"Get a sample email from the database\"\"\"\n",
                "    conn = sqlite3.connect(f\"file:{DEFAULT_DB_PATH}?mode=ro\", uri=True)\n",
                "    cursor = conn.cursor()\n",
                "\n",
                "    # Get a sample email with some content\n",
                "    cursor.execute(\"\"\"\n",
                "        SELECT message_id, subject, from_address, date, body\n",
                "        FROM emails \n",
                "        WHERE subject IS NOT NULL \n",
                "        AND body IS NOT NULL \n",
                "        AND length(body) > 100\n",
                "        AND length(body) < 1000\n",
                "        ORDER BY date DESC\n",
                "        LIMIT 1\n",
                "    \"\"\")\n",
                "\n",
                "    result = cursor.fetchone()\n",
                "    conn.close()\n",
                "\n",
                "    if result:\n",
                "        message_id, subject, from_addr, date, body = result\n",
                "        return {\n",
                "            \"message_id\": message_id,\n",
                "            \"subject\": subject,\n",
                "            \"from_address\": from_addr,\n",
                "            \"date\": date,\n",
                "            \"body\": body,\n",
                "        }\n",
                "    return None\n",
                "\n",
                "\n",
                "# Load and display sample email\n",
                "sample_email = get_sample_email()\n",
                "if sample_email:\n",
                "    print(\"üìß Sample Email from Database:\")\n",
                "    print(\"=\" * 50)\n",
                "    print(f\"üìÖ Date: {sample_email['date']}\")\n",
                "    print(f\"üë§ From: {sample_email['from_address']}\")\n",
                "    print(f\"üìù Subject: {sample_email['subject']}\")\n",
                "    print(f\"üÜî Message ID: {sample_email['message_id']}\")\n",
                "    print(\"\\nüìÑ Body:\")\n",
                "    print(\"-\" * 30)\n",
                "    print(sample_email[\"body\"])\n",
                "    print(\"=\" * 50)\n",
                "else:\n",
                "    print(\"‚ùå No sample email found in database\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9F6D7g0pdrAq",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "## Generating Training Scenarios üßû\n",
                "\n",
                "In addition to our email database, we another form of data: realistic questions that people might ask about their emails. Since a good dataset of real user queries doesn't exist, we need to use a synthetically generated one.\n",
                "\n",
                "To save time, we'll use a [dataset](https://huggingface.co/datasets/corbt/enron_emails_sample_questions) of training scenarios that we've already generated.\n",
                "\n",
                "For each inbox, we iterated through 1000 emails in batches of 20. Then for every email within each batch, gpt-4.1 generated synthetic question-answer pairs. Based on these emails, the model generated questions like:\n",
                "- \\\"What time is the Astros group game against the Cubs?\\\"\n",
                "- \\\"What is my confirmation number for my Continental Airlines flight?\\\"\n",
                "- \\\"When is the Tuesday afternoon meeting with EES?\\\"\n",
                "\n",
                "We linked each question to the correct answer and the source email ID. We even asked gpt-4.1 to rate how realistic each question was - surprisingly effective at filtering out questions no real person would ask!\n",
                "\n",
                "In ART parlance, we refer to these synthetic question-answer pairs as [scenarios](https://art.openpipe.ai/resources/glossary#training-scenarios). Each scenario represents a situation that the agent will encounter during training and learn to handle before it's deployed.\n",
                "\n",
                "*Feel free to expand the cell below if you want to go deeper into the details of how we're loading the scenarios. Otherwise, let's move on to the next step!*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "hHxdHkBndrAq",
                "outputId": "06f6163b-70fd-4d96-9bcb-39b0fb77ead4"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "number of scenarios:  2\n",
                        "--------------------------------\n",
                        "first scenario:\n",
                        "question:  Were there any variances detected for hour 6 on 3/9/01?\n",
                        "answer:  Yes, variances were detected in both Generation and Energy Import/Export schedules for hour 6 on 3/9/01.\n",
                        "message_ids:  ['<17407857.1075840601283.JavaMail.evans@thyme>']\n",
                        "--------------------------------\n",
                        "second scenario:\n",
                        "question:  What changes are happening to the ISDA Master Agreements due to the NationsBank and Bank of America merger?\n",
                        "answer:  On July 5, 1999, NationsBank changed its name to Bank of America, N.A. On July 23, 1999, Bank of America National Trust and Savings Association merged with Bank of America, N.A. The ISDA Master Agreement between Enron Corp. and NationsBank, N.A. will remain in place, with Bank of America, National Association as counterparty. The ISDA Master Agreement between Enron Corp. and Bank of America National Savings and Trust Association is terminated, and existing trades will be transferred to the other agreement.\n",
                        "message_ids:  ['<8710990.1075846915948.JavaMail.evans@thyme>']\n",
                        "‚úÖ General utilities loaded successfully!\n"
                    ]
                }
            ],
            "source": [
                "# @title Training Scenario Utilities {display-mode:\"form\"}\n",
                "# Data types, dataset loading, and trajectory reporting\n",
                "\n",
                "import art\n",
                "from typing import List, Optional, Literal, cast\n",
                "from dataclasses import dataclass\n",
                "from pydantic import BaseModel\n",
                "import json\n",
                "import weave\n",
                "from weave.trace.autopatch import AutopatchSettings\n",
                "from dotenv import load_dotenv\n",
                "\n",
                "# Load environment variables\n",
                "load_dotenv()\n",
                "\n",
                "# Configuration constants\n",
                "HF_REPO_ID = \"corbt/enron_emails_sample_questions\"\n",
                "bad_queries = [49, 101, 129, 171, 208, 266, 327]\n",
                "\n",
                "# ==================== DATA TYPES ====================\n",
                "\n",
                "\n",
                "class TrainingConfig(BaseModel):\n",
                "    trajectories_per_group: int = 6\n",
                "    groups_per_step: int = 1\n",
                "    learning_rate: float = 1.2e-5\n",
                "    eval_steps: int = 30\n",
                "    val_set_size: int = 100\n",
                "    training_dataset_size: int = 4000\n",
                "    num_epochs: int = 4\n",
                "    group_judge_model: str = \"openai/gpt-4.1\"\n",
                "    minimum_reward_std_dev: float = 0.0\n",
                "    training_dataset_seed: int | None = None\n",
                "\n",
                "\n",
                "class ProjectPolicyConfig(BaseModel):\n",
                "    max_turns: int = 10\n",
                "    max_tokens: int = 2048\n",
                "    log_to_openpipe: bool = False\n",
                "    litellm_model_name: str | None = None\n",
                "    stupid_simple_reward_fn: bool = False\n",
                "    training_config: TrainingConfig | None = None\n",
                "\n",
                "\n",
                "class SyntheticQuery(BaseModel):\n",
                "    id: int\n",
                "    question: str\n",
                "    answer: str\n",
                "    message_ids: List[str]\n",
                "    how_realistic: float\n",
                "    inbox_address: str\n",
                "    query_date: str\n",
                "    split: Literal[\"train\", \"test\"]\n",
                "\n",
                "\n",
                "class Email(BaseModel):\n",
                "    message_id: str\n",
                "    date: str\n",
                "    subject: Optional[str] = None\n",
                "    from_address: Optional[str] = None\n",
                "    to_addresses: List[str] = []\n",
                "    cc_addresses: List[str] = []\n",
                "    bcc_addresses: List[str] = []\n",
                "    body: Optional[str] = None\n",
                "    file_name: Optional[str] = None\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class SearchResult:\n",
                "    message_id: str\n",
                "    snippet: str\n",
                "\n",
                "\n",
                "# ==================== DATASET LOADING ====================\n",
                "\n",
                "\n",
                "def load_synthetic_queries(\n",
                "    split: Literal[\"train\", \"test\"] = \"train\",\n",
                "    limit: Optional[int] = None,\n",
                "    max_messages: Optional[int] = 1,\n",
                "    shuffle: bool = False,\n",
                "    seed: Optional[int] = None,\n",
                "    exclude_known_bad_queries: bool = True,\n",
                ") -> List[SyntheticQuery]:\n",
                "    \"\"\"Load synthetic query dataset.\"\"\"\n",
                "    dataset = load_dataset(HF_REPO_ID, split=split)\n",
                "\n",
                "    if max_messages is not None:\n",
                "        dataset = dataset.filter(lambda x: len(x[\"message_ids\"]) <= max_messages)\n",
                "\n",
                "    if exclude_known_bad_queries:\n",
                "        dataset = dataset.filter(lambda x: x[\"id\"] not in bad_queries)\n",
                "\n",
                "    if shuffle or seed is not None:\n",
                "        if seed is not None:\n",
                "            dataset = dataset.shuffle(seed=seed)\n",
                "        else:\n",
                "            dataset = dataset.shuffle()\n",
                "\n",
                "    queries = [SyntheticQuery(**row, split=split) for row in dataset]\n",
                "\n",
                "    if max_messages is not None:\n",
                "        queries = [query for query in queries if len(query.message_ids) <= max_messages]\n",
                "\n",
                "    if limit is not None:\n",
                "        return queries[:limit]\n",
                "    else:\n",
                "        return queries\n",
                "\n",
                "\n",
                "synthetic_queries = load_synthetic_queries(split=\"train\", limit=2)\n",
                "print(\"number of scenarios: \", len(synthetic_queries))\n",
                "print(\"--------------------------------\")\n",
                "print(\"first scenario:\")\n",
                "print(\"question: \", synthetic_queries[0].question)\n",
                "print(\"answer: \", synthetic_queries[0].answer)\n",
                "print(\"message_ids: \", synthetic_queries[0].message_ids)\n",
                "print(\"--------------------------------\")\n",
                "print(\"second scenario:\")\n",
                "print(\"question: \", synthetic_queries[1].question)\n",
                "print(\"answer: \", synthetic_queries[1].answer)\n",
                "print(\"message_ids: \", synthetic_queries[1].message_ids)\n",
                "\n",
                "# ==================== TRAJECTORY REPORTING ====================\n",
                "\n",
                "\n",
                "class ProjectTrajectory(art.Trajectory):\n",
                "    scenario: SyntheticQuery\n",
                "    generated_answer: str | None = None\n",
                "\n",
                "\n",
                "def report_trajectory(\n",
                "    model: art.Model,\n",
                "    trajectory: ProjectTrajectory,\n",
                "    step: int = 0,\n",
                "):\n",
                "    \"\"\"Report trajectory to Weave for logging.\"\"\"\n",
                "    client = weave.init(\n",
                "        model.project, autopatch_settings=AutopatchSettings(disable_autopatch=True)\n",
                "    )\n",
                "\n",
                "    inputs = {\n",
                "        \"model\": model.name,\n",
                "        \"scenario\": trajectory.scenario,\n",
                "        \"step\": step,\n",
                "    }\n",
                "\n",
                "    if isinstance(model, art.TrainableModel):\n",
                "        inputs[\"base_model\"] = model.base_model\n",
                "\n",
                "    call = client.create_call(\"trajectory\", inputs=inputs)\n",
                "    client.finish_call(call, output={\"tr\": trajectory})\n",
                "\n",
                "\n",
                "print(\"‚úÖ General utilities loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "bLOj1sNFdrAr",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "## Giving the Agent Tools üõ†Ô∏è\n",
                "\n",
                "Every agent operates in an environment, which just means \\\"the tools you give it and the information it has access to.\\\" In our case, the environment consists of a database of email inboxes and the tools it can use to search through them.\n",
                "\n",
                "Our agent will have access to 3 tools:\n",
                "\n",
                "1. **`search_emails(keywords, sent_after, sent_before)`** - finds up to 10 emails matching given keywords with date filters applied, returns message IDs and matching snippets\n",
                "2. **`read_email(message_id)`** - returns the full email body for a given message ID  \n",
                "3. **`return_final_answer(answer, sources)`** - returns the final answer to the user's question with the list of message IDs that supported the answer\n",
                "\n",
                "Our agent will have to learn to use these tools wisely to find the right information.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "id": "7jNpYiZ1drAr",
                "outputId": "d6c62480-9f03-42da-ec0e-9359ef13b590"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üîç Testing email search functionality...\n",
                        "‚úÖ Found 3 emails in test search\n",
                        "‚úÖ Successfully read email: RE: Meeting w/PG&E this Friday, Right after DJ's M...\n",
                        "‚úÖ Email search tools working correctly!\n"
                    ]
                }
            ],
            "source": [
                "# Email Search Tools\n",
                "# Database connection and email search/retrieval functionality\n",
                "\n",
                "from typing import List, Optional\n",
                "\n",
                "# Global database connection\n",
                "conn = None\n",
                "\n",
                "\n",
                "def get_conn():\n",
                "    \"\"\"Get database connection (singleton pattern)\"\"\"\n",
                "    global conn\n",
                "    if conn is None:\n",
                "        conn = sqlite3.connect(\n",
                "            f\"file:{DEFAULT_DB_PATH}?mode=ro\", uri=True, check_same_thread=False\n",
                "        )\n",
                "    return conn\n",
                "\n",
                "\n",
                "def search_emails(\n",
                "    inbox: str,\n",
                "    keywords: List[str],\n",
                "    from_addr: Optional[str] = None,\n",
                "    to_addr: Optional[str] = None,\n",
                "    sent_after: Optional[str] = None,\n",
                "    sent_before: Optional[str] = None,\n",
                "    max_results: int = 10,\n",
                ") -> List[SearchResult]:\n",
                "    \"\"\"Search the email database based on keywords and filters.\"\"\"\n",
                "    if not keywords:\n",
                "        raise ValueError(\"No keywords provided for search.\")\n",
                "\n",
                "    if max_results > 10:\n",
                "        raise ValueError(\"max_results must be less than or equal to 10.\")\n",
                "\n",
                "    cursor = get_conn().cursor()\n",
                "    where_clauses = []\n",
                "    params = []\n",
                "\n",
                "    # Keywords (FTS) - Fixed f-string issue\n",
                "    fts_query = \" \".join('\"' + k.replace('\"', '\"\"') + '\"' for k in keywords)\n",
                "    where_clauses.append(\"fts.emails_fts MATCH ?\")\n",
                "    params.append(fts_query)\n",
                "\n",
                "    # Inbox filter\n",
                "    where_clauses.append(\"\"\"\n",
                "        (e.from_address = ? OR EXISTS (\n",
                "            SELECT 1 FROM recipients r_inbox\n",
                "            WHERE r_inbox.recipient_address = ? AND r_inbox.email_id = e.id\n",
                "        ))\n",
                "    \"\"\")\n",
                "    params.extend([inbox, inbox])\n",
                "\n",
                "    # Optional filters\n",
                "    if from_addr:\n",
                "        where_clauses.append(\"e.from_address = ?\")\n",
                "        params.append(from_addr)\n",
                "\n",
                "    if to_addr:\n",
                "        where_clauses.append(\"\"\"\n",
                "            EXISTS (\n",
                "                SELECT 1 FROM recipients r_to\n",
                "                WHERE r_to.recipient_address = ? AND r_to.email_id = e.id\n",
                "            )\n",
                "        \"\"\")\n",
                "        params.append(to_addr)\n",
                "\n",
                "    if sent_after:\n",
                "        where_clauses.append(\"e.date >= ?\")\n",
                "        params.append(f\"{sent_after} 00:00:00\")\n",
                "\n",
                "    if sent_before:\n",
                "        where_clauses.append(\"e.date < ?\")\n",
                "        params.append(f\"{sent_before} 00:00:00\")\n",
                "\n",
                "    sql = f\"\"\"\n",
                "        SELECT\n",
                "            e.message_id,\n",
                "            snippet(emails_fts, -1, '<b>', '</b>', ' ... ', 15) as snippet\n",
                "        FROM\n",
                "            emails e JOIN emails_fts fts ON e.id = fts.rowid\n",
                "        WHERE\n",
                "            {\" AND \".join(where_clauses)}\n",
                "        ORDER BY\n",
                "            e.date DESC\n",
                "        LIMIT ?;\n",
                "    \"\"\"\n",
                "    params.append(max_results)\n",
                "\n",
                "    cursor.execute(sql, params)\n",
                "    results = cursor.fetchall()\n",
                "\n",
                "    return [SearchResult(message_id=row[0], snippet=row[1]) for row in results]\n",
                "\n",
                "\n",
                "def read_email(message_id: str) -> Optional[Email]:\n",
                "    \"\"\"Retrieve a single email by its message_id.\"\"\"\n",
                "    cursor = get_conn().cursor()\n",
                "\n",
                "    # Get email details\n",
                "    email_sql = \"\"\"\n",
                "        SELECT message_id, date, subject, from_address, body, file_name\n",
                "        FROM emails\n",
                "        WHERE message_id = ?;\n",
                "    \"\"\"\n",
                "    cursor.execute(email_sql, (message_id,))\n",
                "    email_row = cursor.fetchone()\n",
                "\n",
                "    if not email_row:\n",
                "        return None\n",
                "\n",
                "    msg_id, date, subject, from_addr, body, file_name = email_row\n",
                "\n",
                "    # Get recipients\n",
                "    recipients_sql = \"\"\"\n",
                "        SELECT recipient_address, recipient_type\n",
                "        FROM recipients\n",
                "        WHERE email_id = ?;\n",
                "    \"\"\"\n",
                "    cursor.execute(recipients_sql, (message_id,))\n",
                "    recipient_rows = cursor.fetchall()\n",
                "\n",
                "    to_addresses = []\n",
                "    cc_addresses = []\n",
                "    bcc_addresses = []\n",
                "\n",
                "    for addr, type in recipient_rows:\n",
                "        type_lower = type.lower()\n",
                "        if type_lower == \"to\":\n",
                "            to_addresses.append(addr)\n",
                "        elif type_lower == \"cc\":\n",
                "            cc_addresses.append(addr)\n",
                "        elif type_lower == \"bcc\":\n",
                "            bcc_addresses.append(addr)\n",
                "\n",
                "    return Email(\n",
                "        message_id=msg_id,\n",
                "        date=date,\n",
                "        subject=subject,\n",
                "        from_address=from_addr,\n",
                "        to_addresses=to_addresses,\n",
                "        cc_addresses=cc_addresses,\n",
                "        bcc_addresses=bcc_addresses,\n",
                "        body=body,\n",
                "        file_name=file_name,\n",
                "    )\n",
                "\n",
                "\n",
                "# Test the search functionality\n",
                "print(\"üîç Testing email search functionality...\")\n",
                "try:\n",
                "    # Test with some sample keywords\n",
                "    test_results = search_emails(\n",
                "        inbox=\"jeff.dasovich@enron.com\", keywords=[\"meeting\", \"schedule\"], max_results=3\n",
                "    )\n",
                "    print(f\"‚úÖ Found {len(test_results)} emails in test search\")\n",
                "\n",
                "    if test_results:\n",
                "        # Test reading an email\n",
                "        email = read_email(test_results[0].message_id)\n",
                "        if email:\n",
                "            print(f\"‚úÖ Successfully read email: {email.subject[:50]}...\")\n",
                "        else:\n",
                "            print(\"‚ùå Failed to read email\")\n",
                "\n",
                "    print(\"‚úÖ Email search tools working correctly!\")\n",
                "\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error testing email search: {e}\")\n",
                "    print(\"This might be expected if the database hasn't been created yet.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Rubric and Correctness\n",
                "\n",
                "As our model learns to answer user queries, we need a way to track its performance over time. We'll define a rubric for storing metrics to be reported to wandb at the end of each training run.\n",
                "\n",
                "### Rubric\n",
                "\n",
                "* `num_turns`: the number of turns the agent took to answer the question\n",
                "* `answer_correct`: Whether the LLM judge determines the answer is correct\n",
                "* `sources_correct`: Whether the agent cited the right source message ID\n",
                "* `num_turns`: Number of conversation turns taken\n",
                "* `attempted_answer`: Whether the agent actually tried to answer\n",
                "* `ever_found_right_email`: Whether the correct email appeared in search results\n",
                "* `ever_read_right_email`: Whether the agent read the correct email\n",
                "* `ever_tried_to_read_invalid_email`: Whether the agent tried to read non-existent emails\n",
                "* `num_sources`: Number of sources cited\n",
                "* `cant_parse_tool_call`: JSON parsing errors in tool calls\n",
                "* `bad_tool_call_name`: Calling non-existent tools\n",
                "* `bad_tool_call_args`: Invalid tool arguments\n",
                "* `ran_out_of_turns`: Exceeded maximum turns\n",
                "* `returned_i_dont_know`: Explicitly said \"I don't know\"\n",
                "* `prompt_tokens`: Total prompt tokens used\n",
                "* `completion_tokens`: Total completion tokens generated\n",
                "\n",
                "Importantly, we'll also judge the `correctness` of the agent's answers. We'll do this by comparing the agent's answer to the real answer for each scenario using an LLM judge and determining whether the agent's answer matches the real answer or not. We'll use `correctness` as a proxy of agent performance to track whether the model is getting better over time or running off the rails.\n",
                "\n",
                "\n",
                "*Expand the cell below to see the full rubric and the correctness judge's prompt.*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Judging Correctness\n",
                "# Trajectory execution, reward calculation, and correctness judging\n",
                "\n",
                "import textwrap\n",
                "from dataclasses import dataclass, asdict\n",
                "from pydantic import BaseModel, Field, validate_call, ValidationError\n",
                "from litellm import acompletion\n",
                "import litellm\n",
                "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
                "from litellm.caching.caching import LiteLLMCacheType, Cache\n",
                "from art.utils.litellm import convert_litellm_choice_to_openai\n",
                "from tenacity import retry, stop_after_attempt\n",
                "import logging\n",
                "\n",
                "# Setup LiteLLM\n",
                "litellm.cache = Cache(type=LiteLLMCacheType.DISK)\n",
                "litellm.drop_params = True\n",
                "logging.getLogger(\"weave.trace.op\").setLevel(logging.WARNING)\n",
                "\n",
                "# ==================== RUBRIC ====================\n",
                "\n",
                "\n",
                "@dataclass\n",
                "class FinalRubric:\n",
                "    answer_correct: bool = False\n",
                "    sources_correct: bool = False\n",
                "    num_turns: int = 0\n",
                "    attempted_answer: bool = False\n",
                "    ever_found_right_email: bool = False\n",
                "    ever_read_right_email: bool = False\n",
                "    cant_parse_tool_call: bool = False\n",
                "    bad_tool_call_name: bool = False\n",
                "    bad_tool_call_args: bool = False\n",
                "    ran_out_of_turns: bool = False\n",
                "    returned_i_dont_know: bool = False\n",
                "    num_sources: int = 0\n",
                "    ever_tried_to_read_invalid_email: bool = False\n",
                "    prompt_tokens: int = 0\n",
                "    completion_tokens: int = 0\n",
                "\n",
                "    def to_metrics(self) -> dict[str, float | int]:\n",
                "        metrics = {k: int(v) for k, v in asdict(self).items()}\n",
                "        metrics[\"failed_format_validation\"] = int(\n",
                "            self.bad_tool_call_name\n",
                "            or self.bad_tool_call_args\n",
                "            or self.cant_parse_tool_call\n",
                "        )\n",
                "        return metrics\n",
                "\n",
                "\n",
                "# ==================== CORRECTNESS JUDGING ====================\n",
                "\n",
                "\n",
                "class CorrectnessJudgeResponse(BaseModel):\n",
                "    thinking: str = Field(description=\"Explanation of the reasoning process.\")\n",
                "    accept: bool = Field(description=\"Whether the AI answer should be accepted.\")\n",
                "\n",
                "\n",
                "@retry(stop=stop_after_attempt(3))\n",
                "async def judge_correctness(\n",
                "    answer: str, query: SyntheticQuery\n",
                ") -> CorrectnessJudgeResponse:\n",
                "    \"\"\"Use an LLM to judge whether answer matches the expected answer.\"\"\"\n",
                "    system_prompt = textwrap.dedent(\"\"\"\n",
                "        You are given a question, the reference answer, and an AI-generated answer.\n",
                "\n",
                "        Follow these steps:\n",
                "        1. Identify EXACTLY what information the question is asking for.\n",
                "        2. Extract ONLY the essential facts from the reference answer.\n",
                "        3. Verify that every essential fact appears in the AI answer.\n",
                "        4. If any essential fact is missing or contradicted, set accept to false.\n",
                "\n",
                "        Return pure JSON with this schema:\n",
                "        {\n",
                "          \"thinking\": string,\n",
                "          \"accept\": boolean\n",
                "        }\n",
                "    \"\"\")\n",
                "\n",
                "    messages = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\n",
                "            \"role\": \"user\",\n",
                "            \"content\": (\n",
                "                f\"Question: {query.question}\\n\"\n",
                "                f\"Reference answer: {query.answer}\\n\"\n",
                "                f\"AI answer: {answer}\"\n",
                "            ),\n",
                "        },\n",
                "    ]\n",
                "\n",
                "    response = await acompletion(\n",
                "        model=\"openai/gpt-4.1\",\n",
                "        messages=messages,\n",
                "        caching=True,\n",
                "        response_format=CorrectnessJudgeResponse,\n",
                "    )\n",
                "\n",
                "    first_choice = response.choices[0]\n",
                "    raw_content = first_choice.message.content or \"{}\"\n",
                "\n",
                "    try:\n",
                "        return CorrectnessJudgeResponse.model_validate_json(raw_content)\n",
                "    except Exception as e:\n",
                "        return CorrectnessJudgeResponse(\n",
                "            thinking=f\"Parse error: {e}\\nRaw: {raw_content}\", accept=False\n",
                "        )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Rollout Function\n",
                "\n",
                "*\\\"Show me the incentive and I'll show you the outcome.\\\"* ‚Äî Charlie Munger\n",
                "\n",
                "(Charlie would have made a great RL researcher!)\n",
                "\n",
                "A robust reward function is imperative to successful RL training. The purpose of the reward function is to let the model know when it did well and when it did poorly, so that its weights can be updated to make the desirable behavior more likely and avoid the undesirable behavior altogether. Historically, reward functions have been tricky to get right. Fortunately, a new technique called GRO (Group Reward Optimization) makes it much easier.\n",
                "\n",
                "Rather than judge the agent's performance individually, GRO groups agent attempts by scenario and sends a history of them to an LLM judge to be ranked. In practice, it's much easier to judge which of two attempts is better than to judge what score each individual attempt should get. This approach also takes advantage of GRPO's ability to learn from relative differences in rewards, rather than absolute scores.\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "id": "IStiv1DVdrAr",
                "outputId": "67d0f9c9-7cb5-48fc-c481-60e31307f591"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Rollout function loaded successfully!\n"
                    ]
                }
            ],
            "source": [
                "# Rollout Function\n",
                "# Trajectory execution, reward calculation, and correctness judging\n",
                "\n",
                "import asyncio\n",
                "from pydantic import BaseModel, Field\n",
                "import litellm\n",
                "from litellm.caching.caching import LiteLLMCacheType, Cache\n",
                "from tenacity import retry, stop_after_attempt\n",
                "import logging\n",
                "\n",
                "# Setup LiteLLM\n",
                "litellm.cache = Cache(type=LiteLLMCacheType.DISK)\n",
                "litellm.drop_params = True\n",
                "logging.getLogger(\"weave.trace.op\").setLevel(logging.WARNING)\n",
                "\n",
                "\n",
                "@retry(stop=stop_after_attempt(3))\n",
                "async def rollout(\n",
                "    model: art.Model,\n",
                "    scenario: SyntheticQuery,\n",
                ") -> ProjectTrajectory:\n",
                "    \"\"\"Execute a single trajectory rollout.\"\"\"\n",
                "    rubric = FinalRubric()\n",
                "    traj = ProjectTrajectory(\n",
                "        messages_and_choices=[],\n",
                "        reward=0,\n",
                "        metadata={\"email_inbox\": scenario.inbox_address, \"scenario_id\": scenario.id},\n",
                "        scenario=scenario,\n",
                "    )\n",
                "\n",
                "    system_prompt = textwrap.dedent(f\"\"\"\\\n",
                "        You are an email search agent. You are given a user query and tools to search emails.\n",
                "        Use the tools to find the answer to the user's query. You may take up to {model.config.max_turns} turns.\n",
                "\n",
                "        User's email address is {scenario.inbox_address}\n",
                "        Today's date is {scenario.query_date}\n",
                "    \"\"\")\n",
                "\n",
                "    async def search_emails_tool(keywords: list[str]) -> list[dict]:\n",
                "        \"\"\"Search the user's email inbox for emails matching keywords.\"\"\"\n",
                "        resp = search_emails(\n",
                "            inbox=scenario.inbox_address,\n",
                "            sent_before=scenario.query_date,\n",
                "            keywords=keywords,\n",
                "        )\n",
                "\n",
                "        for r in resp:\n",
                "            if r.message_id == scenario.message_ids[0]:\n",
                "                rubric.ever_found_right_email = True\n",
                "        return [asdict(r) for r in resp]\n",
                "\n",
                "    async def read_email_tool(message_id: str) -> Email | dict:\n",
                "        \"\"\"Read the content of an email.\"\"\"\n",
                "        email_content = read_email(message_id)\n",
                "\n",
                "        if message_id == scenario.message_ids[0]:\n",
                "            rubric.ever_read_right_email = True\n",
                "        if email_content is None:\n",
                "            return {\"error\": \"Email not found\"}\n",
                "        else:\n",
                "            return email_content.model_dump()\n",
                "\n",
                "    async def return_final_answer(answer: str, sources: list[str]):\n",
                "        \"\"\"Return the final answer with sources.\"\"\"\n",
                "        rubric.attempted_answer = True\n",
                "        traj.generated_answer = answer\n",
                "\n",
                "        if answer == \"I don't know\":\n",
                "            rubric.returned_i_dont_know = True\n",
                "        else:\n",
                "            async with traj.track_duration(\"determine_if_answer_is_correct\"):\n",
                "                judge_response = await judge_correctness(answer, scenario)\n",
                "                traj.logs.append(f\"Correctness judge response: {judge_response}\")\n",
                "                rubric.answer_correct = judge_response.accept\n",
                "            rubric.sources_correct = scenario.message_ids[0] in sources\n",
                "\n",
                "    tools = [search_emails_tool, read_email_tool, return_final_answer]\n",
                "    traj.tools = [convert_to_openai_tool(t) for t in tools]\n",
                "\n",
                "    traj.messages_and_choices = [\n",
                "        {\"role\": \"system\", \"content\": system_prompt},\n",
                "        {\"role\": \"user\", \"content\": scenario.question},\n",
                "    ]\n",
                "\n",
                "    while not rubric.attempted_answer:\n",
                "        rubric.num_turns += 1\n",
                "\n",
                "        if rubric.num_turns > model.config.max_turns:\n",
                "            rubric.ran_out_of_turns = True\n",
                "            break\n",
                "\n",
                "        litellm_model_name = model.config.litellm_model_name\n",
                "        if litellm_model_name is None:\n",
                "            litellm_model_name = f\"hosted_vllm/{model.name}\"\n",
                "\n",
                "        async with traj.track_duration(\"llm_completion\"):\n",
                "            llm_response = await acompletion(\n",
                "                model=litellm_model_name,\n",
                "                base_url=model.inference_base_url,\n",
                "                messages=traj.messages(),\n",
                "                caching=not model.trainable,\n",
                "                api_key=model.inference_api_key,\n",
                "                max_completion_tokens=model.config.max_tokens,\n",
                "                tools=traj.tools,\n",
                "            )\n",
                "\n",
                "        rubric.prompt_tokens += llm_response.usage.prompt_tokens\n",
                "        rubric.completion_tokens += llm_response.usage.completion_tokens\n",
                "        choice = llm_response.choices[0]\n",
                "\n",
                "        # Handle only one tool call at a time\n",
                "        if choice.message.tool_calls is not None and len(choice.message.tool_calls) > 1:\n",
                "            choice.message.tool_calls = choice.message.tool_calls[:1]\n",
                "        traj.messages_and_choices.append(convert_litellm_choice_to_openai(choice))\n",
                "\n",
                "        if choice.message.tool_calls is None:\n",
                "            rubric.bad_tool_call_name = True\n",
                "            break\n",
                "\n",
                "        for tool_call in choice.message.tool_calls:\n",
                "            if tool_call is None:\n",
                "                rubric.bad_tool_call_args = True\n",
                "                break\n",
                "\n",
                "            try:\n",
                "                tool_args = json.loads(tool_call.function.arguments)\n",
                "            except Exception:\n",
                "                rubric.bad_tool_call_args = True\n",
                "                break\n",
                "\n",
                "            for tool_fn in tools:\n",
                "                if tool_fn.__name__ == tool_call.function.name:\n",
                "                    try:\n",
                "                        validated_fn = validate_call(tool_fn)\n",
                "                        result = await validated_fn(**tool_args)\n",
                "                        traj.messages_and_choices.append(\n",
                "                            {\n",
                "                                \"role\": \"tool\",\n",
                "                                \"tool_call_id\": tool_call.id,\n",
                "                                \"content\": json.dumps(result),\n",
                "                            }\n",
                "                        )\n",
                "                    except ValidationError as e:\n",
                "                        rubric.bad_tool_call_args = True\n",
                "                        traj.logs.append(\n",
                "                            f\"Invalid args for {tool_call.function.name}: {e}\"\n",
                "                        )\n",
                "                        break\n",
                "                    break\n",
                "            else:\n",
                "                rubric.bad_tool_call_name = True\n",
                "                break\n",
                "\n",
                "        if rubric.bad_tool_call_name or rubric.bad_tool_call_args:\n",
                "            break\n",
                "\n",
                "    traj.metrics = rubric.to_metrics()\n",
                "\n",
                "    traj.finish()\n",
                "    return traj\n",
                "\n",
                "\n",
                "print(\"‚úÖ Rollout function loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "d1xHf5ZudrAs",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "## The Secret Sauce: LLM-as-Judge Rewards üéì\n",
                "\n",
                "Instead of using a basic reward function that judges the agent's performance individually, we're bringing in an AI teacher (GPT-4) to give nuanced feedback on our agent's performance.\n",
                "\n",
                "**Why this matters:** The AI judge can see subtle differences that our simple reward function might miss. It can tell the difference between \\\"Agent A found the answer but took 8 searches\\\" and \\\"Agent B found it in just 3 searches.\\\" This nuanced feedback makes our training exponentially smarter.\n",
                "\n",
                "**The Group Judge Process:**\n",
                "1. Run our agent 4 times on the same question\n",
                "2. Compare all 4 attempts side-by-side\n",
                "3. Score each one based on efficiency, accuracy, and strategy\n",
                "4. Use these scores to teach our model which approaches work best\n",
                "\n",
                "This is a great way to teach elegant problem-solving. The model learns not just to find answers, but to find them efficiently and reliably.\n",
                "\n",
                "We'll ask our LLM judge to consider the following:\n",
                "\n",
                "**üéØ Answer Correctness** - The big one. Getting the right answer gets the highest reward.\n",
                "\n",
                "**‚ö° Turn Efficiency** - We give small \\\"extra credit\\\" for taking fewer turns. This is a proxy for latency‚Äîfewer round trips means faster answers.\n",
                "\n",
                "**üö´ Hallucination Penalty** - If the agent can't find the right answer, saying \\\"I don't know\\\" is much better than making something up. We add a significant penalty to incorrect answers.\n",
                "\n",
                "**üèÜ Partial Credit** - Small rewards for finding the right email in search results, actually reading it, and getting the source right even if the answer is wrong.\n",
                "\n",
                "*This is the difference between training a model that works and training one that excels.*\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "id": "EGj89TzbdrAs",
                "outputId": "5579a526-199b-4634-aba1-18307a199641"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "‚úÖ Group judgment functionality loaded successfully!\n"
                    ]
                }
            ],
            "source": [
                "# Group Judgement\n",
                "# LLM-based scoring of trajectory groups\n",
                "\n",
                "from typing import List, Literal\n",
                "from pydantic import BaseModel, Field\n",
                "\n",
                "# ==================== JUDGE DATA TYPES ====================\n",
                "\n",
                "\n",
                "class Issue(BaseModel):\n",
                "    label: str = Field(description=\"A short label for the issue.\")\n",
                "    explanation: str = Field(description=\"A human-readable explanation of the issue.\")\n",
                "    severity: Literal[\"minor\", \"major\", \"fatal\"] = Field(\n",
                "        description=\"The severity of the issue.\"\n",
                "    )\n",
                "\n",
                "\n",
                "class RolloutScore(BaseModel):\n",
                "    rollout_id: str = Field(description=\"The id of the rollout being scored.\")\n",
                "    explanation: str = Field(\n",
                "        description=\"A short explanation of why you gave this score.\"\n",
                "    )\n",
                "    score: float = Field(description=\"A score between 0 and 1.\")\n",
                "    issues: List[str] = Field(\n",
                "        description=\"The list of labels for each issue identified.\"\n",
                "    )\n",
                "\n",
                "\n",
                "class JudgeGroupResponse(BaseModel):\n",
                "    new_issues: List[Issue] = Field(\n",
                "        description=\"Any new issues identified on the rollouts.\"\n",
                "    )\n",
                "    scores: List[RolloutScore] = Field(description=\"The scores for each rollout.\")\n",
                "\n",
                "\n",
                "# ==================== GROUP JUDGE CLASS ====================\n",
                "\n",
                "DEFAULT_RUBRIC = \"\"\"\n",
                "- A rollout that achieves its goal should always get a significantly higher score than a rollout that does not achieve its goal.\n",
                "- A rollout that achieves its goal more efficiently should get a higher score than a rollout that achieves its goal less efficiently.\n",
                "- If one rollout is only slightly better than another, the difference in scores should be small. If it is significantly better, the difference in scores should be large.\n",
                "- You may give some partial credit for a rollout that makes progress towards its goal but does not complete it.\n",
                "\"\"\"\n",
                "\n",
                "\n",
                "class GroupJudge:\n",
                "    \"\"\"LLM-based judge for groups of rollouts.\"\"\"\n",
                "\n",
                "    def __init__(\n",
                "        self,\n",
                "        project: str,\n",
                "        judge_model: str = \"openai/gpt-4.1\",\n",
                "        rubric: str = DEFAULT_RUBRIC,\n",
                "        initial_issues: List[Issue] = None,\n",
                "    ):\n",
                "        self.project = project\n",
                "        self.judge_model = judge_model\n",
                "        self.rubric = rubric\n",
                "        self.all_issues = initial_issues or [\n",
                "            Issue(\n",
                "                label=\"looping\",\n",
                "                explanation=\"The assistant repeats itself unnecessarily but is able to recover.\",\n",
                "                severity=\"minor\",\n",
                "            ),\n",
                "            Issue(\n",
                "                label=\"fatal_looping\",\n",
                "                explanation=\"The assistant began repeating itself and is unable to recover.\",\n",
                "                severity=\"fatal\",\n",
                "            ),\n",
                "        ]\n",
                "\n",
                "    async def judge(self, rollouts: list[ProjectTrajectory]) -> list[ProjectTrajectory]:\n",
                "        \"\"\"Score every trajectory in rollouts and write the score to traj.reward.\"\"\"\n",
                "        if not rollouts:\n",
                "            return rollouts\n",
                "\n",
                "        # Determine common prefix to save tokens\n",
                "        message_lists = [traj.messages() for traj in rollouts]\n",
                "        common_prefix_len = 0\n",
                "        for i, msg in enumerate(message_lists[0]):\n",
                "            if all(msg_list[i] == msg for msg_list in message_lists):\n",
                "                common_prefix_len += 1\n",
                "            else:\n",
                "                break\n",
                "\n",
                "        user_text = \"\"\n",
                "        if common_prefix_len > 0:\n",
                "            common_prefix_messages = message_lists[0][:common_prefix_len]\n",
                "            user_text += (\n",
                "                \"<context>\\n\" + json.dumps(common_prefix_messages) + \"\\n</context>\\n\\n\"\n",
                "            )\n",
                "\n",
                "        # Serialize rollouts without common prefix\n",
                "        serialized_rollouts = []\n",
                "        for idx, (traj, full_messages) in enumerate(\n",
                "            zip(rollouts, message_lists), start=1\n",
                "        ):\n",
                "            traj.metrics[\"independent_reward\"] = traj.reward\n",
                "            trimmed_messages = full_messages[common_prefix_len:]\n",
                "            serialized_rollouts.append(\n",
                "                f'<rollout id=\"{idx}\">\\n'\n",
                "                + json.dumps(trimmed_messages)\n",
                "                + \"\\n</rollout>\"\n",
                "            )\n",
                "\n",
                "        user_text += \"Rollouts:\\n\\n\" + \"\\n\\n\".join(serialized_rollouts)\n",
                "\n",
                "        judge_prompt = f\"\"\"\n",
                "All of the rollouts below have been given the same goal. Your job is to consider each of them and give them a score between 0 and 1.\n",
                "\n",
                "Grading standards:\n",
                "{self.rubric}\n",
                "\n",
                "Existing issues:\n",
                "{json.dumps([issue.model_dump() for issue in self.all_issues], indent=2)}\n",
                "\"\"\"\n",
                "\n",
                "        messages = [\n",
                "            {\"role\": \"system\", \"content\": judge_prompt},\n",
                "            {\"role\": \"user\", \"content\": user_text},\n",
                "        ]\n",
                "\n",
                "        response = await acompletion(\n",
                "            model=self.judge_model,\n",
                "            messages=messages,\n",
                "            response_format=JudgeGroupResponse,\n",
                "            caching=True,\n",
                "        )\n",
                "\n",
                "        first_choice = response.choices[0]\n",
                "        content = first_choice.message.content or \"{}\"\n",
                "        parsed = JudgeGroupResponse.model_validate_json(content)\n",
                "\n",
                "        # Merge new issues\n",
                "        if parsed.new_issues:\n",
                "            existing_labels = {fm.label for fm in self.all_issues}\n",
                "            for fm in parsed.new_issues:\n",
                "                if fm.label not in existing_labels:\n",
                "                    self.all_issues.append(fm)\n",
                "                    existing_labels.add(fm.label)\n",
                "\n",
                "        # Apply scores\n",
                "        for traj, score in zip(rollouts, parsed.scores):\n",
                "            traj.metrics[\"group_judge_score\"] = score.score\n",
                "            traj.reward = (\n",
                "                score.score\n",
                "                if traj.metrics.get(\"failed_format_validation\", 0) == 0\n",
                "                else 0\n",
                "            )\n",
                "            traj.logs.append(f\"Judge group explanation: {score.explanation}\")\n",
                "\n",
                "            # Record issue metrics\n",
                "            for issue in self.all_issues:\n",
                "                metric_key = f\"issues/{issue.severity}/{issue.label}\"\n",
                "                traj.metrics[metric_key] = issue.label in score.issues\n",
                "\n",
                "        return rollouts\n",
                "\n",
                "\n",
                "print(\"‚úÖ Group judgment functionality loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "UU1SXTZqdrAs",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "## Let's Train! üöÄ\n",
                "\n",
                "Ok, now that we have our dataset, environment, and reward function, we can train our model!\n",
                "\n",
                "We're using our open source ART library for all the training. ART (Agent Reinforcement Trainer) is purpose-built to make it easy to train real-world multi-turn agents using Group Relative Policy Optimization (GRPO).\n",
                "\n",
                "**The GRPO training loop is beautifully simple:**\n",
                "\n",
                "1. **Load a batch** of 12 questions (and their correct answers) from our dataset\n",
                "2. **Generate trajectories** - For each question, run the agent 4 times using our rollout function\n",
                "3. **Score everything** - Use our reward function + LLM judge to score all 4 trajectories  \n",
                "4. **Learn from the best** - Use all 48 trajectories and their rewards to calculate loss and update the model\n",
                "5. **Repeat** until the model stops improving\n",
                "\n",
                "**What you'll see:** Your agent literally getting smarter in real-time. It will start by making random searches, then gradually learn which keywords work, how to read emails efficiently, and when to say \\\"I don't know\\\" instead of hallucinating.\n",
                "\n",
                "This is reinforcement learning in action - learning from experience, not just memorizing patterns. Let's watch it happen!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "colab": {
                    "referenced_widgets": [
                        "a26f9c3cb5d34926a39e91d6434ab95f",
                        "785d2999790648fe867f4ee35d2dd5d7",
                        "0f652268426f49b0b3cda25e8cb6bac2",
                        "ee9139e25afd42aaa0ec331477dbeb5d",
                        "c69347ccfe29461e929fb35f12edda11"
                    ]
                },
                "id": "YmquWXbvdrAs",
                "outputId": "e05b7950-e470-4f41-d12d-66ac568fbe64"
            },
            "outputs": [],
            "source": [
                "# Main Training Function\n",
                "# Complete training orchestration for email search agent\n",
                "\n",
                "from art.local import LocalBackend\n",
                "from art.utils import iterate_dataset\n",
                "import statistics\n",
                "import os\n",
                "\n",
                "# Ensure database is ready (will skip if already exists)\n",
                "generate_database()\n",
                "\n",
                "config = ProjectPolicyConfig(\n",
                "    max_turns=5,\n",
                "    max_tokens=1024,\n",
                "    stupid_simple_reward_fn=True,  # Use simple reward for testing\n",
                "    training_config=TrainingConfig(\n",
                "        trajectories_per_group=4,  # Small numbers for testing\n",
                "        groups_per_step=24,\n",
                "        learning_rate=1e-5,\n",
                "        eval_steps=5,\n",
                "        val_set_size=10,\n",
                "        training_dataset_size=20,\n",
                "        num_epochs=4,\n",
                "        minimum_reward_std_dev=0.0,\n",
                "    ),\n",
                ")\n",
                "model = art.TrainableModel(\n",
                "    name=\"agent-001\",\n",
                "    project=\"art-e-tutorial\",\n",
                "    config=config,\n",
                "    base_model=\"Qwen/Qwen2.5-7B-Instruct\",  # Use smaller model for testing\n",
                ")\n",
                "\n",
                "if model.config.training_config is None:\n",
                "    raise ValueError(\"Training config is not set\")\n",
                "\n",
                "group_judge = GroupJudge(\n",
                "    project=model.project,\n",
                "    judge_model=model.config.training_config.group_judge_model,\n",
                ")\n",
                "\n",
                "with LocalBackend() as backend:\n",
                "    print(\n",
                "        f\"üîÑ Pulling from S3 bucket: `{os.environ.get('BACKUP_BUCKET', 'default-bucket')}`\"\n",
                "    )\n",
                "\n",
                "    # Try to pull from S3 if bucket is configured\n",
                "    if \"BACKUP_BUCKET\" in os.environ:\n",
                "        await backend._experimental_pull_from_s3(\n",
                "            model,\n",
                "            s3_bucket=os.environ[\"BACKUP_BUCKET\"],\n",
                "            verbose=True,\n",
                "        )\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è  No S3 bucket configured, skipping pull\")\n",
                "\n",
                "    await model.register(backend)\n",
                "\n",
                "    print(\"üìö Loading training data...\")\n",
                "    tc = model.config.training_config\n",
                "    seed = tc.training_dataset_seed if tc is not None else None\n",
                "    train_scenarios = load_synthetic_queries(\n",
                "        split=\"train\",\n",
                "        limit=tc.training_dataset_size if tc is not None else None,\n",
                "        seed=seed,\n",
                "    )\n",
                "    print(\"üìö Loading validation data...\")\n",
                "    val_scenarios = load_synthetic_queries(\n",
                "        split=\"test\", limit=model.config.training_config.val_set_size\n",
                "    )\n",
                "\n",
                "    print(f\"üìä Training data size: {len(train_scenarios)}\")\n",
                "    print(f\"üìä Validation data size: {len(val_scenarios)}\")\n",
                "\n",
                "    train_iterator = iterate_dataset(\n",
                "        train_scenarios,\n",
                "        groups_per_step=model.config.training_config.groups_per_step,\n",
                "        num_epochs=model.config.training_config.num_epochs,\n",
                "        initial_step=await model.get_step(),\n",
                "    )\n",
                "\n",
                "    for batch, epoch, global_step, epoch_step in train_iterator:\n",
                "        if global_step % model.config.training_config.eval_steps == 0:\n",
                "            print(f\"\\nüîç Evaluating at Iteration {global_step}\")\n",
                "            # Note: Evaluation/benchmarking code removed as requested\n",
                "            await model.delete_checkpoints()\n",
                "\n",
                "            if \"BACKUP_BUCKET\" in os.environ:\n",
                "                await backend._experimental_push_to_s3(\n",
                "                    model,\n",
                "                    s3_bucket=os.environ[\"BACKUP_BUCKET\"],\n",
                "                )\n",
                "\n",
                "        print(f\"üéØ Generating trajectories for step {global_step}...\")\n",
                "        groups = await art.gather_trajectory_groups(\n",
                "            (\n",
                "                art.TrajectoryGroup(\n",
                "                    (\n",
                "                        rollout(model, scenario)\n",
                "                        for _ in range(\n",
                "                            model.config.training_config.trajectories_per_group\n",
                "                        )\n",
                "                    )\n",
                "                )\n",
                "                for scenario in batch\n",
                "            )\n",
                "        )\n",
                "\n",
                "        # Apply group judge if configured\n",
                "        training_cfg = model.config.training_config\n",
                "        print(\"‚öñÔ∏è  Applying group judge\")\n",
                "        judge_tasks = [\n",
                "            group_judge.judge(cast(list[ProjectTrajectory], g.trajectories))\n",
                "            for g in groups\n",
                "        ]\n",
                "\n",
                "        results = await asyncio.gather(*judge_tasks, return_exceptions=True)\n",
                "\n",
                "        successful_groups = []\n",
                "        for grp_idx, (g, res) in enumerate(zip(groups, results)):\n",
                "            if isinstance(res, Exception):\n",
                "                print(\n",
                "                    f\"‚ö†Ô∏è  WARNING: Judge group failed for group {grp_idx} at step {global_step}: {res!r}\"\n",
                "                )\n",
                "            else:\n",
                "                successful_groups.append(g)\n",
                "\n",
                "        groups = successful_groups\n",
                "\n",
                "        for g in groups:\n",
                "            for t in g.trajectories:\n",
                "                report_trajectory(model, t, global_step)\n",
                "\n",
                "        if not groups:\n",
                "            print(\n",
                "                f\"‚ö†Ô∏è  WARNING: All judge groups failed at step {global_step}; skipping training step\"\n",
                "            )\n",
                "            continue\n",
                "\n",
                "        # Filter groups by reward standard deviation\n",
                "        if (\n",
                "            training_cfg.minimum_reward_std_dev is not None\n",
                "            and training_cfg.minimum_reward_std_dev > 0\n",
                "        ):\n",
                "            print(\n",
                "                f\"üìä Filtering groups by reward std dev (min: {training_cfg.minimum_reward_std_dev})...\"\n",
                "            )\n",
                "            filtered_groups = []\n",
                "            for grp_idx, g in enumerate(groups):\n",
                "                rewards = [t.reward for t in g.trajectories]\n",
                "                if len(rewards) < 2:\n",
                "                    std_dev = 0.0\n",
                "                else:\n",
                "                    std_dev = statistics.pstdev(rewards)\n",
                "                if std_dev < training_cfg.minimum_reward_std_dev:\n",
                "                    print(\n",
                "                        f\"‚ö†Ô∏è  Dropping group {grp_idx} at step {global_step} (std dev: {std_dev:.4f})\"\n",
                "                    )\n",
                "                    continue\n",
                "                filtered_groups.append(g)\n",
                "\n",
                "            groups = filtered_groups\n",
                "\n",
                "            if not groups:\n",
                "                print(\n",
                "                    f\"‚ö†Ô∏è  WARNING: All groups dropped due to low std dev at step {global_step}; skipping training step\"\n",
                "                )\n",
                "                continue\n",
                "\n",
                "        print(f\"üéì Training model with {len(groups)} groups...\")\n",
                "        await model.train(\n",
                "            groups,\n",
                "            config=art.TrainConfig(\n",
                "                learning_rate=model.config.training_config.learning_rate\n",
                "            ),\n",
                "        )\n",
                "        print(f\"‚úÖ Completed training step {global_step}\")\n",
                "\n",
                "    # Final backup\n",
                "    print(\"üíæ Final model backup...\")\n",
                "    if \"BACKUP_BUCKET\" in os.environ:\n",
                "        await backend._experimental_push_to_s3(\n",
                "            model,\n",
                "            s3_bucket=os.environ[\"BACKUP_BUCKET\"],\n",
                "        )\n",
                "    print(\"üéâ Training finished successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qWq9ze-rdrAs",
                "vscode": {
                    "languageId": "raw"
                }
            },
            "source": [
                "## üéâ You Just Built ART¬∑E!\n",
                "\n",
                "**Congratulations!** You just built an email research agent that can beat o3 on this task!\n",
                "\n",
                "### What You Just Accomplished:\n",
                "- üèóÔ∏è **Built a realistic environment** using 500K real business emails\n",
                "- üß† **Created synthetic training data** with thousands of realistic question-answer pairs\n",
                "- üõ†Ô∏è **Designed a minimal but powerful toolset** (search, read, answer)\n",
                "- üéØ **Implemented a multi-objective reward function** that optimizes for accuracy AND efficiency\n",
                "- üéì **Used LLM-as-judge** to provide nuanced feedback during training\n",
                "- üöÄ **Trained with GRPO** to create an agent that learns from experience\n",
                "\n",
                "**This is the future of AI.** You've just built something that:\n",
                "- Is **faster** than o3 (fewer turns to find answers)\n",
                "- Is **cheaper** than o3 (smaller model, more efficient)\n",
                "- Is **more accurate** than o3 (better at finding the right emails)\n",
                "- **Learns from experience** instead of just memorizing patterns\n",
                "\n",
                "**The possibilities are endless!** You now have the blueprint to build RL agents for any task. Email search was just the beginning - what will you build next? üåü\n",
                "\n",
                "*P.S. - Your agent is now equipped to handle the \\\"How do I RSVP for my daughter's classroom party?\\\" and \\\"What time is my brother's flight?\\\" questions that started this whole journey. So 2025! üòâ*\n"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}