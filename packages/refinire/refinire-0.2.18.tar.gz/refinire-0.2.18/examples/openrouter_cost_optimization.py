#!/usr/bin/env python3
"""
OpenRouter Cost Optimization Example
OpenRouterã‚³ã‚¹ãƒˆæœ€é©åŒ–ä¾‹

This example demonstrates cost optimization strategies when using OpenRouter,
including model selection, token management, and cost-effective workflows.
ã“ã®ä¾‹ã¯ã€OpenRouterã‚’ä½¿ç”¨ã™ã‚‹éš›ã®ã‚³ã‚¹ãƒˆæœ€é©åŒ–æˆ¦ç•¥ã‚’ç¤ºã—ã€
ãƒ¢ãƒ‡ãƒ«é¸æŠã€ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†ã€ã‚³ã‚¹ãƒˆåŠ¹ç‡çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å«ã¿ã¾ã™ã€‚

Before running this example, ensure you have:
ã“ã®ä¾‹ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š
1. Set OPENROUTER_API_KEY environment variable
   OPENROUTER_API_KEYç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
2. Install refinire: pip install -e .
   refinireã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install -e .
"""

import os
import asyncio
import time
from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
from refinire.core.llm import get_llm
from pydantic import BaseModel, Field

# Clear other provider environment variables to ensure OpenRouter is used
# ä»–ã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç’°å¢ƒå¤‰æ•°ã‚’ã‚¯ãƒªã‚¢ã—ã¦OpenRouterãŒä½¿ç”¨ã•ã‚Œã‚‹ã‚ˆã†ã«ã™ã‚‹
os.environ.pop('OLLAMA_API_KEY', None)
os.environ.pop('LMSTUDIO_API_KEY', None)
os.environ.pop('OLLAMA_BASE_URL', None)
os.environ.pop('LMSTUDIO_BASE_URL', None)

@dataclass
class ModelCostInfo:
    """
    Cost information for different models
    ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¹ãƒˆæƒ…å ±
    """
    name: str
    cost_per_1k_tokens: float  # Approximate cost in USD
    speed_tier: str  # fast, medium, slow
    quality_tier: str  # basic, good, excellent
    recommended_for: List[str]

# Cost-optimized model configurations (approximate costs)
# ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆæ¦‚ç®—ã‚³ã‚¹ãƒˆï¼‰
COST_OPTIMIZED_MODELS = {
    "budget": ModelCostInfo(
        name="meta-llama/llama-3-8b-instruct",
        cost_per_1k_tokens=0.0001,
        speed_tier="fast",
        quality_tier="good",
        recommended_for=["simple_tasks", "high_volume", "prototyping"]
    ),
    "balanced": ModelCostInfo(
        name="anthropic/claude-3-haiku",
        cost_per_1k_tokens=0.0005,
        speed_tier="fast",
        quality_tier="good",
        recommended_for=["general_purpose", "balanced_workloads"]
    ),
    "premium": ModelCostInfo(
        name="openai/gpt-4",
        cost_per_1k_tokens=0.003,
        speed_tier="medium",
        quality_tier="excellent",
        recommended_for=["complex_reasoning", "high_quality_output"]
    ),
    "efficient": ModelCostInfo(
        name="mistralai/mixtral-8x7b-instruct",
        cost_per_1k_tokens=0.0002,
        speed_tier="medium",
        quality_tier="good",
        recommended_for=["technical_tasks", "structured_output"]
    )
}

class CostTracker:
    """
    Track and estimate costs for API calls
    APIå‘¼ã³å‡ºã—ã®ã‚³ã‚¹ãƒˆã‚’è¿½è·¡ãƒ»æ¨å®š
    """
    
    def __init__(self):
        self.total_cost = 0.0
        self.call_history = []
    
    def estimate_tokens(self, text: str) -> int:
        """
        Rough token estimation (1 token â‰ˆ 4 characters)
        å¤§ã¾ã‹ãªãƒˆãƒ¼ã‚¯ãƒ³æ¨å®šï¼ˆ1ãƒˆãƒ¼ã‚¯ãƒ³â‰ˆ4æ–‡å­—ï¼‰
        """
        return len(text) // 4
    
    def calculate_cost(self, prompt: str, response: str, model_cost_info: ModelCostInfo) -> float:
        """
        Calculate cost for a single API call
        å˜ä¸€ã®APIå‘¼ã³å‡ºã—ã®ã‚³ã‚¹ãƒˆã‚’è¨ˆç®—
        """
        prompt_tokens = self.estimate_tokens(prompt)
        response_tokens = self.estimate_tokens(response)
        total_tokens = prompt_tokens + response_tokens
        
        cost = (total_tokens / 1000) * model_cost_info.cost_per_1k_tokens
        return cost
    
    def track_call(self, prompt: str, response: str, model_name: str, duration: float):
        """
        Track an API call for cost analysis
        ã‚³ã‚¹ãƒˆåˆ†æã®ãŸã‚ã®APIå‘¼ã³å‡ºã—ã‚’è¿½è·¡
        """
        model_info = None
        for info in COST_OPTIMIZED_MODELS.values():
            if info.name == model_name:
                model_info = info
                break
        
        if model_info:
            cost = self.calculate_cost(prompt, response, model_info)
            self.total_cost += cost
            
            call_record = {
                "model": model_name,
                "prompt_length": len(prompt),
                "response_length": len(response),
                "estimated_tokens": self.estimate_tokens(prompt + response),
                "estimated_cost": cost,
                "duration": duration
            }
            self.call_history.append(call_record)
            
            return call_record
        return None
    
    def get_summary(self) -> Dict:
        """
        Get cost summary and statistics
        ã‚³ã‚¹ãƒˆæ¦‚è¦ã¨çµ±è¨ˆã‚’å–å¾—
        """
        if not self.call_history:
            return {"total_cost": 0, "total_calls": 0}
        
        total_calls = len(self.call_history)
        total_tokens = sum(record["estimated_tokens"] for record in self.call_history)
        avg_cost_per_call = self.total_cost / total_calls if total_calls > 0 else 0
        
        return {
            "total_cost": self.total_cost,
            "total_calls": total_calls,
            "total_tokens": total_tokens,
            "avg_cost_per_call": avg_cost_per_call,
            "calls_by_model": {}
        }

class CostOptimizedWorkflow:
    """
    Workflow that optimizes for cost efficiency
    ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’æœ€é©åŒ–ã™ã‚‹ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
    """
    
    def __init__(self):
        self.cost_tracker = CostTracker()
        self.models = {}
    
    async def get_cost_effective_model(self, task_complexity: str, quality_requirement: str):
        """
        Select the most cost-effective model for given requirements
        æŒ‡å®šã•ã‚ŒãŸè¦ä»¶ã«å¯¾ã—ã¦æœ€ã‚‚ã‚³ã‚¹ãƒˆåŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠ
        """
        if task_complexity == "simple" and quality_requirement == "basic":
            return COST_OPTIMIZED_MODELS["budget"]
        elif task_complexity == "medium" and quality_requirement == "good":
            return COST_OPTIMIZED_MODELS["balanced"]
        elif task_complexity == "complex" or quality_requirement == "excellent":
            return COST_OPTIMIZED_MODELS["premium"]
        else:
            return COST_OPTIMIZED_MODELS["efficient"]
    
    async def generate_with_cost_tracking(self, prompt: str, model_info: ModelCostInfo, **kwargs):
        """
        Generate response with cost tracking
        ã‚³ã‚¹ãƒˆè¿½è·¡ä»˜ãã§å¿œç­”ã‚’ç”Ÿæˆ
        """
        if model_info.name not in self.models:
            self.models[model_info.name] = get_llm(provider="openrouter", model=model_info.name)
        
        model = self.models[model_info.name]
        
        start_time = time.time()
        response = await model.generate(prompt, **kwargs)
        end_time = time.time()
        
        duration = end_time - start_time
        call_record = self.cost_tracker.track_call(prompt, response, model_info.name, duration)
        
        return response, call_record

async def demonstrate_cost_comparison():
    """
    Compare costs across different models for the same task
    åŒã˜ã‚¿ã‚¹ã‚¯ã§ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¹ãƒˆã‚’æ¯”è¼ƒ
    """
    print("=== Cost Comparison Example ===")
    print("=== ã‚³ã‚¹ãƒˆæ¯”è¼ƒä¾‹ ===\n")
    
    workflow = CostOptimizedWorkflow()
    
    # Task to compare across models
    # ãƒ¢ãƒ‡ãƒ«é–“ã§æ¯”è¼ƒã™ã‚‹ã‚¿ã‚¹ã‚¯
    task_prompt = "Write a professional email to a client explaining a project delay and proposing a solution."
    
    print(f"Task: {task_prompt}")
    print(f"ã‚¿ã‚¹ã‚¯: {task_prompt}")
    print(f"Comparing costs across {len(COST_OPTIMIZED_MODELS)} models...")
    print(f"{len(COST_OPTIMIZED_MODELS)}å€‹ã®ãƒ¢ãƒ‡ãƒ«ã§ã‚³ã‚¹ãƒˆã‚’æ¯”è¼ƒä¸­...")
    print()
    
    results = []
    
    for tier, model_info in COST_OPTIMIZED_MODELS.items():
        print(f"Testing {tier} tier: {model_info.name}")
        print(f"{tier}ãƒ†ã‚£ã‚¢ã‚’ãƒ†ã‚¹ãƒˆä¸­: {model_info.name}")
        
        try:
            response, call_record = await workflow.generate_with_cost_tracking(
                task_prompt, 
                model_info, 
                temperature=0.7
            )
            
            results.append({
                "tier": tier,
                "model": model_info.name,
                "cost": call_record["estimated_cost"],
                "duration": call_record["duration"],
                "response_length": len(response),
                "response_preview": response[:100]
            })
            
            print(f"  Cost: ${call_record['estimated_cost']:.6f}")
            print(f"  ã‚³ã‚¹ãƒˆ: ${call_record['estimated_cost']:.6f}")
            print(f"  Duration: {call_record['duration']:.2f}s")
            print(f"  æœŸé–“: {call_record['duration']:.2f}ç§’")
            print(f"  Response length: {len(response)} chars")
            print(f"  å¿œç­”ã®é•·ã•: {len(response)} æ–‡å­—")
            print()
            
        except Exception as e:
            print(f"  Error: {e}")
            print(f"  ã‚¨ãƒ©ãƒ¼: {e}")
            print()
    
    # Show cost comparison summary
    # ã‚³ã‚¹ãƒˆæ¯”è¼ƒæ¦‚è¦ã‚’è¡¨ç¤º
    if results:
        print("Cost Comparison Summary:")
        print("ã‚³ã‚¹ãƒˆæ¯”è¼ƒæ¦‚è¦:")
        results.sort(key=lambda x: x["cost"])
        
        cheapest = results[0]
        most_expensive = results[-1]
        
        print(f"  Cheapest: {cheapest['tier']} (${cheapest['cost']:.6f})")
        print(f"  æœ€å®‰å€¤: {cheapest['tier']} (${cheapest['cost']:.6f})")
        print(f"  Most expensive: {most_expensive['tier']} (${most_expensive['cost']:.6f})")
        print(f"  æœ€é«˜å€¤: {most_expensive['tier']} (${most_expensive['cost']:.6f})")
        print(f"  Cost difference: {most_expensive['cost'] / cheapest['cost']:.1f}x")
        print(f"  ã‚³ã‚¹ãƒˆå·®: {most_expensive['cost'] / cheapest['cost']:.1f}å€")
        print()

async def demonstrate_batch_processing():
    """
    Demonstrate cost-efficient batch processing
    ã‚³ã‚¹ãƒˆåŠ¹ç‡çš„ãªãƒãƒƒãƒå‡¦ç†ã‚’å®Ÿæ¼”
    """
    print("=== Batch Processing Cost Optimization ===")
    print("=== ãƒãƒƒãƒå‡¦ç†ã‚³ã‚¹ãƒˆæœ€é©åŒ– ===\n")
    
    workflow = CostOptimizedWorkflow()
    
    # Simulate multiple similar tasks that can be batched
    # ãƒãƒƒãƒå‡¦ç†å¯èƒ½ãªè¤‡æ•°ã®é¡ä¼¼ã‚¿ã‚¹ã‚¯ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    individual_tasks = [
        "Summarize: 'The quarterly report shows 15% growth in sales.'",
        "Summarize: 'Customer satisfaction ratings improved by 8% this month.'",
        "Summarize: 'New product launch exceeded initial projections by 20%.'",
        "Summarize: 'Marketing campaign resulted in 300% increase in website traffic.'",
        "Summarize: 'Employee retention rate improved to 94% after policy changes.'"
    ]
    
    # Method 1: Individual processing (less efficient)
    # æ–¹æ³•1: å€‹åˆ¥å‡¦ç†ï¼ˆåŠ¹ç‡ãŒæ‚ªã„ï¼‰
    print("Method 1: Individual Processing")
    print("æ–¹æ³•1: å€‹åˆ¥å‡¦ç†")
    
    model_info = COST_OPTIMIZED_MODELS["budget"]
    individual_cost = 0
    
    for i, task in enumerate(individual_tasks, 1):
        response, call_record = await workflow.generate_with_cost_tracking(
            task, model_info, temperature=0.3
        )
        individual_cost += call_record["estimated_cost"]
        print(f"  Task {i}: ${call_record['estimated_cost']:.6f}")
    
    print(f"  Total individual cost: ${individual_cost:.6f}")
    print(f"  å€‹åˆ¥å‡¦ç†ã®ç·ã‚³ã‚¹ãƒˆ: ${individual_cost:.6f}")
    print()
    
    # Method 2: Batch processing (more efficient)
    # æ–¹æ³•2: ãƒãƒƒãƒå‡¦ç†ï¼ˆã‚ˆã‚ŠåŠ¹ç‡çš„ï¼‰
    print("Method 2: Batch Processing")
    print("æ–¹æ³•2: ãƒãƒƒãƒå‡¦ç†")
    
    batch_prompt = "Summarize each of the following business updates in one sentence:\n\n"
    for i, task in enumerate(individual_tasks, 1):
        batch_prompt += f"{i}. {task.replace('Summarize: ', '')}\n"
    
    batch_response, batch_call_record = await workflow.generate_with_cost_tracking(
        batch_prompt, model_info, temperature=0.3
    )
    
    print(f"  Batch cost: ${batch_call_record['estimated_cost']:.6f}")
    print(f"  ãƒãƒƒãƒã‚³ã‚¹ãƒˆ: ${batch_call_record['estimated_cost']:.6f}")
    print(f"  Cost savings: ${individual_cost - batch_call_record['estimated_cost']:.6f}")
    print(f"  ã‚³ã‚¹ãƒˆç¯€ç´„: ${individual_cost - batch_call_record['estimated_cost']:.6f}")
    print(f"  Efficiency gain: {individual_cost / batch_call_record['estimated_cost']:.1f}x")
    print(f"  åŠ¹ç‡æ”¹å–„: {individual_cost / batch_call_record['estimated_cost']:.1f}å€")
    print()

async def demonstrate_smart_caching():
    """
    Demonstrate smart caching to reduce API calls
    APIå‘¼ã³å‡ºã—ã‚’å‰Šæ¸›ã™ã‚‹ã‚¹ãƒãƒ¼ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã‚’å®Ÿæ¼”
    """
    print("=== Smart Caching Example ===")
    print("=== ã‚¹ãƒãƒ¼ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ä¾‹ ===\n")
    
    class CachedWorkflow(CostOptimizedWorkflow):
        def __init__(self):
            super().__init__()
            self.response_cache = {}
        
        async def generate_with_cache(self, prompt: str, model_info: ModelCostInfo, **kwargs):
            """
            Generate response with caching
            ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ä»˜ãã§å¿œç­”ã‚’ç”Ÿæˆ
            """
            cache_key = f"{model_info.name}:{hash(prompt)}:{kwargs.get('temperature', 0.7)}"
            
            if cache_key in self.response_cache:
                print(f"  âœ“ Cache hit for prompt: {prompt[:50]}...")
                print(f"  âœ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {prompt[:50]}...")
                return self.response_cache[cache_key], None
            
            response, call_record = await self.generate_with_cost_tracking(
                prompt, model_info, **kwargs
            )
            
            self.response_cache[cache_key] = response
            print(f"  âœ“ Generated and cached: {prompt[:50]}...")
            print(f"  âœ“ ç”Ÿæˆã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {prompt[:50]}...")
            
            return response, call_record
    
    cached_workflow = CachedWorkflow()
    model_info = COST_OPTIMIZED_MODELS["balanced"]
    
    # Simulate repeated queries
    # ç¹°ã‚Šè¿”ã—ã‚¯ã‚¨ãƒªã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
    queries = [
        "What is machine learning?",
        "Explain neural networks briefly.",
        "What is machine learning?",  # Duplicate
        "How does deep learning work?",
        "What is machine learning?",  # Duplicate
        "Explain neural networks briefly.",  # Duplicate
    ]
    
    print(f"Processing {len(queries)} queries (some duplicates)...")
    print(f"{len(queries)}å€‹ã®ã‚¯ã‚¨ãƒªã‚’å‡¦ç†ä¸­ï¼ˆä¸€éƒ¨é‡è¤‡ï¼‰...")
    print()
    
    total_cost = 0
    cache_hits = 0
    
    for i, query in enumerate(queries, 1):
        print(f"Query {i}: {query}")
        print(f"ã‚¯ã‚¨ãƒª {i}: {query}")
        
        response, call_record = await cached_workflow.generate_with_cache(
            query, model_info, temperature=0.5
        )
        
        if call_record:
            total_cost += call_record["estimated_cost"]
            print(f"  Cost: ${call_record['estimated_cost']:.6f}")
            print(f"  ã‚³ã‚¹ãƒˆ: ${call_record['estimated_cost']:.6f}")
        else:
            cache_hits += 1
            print(f"  Cost: $0.000000 (cached)")
            print(f"  ã‚³ã‚¹ãƒˆ: $0.000000 (ã‚­ãƒ£ãƒƒã‚·ãƒ¥)")
        
        print()
    
    print(f"Total cost: ${total_cost:.6f}")
    print(f"ç·ã‚³ã‚¹ãƒˆ: ${total_cost:.6f}")
    print(f"Cache hits: {cache_hits}/{len(queries)}")
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {cache_hits}/{len(queries)}")
    print(f"Cost savings from caching: {cache_hits / len(queries) * 100:.1f}%")
    print(f"ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã«ã‚ˆã‚‹ã‚³ã‚¹ãƒˆç¯€ç´„: {cache_hits / len(queries) * 100:.1f}%")
    print()

async def cost_optimization_recommendations():
    """
    Provide cost optimization recommendations
    ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®æ¨å¥¨äº‹é …ã‚’æä¾›
    """
    print("=== Cost Optimization Recommendations ===")
    print("=== ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®æ¨å¥¨äº‹é … ===\n")
    
    recommendations = [
        {
            "title": "Model Selection Strategy",
            "title_ja": "ãƒ¢ãƒ‡ãƒ«é¸æŠæˆ¦ç•¥",
            "tips": [
                "Use budget models for simple tasks (summarization, basic Q&A)",
                "Reserve premium models for complex reasoning and creative tasks",
                "Consider model switching based on task complexity"
            ],
            "tips_ja": [
                "å˜ç´”ãªã‚¿ã‚¹ã‚¯ï¼ˆè¦ç´„ã€åŸºæœ¬çš„ãªQ&Aï¼‰ã«ã¯äºˆç®—ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨",
                "è¤‡é›‘ãªæ¨è«–ã‚„å‰µé€ çš„ãªã‚¿ã‚¹ã‚¯ã«ã¯é«˜å“è³ªãƒ¢ãƒ‡ãƒ«ã‚’äºˆç´„",
                "ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã“ã¨ã‚’æ¤œè¨"
            ]
        },
        {
            "title": "Batch Processing",
            "title_ja": "ãƒãƒƒãƒå‡¦ç†",
            "tips": [
                "Combine similar tasks into single API calls",
                "Use structured prompts for multiple items",
                "Reduce overhead from multiple API calls"
            ],
            "tips_ja": [
                "é¡ä¼¼ã‚¿ã‚¹ã‚¯ã‚’å˜ä¸€ã®APIå‘¼ã³å‡ºã—ã«ã¾ã¨ã‚ã‚‹",
                "è¤‡æ•°ã‚¢ã‚¤ãƒ†ãƒ ã«ã¯æ§‹é€ åŒ–ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨",
                "è¤‡æ•°ã®APIå‘¼ã³å‡ºã—ã‹ã‚‰ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›"
            ]
        },
        {
            "title": "Smart Caching",
            "title_ja": "ã‚¹ãƒãƒ¼ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°",
            "tips": [
                "Cache frequently requested responses",
                "Implement TTL for cache entries",
                "Use semantic similarity for cache matching"
            ],
            "tips_ja": [
                "é »ç¹ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã•ã‚Œã‚‹å¿œç­”ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥",
                "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã«TTLã‚’å®Ÿè£…",
                "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒãƒƒãƒãƒ³ã‚°ã«ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯é¡ä¼¼åº¦ã‚’ä½¿ç”¨"
            ]
        },
        {
            "title": "Token Management",
            "title_ja": "ãƒˆãƒ¼ã‚¯ãƒ³ç®¡ç†",
            "tips": [
                "Optimize prompt length and clarity",
                "Use shorter system messages when possible",
                "Consider output length requirements"
            ],
            "tips_ja": [
                "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã¨æ˜ç¢ºæ€§ã‚’æœ€é©åŒ–",
                "å¯èƒ½ãªå ´åˆã¯ã‚ˆã‚ŠçŸ­ã„ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½¿ç”¨",
                "å‡ºåŠ›é•·è¦ä»¶ã‚’è€ƒæ…®"
            ]
        }
    ]
    
    for rec in recommendations:
        print(f"ğŸ“Š {rec['title']} / {rec['title_ja']}")
        for tip, tip_ja in zip(rec['tips'], rec['tips_ja']):
            print(f"  â€¢ {tip}")
            print(f"    {tip_ja}")
        print()

async def main():
    """
    Main function to run all cost optimization examples
    ã™ã¹ã¦ã®ã‚³ã‚¹ãƒˆæœ€é©åŒ–ä¾‹ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    # Check if OpenRouter API key is set
    # OpenRouter APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    if not os.getenv('OPENROUTER_API_KEY'):
        print("ERROR: OPENROUTER_API_KEY environment variable not set")
        print("ã‚¨ãƒ©ãƒ¼: OPENROUTER_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("Please set it with: export OPENROUTER_API_KEY=your_api_key")
        print("æ¬¡ã®ã‚³ãƒãƒ³ãƒ‰ã§è¨­å®šã—ã¦ãã ã•ã„: export OPENROUTER_API_KEY=your_api_key")
        return
    
    print("OpenRouter Cost Optimization Examples")
    print("OpenRouterã‚³ã‚¹ãƒˆæœ€é©åŒ–ä¾‹")
    print("=" * 50)
    print()
    
    # Run cost optimization examples
    # ã‚³ã‚¹ãƒˆæœ€é©åŒ–ä¾‹ã‚’å®Ÿè¡Œ
    await demonstrate_cost_comparison()
    await demonstrate_batch_processing()
    await demonstrate_smart_caching()
    await cost_optimization_recommendations()
    
    print("=" * 50)
    print("All cost optimization examples completed!")
    print("ã™ã¹ã¦ã®ã‚³ã‚¹ãƒˆæœ€é©åŒ–ä¾‹ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print("\nRemember: These are estimated costs based on approximate token counts.")
    print("æ³¨æ„: ã“ã‚Œã‚‰ã¯è¿‘ä¼¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆã«åŸºã¥ãæ¨å®šã‚³ã‚¹ãƒˆã§ã™ã€‚")
    print("Actual costs may vary based on OpenRouter's pricing and model availability.")
    print("å®Ÿéš›ã®ã‚³ã‚¹ãƒˆã¯ã€OpenRouterã®ä¾¡æ ¼è¨­å®šã¨ãƒ¢ãƒ‡ãƒ«ã®åˆ©ç”¨å¯èƒ½æ€§ã«ã‚ˆã‚Šç•°ãªã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚")

if __name__ == "__main__":
    asyncio.run(main())