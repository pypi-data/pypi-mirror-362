# coding=utf-8
# Copyright 2025 DeepMind Technologies Limited.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Library for distributed noise generation in JAX.

This library provides utilities for generating correlated noise efficiently
in distributed environments. The main entry point is
`streaming_matrix_to_sharded_privatizer`, which accepts a streaming matrix and
generates a `GradientPrivatizer` object that can be used to privatize
gradient streams.

This file is based on the implementation described in [Scaling up the Banded
Matrix Factorization Mechanism for Differentially Private
ML](https://arxiv.org/abs/2405.15913). It is additionally described alongside
other alternatives in [Correlated Noise Mechanisms for Differentially Private
Learning](https://arxiv.org/abs/2506.08201). It is designed to produce general
PyTree-structured noise, and is carefully designed to handle edge cases
correctly when the size of the leaves in the pytree are are not evenly
divisible by the number of devices.

The correlated noise generated by this library will be sharded according to the
input `sum_of_clipped_grads` to privatizer.privatize. Internally, the state
required to generate this noise will be sharded across all devices in the mesh
to save memory.Therefore, the efficiency and scalability of noise generation is
primarily determined by the per-device memory usage, rather than the total
memory, and is hence dependent on the number of devices.

This library relies heavy on JAX's
[Distributed arrays and automatic parallelization](
https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html).
"""

import functools
import math
from typing import Any, TypeAlias

import jax
# pylint: disable=g-importing-member
from jax.experimental.shard import reshard
import numpy as np

from ..dp_sgd import typing
from ..matrix_factorization import streaming_matrix
from . import gradient_privatizer


# pylint: disable=invalid-name

PyTree: TypeAlias = Any
PartitionSpecPyTree: TypeAlias = Any


def _padded_size(x: jax.Array) -> int:
  """Array size, padded to the next multiple of jax.device_count()."""
  unpadded = x.size
  num_devices = jax.device_count()
  remainder = unpadded % num_devices
  return unpadded + num_devices - remainder if remainder != 0 else unpadded


def _flatten_pspec(p: jax.sharding.PartitionSpec) -> jax.sharding.PartitionSpec:
  """Flatten a PartitionSpec from a nD sharding to an "equivalent" 1D sharding.

  Args:
    p: A PartitionSpec defined over a nD array.

  Returns:
    A PartitionSpec defined over the same devices / mesh axes as p, but defined
    wrt a flattened version of the original array.
  """
  # E.g., (None, ('x', 'y'), None, 'z') --> ('x', 'y', 'z').
  # E.g., ('data', None, ('replica', 'mdl')) -> ('data', 'replica', 'mdl')
  result = []
  for item in p:
    if isinstance(item, tuple):
      result.extend(item)
    elif isinstance(item, str):
      result.append(item)
    elif item is None:
      continue
    else:
      raise ValueError(f'Unexpected item in PartitionSpec: {item}.')
  return jax.sharding.PartitionSpec(tuple(result))


def _tree_unzip(tree, treedef):
  """Given a tree of tuples, return a tuple of trees."""
  leaves = treedef.flatten_up_to(tree)
  return tuple(treedef.unflatten(x) for x in zip(*leaves))


def _reshape_add(
    x: jax.Array,
    y: jax.Array,
) -> jax.Array:
  """Reshapes y[:x.size] into x.shape and x.sharding and adds to x.

  The output of this function will respect the out_sharding.  Generally,
  the input x should also be sharded according to out_sharding.  Internally,
  this function uses JAX's shard_map primitive to perform a local reshape on
  each device, avoiding the need to transfer data between devices.

  Note that this function is not quite equivalent to
  x + y[:x.size].reshape(x.shape).  In particular, the precise mapping between
  the entries of y and it's reshaped version is not in general the same
  as jnp.reshape.  We do guarantee (and test) that each entry of y will be
  mapped to at most one entry of the reshaped version.  Since y are typically
  i.i.d. random bits, the precise mapping is not important for our purposes.

  Args:
    x: The first array.  Should be sharded according to out_sharding.
    y: The second array.  Should be 1D, and sharded across all devices in the
      out_sharding.mesh, and have size greater than or equal to x.size.

  Returns:
    x + reshape(y[:x.size], x.shape), with sharding equal to out_sharding.
  """
  out_sharding = jax.typeof(x).sharding
  per_device_shape = out_sharding.shard_shape(x.shape)
  per_device_size = math.prod(per_device_shape)

  z = jax.shard_map(
      lambda v: v[:per_device_size].reshape(per_device_shape),
      mesh=out_sharding.mesh,
      # Here the input will be replicated across the mesh axes not
      # in out_sharding.spec (as desired).
      in_specs=_flatten_pspec(out_sharding.spec),
      out_specs=out_sharding.spec,
  )(y)
  return (x + z).astype(x.dtype)


def infer_state_sharding(
    noising_matrix: streaming_matrix.StreamingMatrix,
    flat_pspec: jax.sharding.PartitionSpec,
) -> PartitionSpecPyTree:
  """Infer the sharding of the state for a given noising matrix.

  Specifically, this function determines how inputs to the StreamingMatrix
  propagate to its state (which pytree leaves and axis therein). This function
  returns a pytree of PartitionSpecs where each array axis whose size matches
  the input size will be sharded according to flat_psec, while other leaves
  and leaf axis's will be replicated.

  Assumptions:
    - The noising_matrix will be used to generate 1D noise slices.
    - flat_pspec specifies how these noise slices should be sharded.
    - The state of the streaming matrix includes some arrays which have an axis
    of size equal to the size of the 1D noise slices to be produced.  If this
    assumption is violated, then the state will be fully replicated.

  Args:
    noising_matrix: The noising matrix to use.
    flat_pspec: The sharding to use for the 1D noise slices.

  Returns:
    A sharding specification for the state of the streaming matrix.
  """
  if len(flat_pspec) != 1:
    raise ValueError(f'flat_pspec must have exactly one axis, got {flat_pspec}')
  # We use JAX's tracing machinery to get the shape of the output from a
  # placeholder input.  By looking at where the "2147483647" axis size gets
  # propagated in the state, we can infer which Arrays and Array axis' of the
  # state pytree look like the input parameters.
  abstract_leaf_size = 2147483647

  def pspecs_for_placeholder_array(x):
    pspecs = [None] * x.ndim
    for i in range(x.ndim):
      if x.shape[i] == abstract_leaf_size:
        pspecs[i] = flat_pspec[0]
        break
    return jax.sharding.PartitionSpec(*pspecs)

  placeholder_state = jax.eval_shape(
      functools.partial(noising_matrix.init_multiply, (abstract_leaf_size,))
  )
  return jax.tree.map(pspecs_for_placeholder_array, placeholder_state)


def _apply_sharding_constraints_to_state(
    noising_matrix: streaming_matrix.StreamingMatrix,
    flat_sharding: jax.sharding.NamedSharding,
    leaf: PyTree,
) -> PyTree:
  """Apply sharding constraints to the state of a streaming matrix."""
  leaf_shardings = jax.tree.map(
      lambda x: jax.sharding.NamedSharding(flat_sharding.mesh, x),
      infer_state_sharding(noising_matrix, flat_sharding.spec),
  )
  return reshard(leaf, leaf_shardings)


def _extract_zero_shardings(pytree):
  """Maps leaves to a 1D sharding fully sharded over all mesh axes."""

  def extract(x):
    shd = jax.typeof(x).sharding
    return jax.sharding.NamedSharding(
        shd.mesh, jax.sharding.PartitionSpec(shd.mesh.axis_names)
    )

  return jax.tree.map(extract, pytree)


def _isotropic_gaussian_noise_fn(rng, index, stddev, example_grad):
  flat_examples, examples_treedef = jax.tree_util.tree_flatten(example_grad)
  combined_key = jax.random.fold_in(rng, index)
  rngs = jax.random.split(combined_key, len(flat_examples))
  noise = [
      stddev * jax.random.normal(r, arr.shape, arr.dtype)
      for r, arr in zip(rngs, flat_examples)
  ]
  return jax.tree_util.tree_unflatten(examples_treedef, noise)


def streaming_matrix_to_sharded_privatizer(
    noising_matrix: streaming_matrix.StreamingMatrix,
    stddev: float,
    noise_key: jax.Array,
) -> gradient_privatizer.GradientPrivatizer:
  """Construct a GradientPrivatizer from a streaming matrix.

  The returned GradientPrivatizer is internally designed to produce noise
  sharded according to the out_sharding, which should match the PyTree
  structure and shardings of the model and gradients.  However, the state
  required to produce this noise is sharded across all devices in the mesh,
  which allows this library to scale up to large noise states that may be
  encountered when running e.g., Unamplified BandMF.  In general, the
  performance characteristics of the returned GradientPrivatizer is determined
  primarily by the per-device memory usage of the internal state:

  ```
  GB_PER_FLOAT = 4 / 2**30  # Assuming float32 state
  PER_DEVICE_GB = MODEL_SIZE * NUM_BUFFERS * GB_PER_FLOAT / NUM_DEVCIES
  ```

  Here, MODEL_SIZE is the number of model parameters, NUM_BUFFERS is the number
  of buffers in the streaming matrix (number of bands for BandMF), and
  NUM_DEVICES is the number of devices in the mesh.

  NOTE: When using this function in a distributed environment, please ensure to
  enable a "partitionable" random number generator, e.g. via the
  'jax_threefry_partitionable' config option.  For more information, see
  https://jax.readthedocs.io/en/latest/jax.random.html#advanced-rng-configuration.
  See jax_privacy/examples/distributed_noise_generation.py for an example
  of how to use this function and visualize the array shardings.

  Args:
    noising_matrix: A streaming matrix representing the matrix $C^{-1}$.
    stddev: The standard deviation of the (uncorrelated) source noise. That is,
      the returned GradientPrivatizer will add noise corresponding to the rows
      of `noising_matrix @ Z`, where Z is a matrix of iid Gaussian noise with
      standard deviation `stddev`.
    noise_key: PRNGKey array representing the key to use for the
      noise-generation of this privatizer.

  Returns:
    A GradientPrivatizer object.
  """

  def init(mdl_params: typing.ParamsT) -> typing.NoiseStateT:
    def init_leaf(x, shd):
      size = (_padded_size(x),)
      # Keep chex happy and ensure consistent return type with or without JIT.
      state = jax.tree.map(jax.numpy.array, noising_matrix.init_multiply(size))
      return _apply_sharding_constraints_to_state(noising_matrix, shd, state)

    shardings = _extract_zero_shardings(mdl_params)
    return noise_key, jax.tree.map(init_leaf, mdl_params, shardings)

  def privatize(
      *,
      sum_of_clipped_grads: typing.ParamsT,
      noise_state: typing.NoiseStateT,
  ) -> tuple[typing.ParamsT, typing.NoiseStateT]:
    prng_key, state_tree = noise_state
    prng_key, sub_key = jax.random.split(prng_key)

    target_tree = jax.tree.map(
        lambda x: jax.ShapeDtypeStruct((_padded_size(x),), x.dtype),
        sum_of_clipped_grads,
    )

    zero_shardings = _extract_zero_shardings(sum_of_clipped_grads)

    iid_noise = _isotropic_gaussian_noise_fn(sub_key, 0, stddev, target_tree)
    iid_noise = reshard(iid_noise, zero_shardings)

    correlated_noise, state = _tree_unzip(
        jax.tree.map(noising_matrix.multiply_next, iid_noise, state_tree),
        jax.tree.structure(iid_noise),
    )

    noisy_grad = jax.tree.map(
        _reshape_add, sum_of_clipped_grads, correlated_noise
    )

    return noisy_grad, (prng_key, state)

  return gradient_privatizer.GradientPrivatizer(init, privatize)


def streaming_matrix_to_single_machine_privatizer(
    noising_matrix: streaming_matrix.StreamingMatrix,
    stddev: float,
    noise_key: jax.Array | None = None,
) -> gradient_privatizer.GradientPrivatizer:
  """Construct a GradientPrivatizer from a streaming matrix."""
  if jax.device_count() != 1:
    raise ValueError(
        'This function is only intended to be used on a single machine.'
        'Use streaming_matrix_to_sharded_privatizer in distributed settings.'
        f'{jax.device_count()=}.'
        f'{jax.devices()=}.'
    )
  if noise_key is None:
    noise_key = jax.random.key(np.random.randint(0, 2**32 - 1))
  with jax.sharding.use_mesh(jax.make_mesh((), ())):
    return streaming_matrix_to_sharded_privatizer(
        noising_matrix, stddev, noise_key
    )
