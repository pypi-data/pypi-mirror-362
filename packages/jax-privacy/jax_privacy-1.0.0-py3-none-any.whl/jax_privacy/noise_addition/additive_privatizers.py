# coding=utf-8
# Copyright 2025 DeepMind Technologies Limited.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Implementations of GradientPrivatizer constructors which add noise."""

import functools

import chex
import jax
from jax import numpy as jnp
import jaxtyping
import optax

from ..matrix_factorization import streaming_matrix
from . import gradient_privatizer


IntScalar = jaxtyping.Int[jaxtyping.Array, '']
FloatScalar = jaxtyping.Float[jaxtyping.Array, '']


def _cast_to_dtype(tree1, tree2):
  """Casts all leaves of tree1 to the dtype of tree2."""
  return jax.tree.map(lambda x, y: x.astype(y.dtype), tree1, tree2)


def matrix_factorization_privatizer(
    noising_matrix: jax.typing.ArrayLike | streaming_matrix.StreamingMatrix,
    *,
    noise_key: jax.Array,
    stddev: float,
) -> gradient_privatizer.GradientPrivatizer:
  """Creates a `GradientPrivatizer` adding DP-MF-FTRL noise to gradients.

  This implementation is described in Section 4.4 of [Correlated Noise
  Mechanisms for Differentially Private Learning]
  (https://arxiv.org/pdf/2506.08201). A different implementation will be used
  depending on whether the noising_matrix is a jax.Array or a StreamingMatrix.
  The dtype of the noise generated by this privatizer will be determined by the
  noising_matrix and the input gradients according to standard jax type
  promotion rules. The output of the privatize transformation will always match
  the input dtype.

  For naming of these parameters, see ../matrix_factorization/README.md.

  Args:
    noising_matrix: A matrix used to generate correlated noise. Noise samples
      will be distributed according to a multivariate Gaussian with covariance
      matrix `noising_matrix.T @ noising_matrix`.
    noise_key: a PRNGKey array representing the source of randomness.
    stddev: Standard deviation to use for the noise of this privatizer.

  Returns:
    A `GradientPrivatizer` which adds samples from Gaussian correlated by
    `noising_matrix` (i.e., samples from a Gaussian with covariance
    `noising_matrix.T @ noising_matrix`), keyed by `noise_key`, to its stream
    of gradients.
  """
  if isinstance(noising_matrix, jax.typing.ArrayLike):
    impl = _dense_matrix_factorization_privatizer
  elif isinstance(noising_matrix, streaming_matrix.StreamingMatrix):
    impl = _streaming_matrix_factorization_privatizer
  else:
    raise NotImplementedError('Unsupported noising_matrix: ', noising_matrix)

  return impl(noising_matrix, noise_key=noise_key, stddev=stddev)


gaussian_privatizer = functools.partial(
    matrix_factorization_privatizer,
    streaming_matrix.identity(),
)
gaussian_privatizer.__doc__ = (
    """Constructs `GradientPrivatizer` adding isotropic Gaussian noise."""
)


def _compute_loop_bounds(matrix_row: chex.Array) -> tuple[IntScalar, IntScalar]:
  """Computes bounds for jax.lax.fori_loop version of row-column reduction."""
  assert matrix_row.ndim == 1
  nonzero_entries = (matrix_row != 0).astype(jnp.int32)
  first_nonzero = jnp.argmax(nonzero_entries)
  # We reverse to get the last index.
  last_nonzero = matrix_row.shape[0] - jnp.argmax(nonzero_entries[::-1])
  return first_nonzero, last_nonzero


def _gaussian_linear_combination(
    matrix_row: jax.Array,
    key: jax.Array,
    shape: tuple[int, ...],
    dtype: jax.typing.DTypeLike,
) -> jax.Array:
  """Computes a linear combination of standard Gaussian random variables."""
  assert matrix_row.ndim == 1

  def loop_body(idx, partial):
    coef = matrix_row[idx].astype(dtype)
    sub_key = jax.random.fold_in(key, idx)
    return partial + coef * jax.random.normal(sub_key, shape, dtype)

  lower_bound, upper_bound = _compute_loop_bounds(matrix_row)
  loop_state = jnp.zeros(shape, dtype)
  return jax.lax.fori_loop(lower_bound, upper_bound, loop_body, loop_state)


def _dense_matrix_factorization_privatizer(
    noising_matrix: jax.typing.ArrayLike,
    *,
    noise_key: jax.Array,
    stddev: float,
) -> gradient_privatizer.GradientPrivatizer:
  """Creates a `GradientPrivatizer` from a dense matrix C^{-1}."""
  # See Section 4.4.5 of https://arxiv.org/pdf/2506.08201 (Approach 2)
  noising_matrix = jnp.asarray(noising_matrix)

  if noising_matrix.ndim != 2:
    raise ValueError(f'Expected 2D matrix, found {noising_matrix.shape=}.')

  def privatize(
      *,
      sum_of_clipped_grads: chex.ArrayTree,
      noise_state: IntScalar,
  ) -> tuple[chex.ArrayTree, IntScalar]:
    index = noise_state
    matrix_row = noising_matrix[index]
    noise = optax.tree.random_like(
        rng_key=noise_key,
        target_tree=sum_of_clipped_grads,
        sampler=functools.partial(_gaussian_linear_combination, matrix_row),
    )
    noisy_grads = optax.tree.add_scale(sum_of_clipped_grads, stddev, noise)
    return _cast_to_dtype(noisy_grads, sum_of_clipped_grads), index + 1

  init = lambda _: jnp.array(0)
  return gradient_privatizer.GradientPrivatizer(init, privatize)


def _tree_unzip(tree, treedef):
  """Given a tree of tuples, return a tuple of trees."""
  leaves = treedef.flatten_up_to(tree)
  return tuple(treedef.unflatten(x) for x in zip(*leaves))


def _streaming_matrix_factorization_privatizer(
    noising_matrix: streaming_matrix.StreamingMatrix,
    *,
    noise_key: jax.Array,
    stddev: float,
) -> gradient_privatizer.GradientPrivatizer:
  """Creates a `GradientPrivatizer` from a StreamingMatrix C^{-1}."""

  def init(model):
    state = jax.tree.map(lambda x: noising_matrix.init_multiply(x.shape), model)
    return noise_key, jax.tree.map(jnp.array, state)

  def privatize(*, sum_of_clipped_grads, noise_state):
    prng_key, inner_state = noise_state
    new_key, sub_key = jax.random.split(prng_key)

    iid_noise = optax.tree.random_like(
        rng_key=sub_key,
        target_tree=sum_of_clipped_grads,
        sampler=jax.random.normal,
    )

    corr_noise, new_state = _tree_unzip(
        jax.tree.map(noising_matrix.multiply_next, iid_noise, inner_state),
        jax.tree.structure(iid_noise),
    )
    noisy_grads = optax.tree.add_scale(sum_of_clipped_grads, stddev, corr_noise)
    new_noise_state = (new_key, new_state)

    return _cast_to_dtype(noisy_grads, sum_of_clipped_grads), new_noise_state

  return gradient_privatizer.GradientPrivatizer(init, privatize)
