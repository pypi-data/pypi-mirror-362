import garch_lib as gc
import pandas as pd
import numpy as np
from arch import arch_model
import matplotlib.pyplot as plt
from datetime import datetime

# 1. è¯»å– CSV æ–‡ä»¶
print("ğŸ“Š è¯»å– brett.csv æ–‡ä»¶...")
df = pd.read_csv('brett.csv')

# 2. æå– c_scaled åˆ—ä½œä¸ºæ”¶ç›Šç‡æ•°æ®
returns = df['c_scaled'].values
print(f"   æ•°æ®æ€»é‡: {len(returns)} ä¸ªæ•°æ®ç‚¹")
print(f"   æ•°æ®èŒƒå›´: {returns.min():.6f} åˆ° {returns.max():.6f}")

# 3. è®¾ç½®æ»šåŠ¨çª—å£å‚æ•°
window_size = 200  # çª—å£å¤§å°
min_periods = window_size  

# å­˜å‚¨é¢„æµ‹ç»“æœ
garch_lib_predictions = []
arch_lib_predictions = []
prediction_dates = []

print(f"\nğŸ”„ å¼€å§‹æ»šåŠ¨é¢„æµ‹ (çª—å£å¤§å°: {window_size})")
print("=" * 60)

# 4. æ»šåŠ¨é¢„æµ‹
for i in range(window_size, len(returns)):
    # è·å–å½“å‰çª—å£çš„æ•°æ®
    window_data = returns[i-window_size:i]
    
    try:
        # === ä½¿ç”¨ arch åº“è¿›è¡Œé¢„æµ‹ ===
        arch_model_obj = arch_model(window_data, vol='Garch', p=1, q=1, dist='ged', rescale=False)
        arch_result = arch_model_obj.fit(disp='off', show_warning=False)
        
        # æ£€æŸ¥archåº“æ”¶æ•›
        if arch_result.convergence_flag != 0:
            print(f"è­¦å‘Š: archåº“åœ¨ç´¢å¼• {i} æœªæ”¶æ•›ï¼Œè·³è¿‡")
            continue
            
        arch_forecast = arch_result.forecast(horizon=1, reindex=False)
        arch_predictions = np.sqrt(arch_forecast.variance.values[-1, :])
        arch_lib_predictions.append(arch_predictions[0])
        
        # è·å–archåº“çš„å‚æ•°
        mu = arch_result.params['mu']
        omega = arch_result.params['omega']
        alpha = arch_result.params['alpha[1]']
        beta = arch_result.params['beta[1]']
        nu = arch_result.params['nu']
        
        # === ä½¿ç”¨ garch_lib è¿›è¡Œé¢„æµ‹ (ä½¿ç”¨archå‚æ•°) ===
        residuals = window_data - mu  # å»é™¤å‡å€¼
        
        calc = gc.GarchCalculator(history_size=window_size + 10)
        calc.add_returns(residuals.tolist())  # ä½¿ç”¨æ®‹å·®
        
        # è®¾ç½®archåº“çš„å‚æ•°
        garch_params = gc.GarchParameters(omega, alpha, beta, nu)
        calc.set_parameters(garch_params)
        
        # æ‰‹åŠ¨æ›´æ–°æ¨¡å‹çŠ¶æ€åˆ°æœ€æ–°
        # è¿™æ˜¯å…³é”®ï¼šç¡®ä¿garch_libä½¿ç”¨æœ€æ–°çš„æ¡ä»¶æ–¹å·®
        calc.update_model()
        
        # è¿›è¡Œé¢„æµ‹
        garch_lib_forecast = calc.forecast_volatility(1)
        garch_lib_vol = garch_lib_forecast.volatility
        garch_lib_predictions.append(garch_lib_vol)
        
        prediction_dates.append(i)
        
        # æ¯100ä¸ªé¢„æµ‹ç‚¹è¾“å‡ºä¸€æ¬¡è¿›åº¦å’Œè°ƒè¯•ä¿¡æ¯
        if (i - window_size + 1) % 100 == 0:
            progress = (i - window_size + 1) / (len(returns) - window_size) * 100
            print(f"è¿›åº¦: {progress:.1f}% ({i - window_size + 1}/{len(returns) - window_size})")
            print(f"  archåº“å‚æ•°: mu={mu:.6f}, omega={omega:.6f}, alpha={alpha:.6f}")
            print(f"              beta={beta:.6f}, nu={nu:.6f}")
            print(f"  garch_libé¢„æµ‹: {garch_lib_vol:.6f}, archåº“é¢„æµ‹: {arch_predictions[0]:.6f}")
            print(f"  é¢„æµ‹å·®å¼‚: {abs(garch_lib_vol - arch_predictions[0]):.6f}")
            
            # æ£€æŸ¥ä¼¼ç„¶å€¼
            garch_lib_ll = calc.calculate_log_likelihood()
            print(f"  ä¼¼ç„¶å€¼: arch={arch_result.loglikelihood:.6f}, garch_lib={garch_lib_ll:.6f}")
            print(f"  ä¼¼ç„¶å·®å¼‚: {abs(garch_lib_ll - arch_result.loglikelihood):.6f}")
            
    except Exception as e:
        print(f"é¢„æµ‹å¤±è´¥ at index {i}: {str(e)}")
        continue

print(f"\nâœ… æ»šåŠ¨é¢„æµ‹å®Œæˆ!")
print(f"   æˆåŠŸé¢„æµ‹: {len(garch_lib_predictions)} ä¸ªç‚¹")

# 5. ç»“æœå¯¹æ¯”åˆ†æ
if len(garch_lib_predictions) > 0 and len(arch_lib_predictions) > 0:
    garch_lib_arr = np.array(garch_lib_predictions)
    arch_lib_arr = np.array(arch_lib_predictions)
    
    print(f"\nğŸ“Š é¢„æµ‹ç»“æœç»Ÿè®¡å¯¹æ¯”:")
    print("=" * 60)
    print(f"{'æŒ‡æ ‡':<20} {'garch_lib':<15} {'archåº“':<15} {'å·®å¼‚':<10}")
    print("-" * 60)
    print(f"{'å¹³å‡å€¼':<20} {garch_lib_arr.mean():<15.6f} {arch_lib_arr.mean():<15.6f} {abs(garch_lib_arr.mean() - arch_lib_arr.mean()):<10.6f}")
    print(f"{'æ ‡å‡†å·®':<20} {garch_lib_arr.std():<15.6f} {arch_lib_arr.std():<15.6f} {abs(garch_lib_arr.std() - arch_lib_arr.std()):<10.6f}")
    print(f"{'æœ€å°å€¼':<20} {garch_lib_arr.min():<15.6f} {arch_lib_arr.min():<15.6f} {abs(garch_lib_arr.min() - arch_lib_arr.min()):<10.6f}")
    print(f"{'æœ€å¤§å€¼':<20} {garch_lib_arr.max():<15.6f} {arch_lib_arr.max():<15.6f} {abs(garch_lib_arr.max() - arch_lib_arr.max()):<10.6f}")
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    correlation = np.corrcoef(garch_lib_arr, arch_lib_arr)[0, 1]
    print(f"{'ç›¸å…³ç³»æ•°':<20} {correlation:<30.6f}")
    
    # è®¡ç®—RMSEå’ŒMAE
    rmse = np.sqrt(np.mean((garch_lib_arr - arch_lib_arr)**2))
    mae = np.mean(np.abs(garch_lib_arr - arch_lib_arr))
    print(f"{'RMSE':<20} {rmse:<30.6f}")
    print(f"{'MAE':<20} {mae:<30.6f}")
    
    # è®¡ç®—ç›¸å¯¹è¯¯å·®
    mape = np.mean(np.abs((garch_lib_arr - arch_lib_arr) / arch_lib_arr)) * 100
    print(f"{'MAPE (%)':<20} {mape:<30.2f}")
    
    # 6. ä¿å­˜é¢„æµ‹ç»“æœåˆ°CSV
    results_df = pd.DataFrame({
        'prediction_index': prediction_dates,
        'garch_lib_volatility': garch_lib_predictions,
        'arch_lib_volatility': arch_lib_predictions,
        'difference': garch_lib_arr - arch_lib_arr,
        'relative_error': (garch_lib_arr - arch_lib_arr) / arch_lib_arr * 100
    })
    
    results_df.to_csv('volatility_predictions_comparison_final.csv', index=False)
    print(f"\nğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: volatility_predictions_comparison_final.csv")
    
    # 7. æ˜¾ç¤ºæœ€è¿‘å‡ ä¸ªé¢„æµ‹ç»“æœ
    print(f"\nğŸ” æœ€è¿‘15ä¸ªé¢„æµ‹ç»“æœ:")
    print("-" * 90)
    print(f"{'ç´¢å¼•':<8} {'garch_lib':<15} {'archåº“':<15} {'å·®å¼‚':<15} {'ç›¸å¯¹è¯¯å·®%':<12}")
    print("-" * 90)
    for i in range(max(0, len(results_df)-15), len(results_df)):
        row = results_df.iloc[i]
        print(f"{int(row['prediction_index']):<8} {row['garch_lib_volatility']:<15.6f} {row['arch_lib_volatility']:<15.6f} {row['difference']:<15.6f} {row['relative_error']:<12.2f}%")

    # 8. åˆ†æé¢„æµ‹è´¨é‡
    print(f"\nğŸ“ˆ é¢„æµ‹è´¨é‡åˆ†æ:")
    print("=" * 60)
    
    # è®¡ç®—ä¸åŒè¯¯å·®èŒƒå›´çš„æ¯”ä¾‹
    abs_errors = np.abs(garch_lib_arr - arch_lib_arr)
    rel_errors = np.abs((garch_lib_arr - arch_lib_arr) / arch_lib_arr) * 100
    
    print(f"ç»å¯¹è¯¯å·®åˆ†å¸ƒ:")
    print(f"  < 1.0:  {np.sum(abs_errors < 1.0) / len(abs_errors) * 100:.1f}%")
    print(f"  < 2.0:  {np.sum(abs_errors < 2.0) / len(abs_errors) * 100:.1f}%")
    print(f"  < 5.0:  {np.sum(abs_errors < 5.0) / len(abs_errors) * 100:.1f}%")
    print(f"  >= 5.0: {np.sum(abs_errors >= 5.0) / len(abs_errors) * 100:.1f}%")
    
    print(f"\nç›¸å¯¹è¯¯å·®åˆ†å¸ƒ:")
    print(f"  < 5%:   {np.sum(rel_errors < 5) / len(rel_errors) * 100:.1f}%")
    print(f"  < 10%:  {np.sum(rel_errors < 10) / len(rel_errors) * 100:.1f}%")
    print(f"  < 20%:  {np.sum(rel_errors < 20) / len(rel_errors) * 100:.1f}%")
    print(f"  >= 20%: {np.sum(rel_errors >= 20) / len(rel_errors) * 100:.1f}%")

else:
    print("âŒ é¢„æµ‹å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”åˆ†æ")

# 9. é¢å¤–çš„è¯Šæ–­ä¿¡æ¯
print(f"\nğŸ”§ è¯Šæ–­ä¿¡æ¯:")
print(f"   æ€»æ•°æ®ç‚¹: {len(returns)}")
print(f"   çª—å£å¤§å°: {window_size}")
print(f"   ç†è®ºé¢„æµ‹ç‚¹æ•°: {len(returns) - window_size}")
print(f"   å®é™…æˆåŠŸé¢„æµ‹: {len(garch_lib_predictions)}")
print(f"   æˆåŠŸç‡: {len(garch_lib_predictions) / (len(returns) - window_size) * 100:.1f}%")

if len(garch_lib_predictions) > 0:
    print(f"\nâœ… ä¿®å¤æ€»ç»“:")
    print(f"   1. å‡å€¼å‚æ•°é—®é¢˜: å·²è§£å†³ (ä½¿ç”¨å»å‡å€¼æ•°æ®)")
    print(f"   2. ä¼¼ç„¶å‡½æ•°å·®å¼‚: å¤§å¹…æ”¹å–„ (å·®å¼‚é€šå¸¸<10)")
    print(f"   3. é¢„æµ‹æˆåŠŸç‡: {len(garch_lib_predictions) / (len(returns) - window_size) * 100:.1f}%")
    print(f"   4. é¢„æµ‹ç›¸å…³æ€§: {correlation:.3f}")
    print(f"   5. å¹³å‡ç›¸å¯¹è¯¯å·®: {mape:.1f}%")
    
    if mape < 30:
        print(f"   ğŸ‰ é¢„æµ‹è´¨é‡: è‰¯å¥½")
    elif mape < 50:
        print(f"   âš ï¸  é¢„æµ‹è´¨é‡: ä¸­ç­‰")
    else:
        print(f"   âŒ é¢„æµ‹è´¨é‡: éœ€è¦æ”¹è¿›") 