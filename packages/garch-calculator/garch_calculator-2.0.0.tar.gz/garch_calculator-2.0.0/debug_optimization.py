import garch_lib as gc
import pandas as pd
import numpy as np
from arch import arch_model
import time
import numpy as np

# è¯»å–æ•°æ®
df = pd.read_csv('brett.csv')
returns = df['c_scaled'].values[:300]

print("ğŸ” æ·±åº¦è¯Šæ–­ä¼˜åŒ–ç®—æ³•é—®é¢˜")
print("=" * 80)

# 1. archåº“å‚æ•°ä¼°è®¡
arch_model_obj = arch_model(returns, vol='Garch', p=1, q=1, dist='ged', rescale=False)
arch_result = arch_model_obj.fit(disp='off', show_warning=False)

mu = arch_result.params['mu']
omega = arch_result.params['omega']
alpha = arch_result.params['alpha[1]']
beta = arch_result.params['beta[1]']
nu = arch_result.params['nu']

print(f"archåº“æœ€ä¼˜å‚æ•°: Î¼={mu:.6f}, Ï‰={omega:.6f}, Î±={alpha:.6f}, Î²={beta:.6f}, Î½={nu:.6f}")
print(f"archåº“ä¼¼ç„¶å€¼: {arch_result.loglikelihood:.6f}")

# 2. æµ‹è¯•garch_libåœ¨archåº“æœ€ä¼˜å‚æ•°å¤„çš„ä¼¼ç„¶å€¼
calc = gc.GarchCalculator(history_size=500)
calc.add_returns(returns.tolist())

# è®¾ç½®archåº“çš„æœ€ä¼˜å‚æ•°
arch_optimal_params = gc.GarchParameters(mu, omega, alpha, beta, nu)
calc.set_parameters(arch_optimal_params)

garch_lib_ll_at_optimal = calc.calculate_log_likelihood()
print(f"\ngarch_libåœ¨archåº“æœ€ä¼˜å‚æ•°å¤„çš„ä¼¼ç„¶å€¼: {garch_lib_ll_at_optimal:.6f}")
print(f"ä¸archåº“çš„å·®å¼‚: {abs(garch_lib_ll_at_optimal - arch_result.loglikelihood):.6f}")

# 3. æµ‹è¯•æ¢¯åº¦è®¡ç®—
print(f"\nğŸ”§ æ¢¯åº¦è®¡ç®—æµ‹è¯•:")

# åˆ›å»ºä¸€ä¸ªç®€å•çš„æ•°å€¼æ¢¯åº¦å‡½æ•°
def numerical_gradient(params, epsilon=1e-6):
    grad = []
    
    # muæ¢¯åº¦
    params_plus = gc.GarchParameters(params.mu + epsilon, params.omega, params.alpha, params.beta, params.nu)
    params_minus = gc.GarchParameters(params.mu - epsilon, params.omega, params.alpha, params.beta, params.nu)
    ll_plus = calc.calculate_log_likelihood(params_plus)
    ll_minus = calc.calculate_log_likelihood(params_minus)
    if np.isfinite(ll_plus) and np.isfinite(ll_minus):
        grad_mu = (ll_plus - ll_minus) / (2 * epsilon)
    else:
        grad_mu = 0.0
    grad.append(grad_mu)
    
    # omegaæ¢¯åº¦
    params_plus = gc.GarchParameters(params.mu, params.omega + epsilon, params.alpha, params.beta, params.nu)
    params_minus = gc.GarchParameters(params.mu, params.omega - epsilon, params.alpha, params.beta, params.nu)
    ll_plus = calc.calculate_log_likelihood(params_plus)
    ll_minus = calc.calculate_log_likelihood(params_minus)
    if np.isfinite(ll_plus) and np.isfinite(ll_minus):
        grad_omega = (ll_plus - ll_minus) / (2 * epsilon)
    else:
        grad_omega = 0.0
    grad.append(grad_omega)
    
    # alphaæ¢¯åº¦
    params_plus = gc.GarchParameters(params.mu, params.omega, params.alpha + epsilon, params.beta, params.nu)
    params_minus = gc.GarchParameters(params.mu, params.omega, params.alpha - epsilon, params.beta, params.nu)
    ll_plus = calc.calculate_log_likelihood(params_plus)
    ll_minus = calc.calculate_log_likelihood(params_minus)
    if np.isfinite(ll_plus) and np.isfinite(ll_minus):
        grad_alpha = (ll_plus - ll_minus) / (2 * epsilon)
    else:
        grad_alpha = 0.0
    grad.append(grad_alpha)
    
    # betaæ¢¯åº¦
    params_plus = gc.GarchParameters(params.mu, params.omega, params.alpha, params.beta + epsilon, params.nu)
    params_minus = gc.GarchParameters(params.mu, params.omega, params.alpha, params.beta - epsilon, params.nu)
    ll_plus = calc.calculate_log_likelihood(params_plus)
    ll_minus = calc.calculate_log_likelihood(params_minus)
    if np.isfinite(ll_plus) and np.isfinite(ll_minus):
        grad_beta = (ll_plus - ll_minus) / (2 * epsilon)
    else:
        grad_beta = 0.0
    grad.append(grad_beta)
    
    # nuæ¢¯åº¦
    params_plus = gc.GarchParameters(params.mu, params.omega, params.alpha, params.beta, params.nu + epsilon)
    params_minus = gc.GarchParameters(params.mu, params.omega, params.alpha, params.beta, params.nu - epsilon)
    ll_plus = calc.calculate_log_likelihood(params_plus)
    ll_minus = calc.calculate_log_likelihood(params_minus)
    if np.isfinite(ll_plus) and np.isfinite(ll_minus):
        grad_nu = (ll_plus - ll_minus) / (2 * epsilon)
    else:
        grad_nu = 0.0
    grad.append(grad_nu)
    
    return grad

# åœ¨æ›´åˆç†çš„å‚æ•°å¤„æµ‹è¯•æ¢¯åº¦
reasonable_params = gc.GarchParameters(0.0, 10.0, 0.1, 0.8, 2.0)
numerical_grad = numerical_gradient(reasonable_params)

print(f"åˆç†å‚æ•°å¤„çš„æ•°å€¼æ¢¯åº¦:")
print(f"  âˆ‚L/âˆ‚Î¼ = {numerical_grad[0]:.6f}")
print(f"  âˆ‚L/âˆ‚Ï‰ = {numerical_grad[1]:.6f}")
print(f"  âˆ‚L/âˆ‚Î± = {numerical_grad[2]:.6f}")
print(f"  âˆ‚L/âˆ‚Î² = {numerical_grad[3]:.6f}")
print(f"  âˆ‚L/âˆ‚Î½ = {numerical_grad[4]:.6f}")

# æµ‹è¯•åœ¨archåº“æœ€ä¼˜å‚æ•°é™„è¿‘çš„æ¢¯åº¦
near_optimal_params = gc.GarchParameters(mu, omega, alpha, beta, nu)
numerical_grad_optimal = numerical_gradient(near_optimal_params)

print(f"\narchåº“æœ€ä¼˜å‚æ•°å¤„çš„æ•°å€¼æ¢¯åº¦:")
print(f"  âˆ‚L/âˆ‚Î¼ = {numerical_grad_optimal[0]:.6f}")
print(f"  âˆ‚L/âˆ‚Ï‰ = {numerical_grad_optimal[1]:.6f}")
print(f"  âˆ‚L/âˆ‚Î± = {numerical_grad_optimal[2]:.6f}")
print(f"  âˆ‚L/âˆ‚Î² = {numerical_grad_optimal[3]:.6f}")
print(f"  âˆ‚L/âˆ‚Î½ = {numerical_grad_optimal[4]:.6f}")

# 4. æµ‹è¯•å‚æ•°ç©ºé—´çš„ä¼¼ç„¶å‡½æ•°å½¢çŠ¶
print(f"\nğŸ“Š ä¼¼ç„¶å‡½æ•°å½¢çŠ¶åˆ†æ:")

# æµ‹è¯•omegaå‚æ•°çš„å½±å“
omega_values = [0.1, 1.0, 5.0, 10.0, 15.0, 20.0, 25.0]
print(f"omegaå‚æ•°æ‰«æ (å›ºå®šå…¶ä»–å‚æ•°ä¸ºåˆå§‹å€¼):")
for omega_test in omega_values:
    test_params = gc.GarchParameters(0.0, omega_test, 0.1, 0.85, 1.5)
    if test_params.is_valid():
        ll = calc.calculate_log_likelihood(test_params)
        print(f"  Ï‰={omega_test:5.1f}: LL={ll:10.6f}")

# æµ‹è¯•alphaå‚æ•°çš„å½±å“
alpha_values = [0.05, 0.1, 0.15, 0.2, 0.25, 0.3]
print(f"\nalphaå‚æ•°æ‰«æ (å›ºå®šå…¶ä»–å‚æ•°ä¸ºåˆå§‹å€¼):")
for alpha_test in alpha_values:
    test_params = gc.GarchParameters(0.0, 0.00001, alpha_test, 0.85, 1.5)
    if test_params.is_valid():
        ll = calc.calculate_log_likelihood(test_params)
        print(f"  Î±={alpha_test:5.2f}: LL={ll:10.6f}")

# 5. æµ‹è¯•ä¼˜åŒ–ç®—æ³•çš„ç¬¬ä¸€æ­¥
print(f"\nğŸš€ ä¼˜åŒ–ç®—æ³•ç¬¬ä¸€æ­¥æµ‹è¯•:")
result = calc.estimate_parameters()
print(f"ä¼˜åŒ–ç»“æœ:")
print(f"  æ”¶æ•›: {result.converged}")
print(f"  è¿­ä»£æ¬¡æ•°: {result.iterations}")
print(f"  æœ€ç»ˆå‚æ•°: Î¼={result.parameters.mu:.6f}, Ï‰={result.parameters.omega:.6f}")
print(f"              Î±={result.parameters.alpha:.6f}, Î²={result.parameters.beta:.6f}, Î½={result.parameters.nu:.6f}")
print(f"  æœ€ç»ˆä¼¼ç„¶å€¼: {result.log_likelihood:.6f}")

# 6. æ‰‹åŠ¨æµ‹è¯•ä¸€ä¸ªç®€å•çš„æ¢¯åº¦ä¸Šå‡æ­¥éª¤
print(f"\nğŸ”§ æ‰‹åŠ¨æ¢¯åº¦ä¸Šå‡æµ‹è¯•:")
current_params = gc.GarchParameters(0.0, 10.0, 0.2, 0.7, 2.0)  # æ›´åˆç†çš„åˆå§‹å€¼
current_ll = calc.calculate_log_likelihood(current_params)
print(f"èµ·å§‹å‚æ•°: Î¼={current_params.mu:.6f}, Ï‰={current_params.omega:.6f}, Î±={current_params.alpha:.6f}, Î²={current_params.beta:.6f}, Î½={current_params.nu:.6f}")
print(f"èµ·å§‹ä¼¼ç„¶å€¼: {current_ll:.6f}")

# è®¡ç®—æ¢¯åº¦
grad = numerical_gradient(current_params, epsilon=1e-4)
print(f"æ¢¯åº¦: [{grad[0]:.6f}, {grad[1]:.6f}, {grad[2]:.6f}, {grad[3]:.6f}, {grad[4]:.6f}]")

# å°è¯•ä¸€ä¸ªå°çš„æ¢¯åº¦ä¸Šå‡æ­¥éª¤
step_size = 0.001
new_params = gc.GarchParameters(
    current_params.mu + step_size * grad[0],
    max(1e-6, current_params.omega + step_size * grad[1]),
    max(0.0, min(0.99, current_params.alpha + step_size * grad[2])),
    max(0.0, min(0.99, current_params.beta + step_size * grad[3])),
    max(1.01, current_params.nu + step_size * grad[4])
)

# ç¡®ä¿å¹³ç¨³æ€§çº¦æŸ
if new_params.alpha + new_params.beta >= 0.999:
    scale = 0.998 / (new_params.alpha + new_params.beta)
    new_params.alpha *= scale
    new_params.beta *= scale

new_ll = calc.calculate_log_likelihood(new_params)
print(f"æ¢¯åº¦æ­¥åå‚æ•°: Î¼={new_params.mu:.6f}, Ï‰={new_params.omega:.6f}, Î±={new_params.alpha:.6f}, Î²={new_params.beta:.6f}, Î½={new_params.nu:.6f}")
print(f"æ¢¯åº¦æ­¥åä¼¼ç„¶å€¼: {new_ll:.6f}")
print(f"ä¼¼ç„¶å€¼æ”¹è¿›: {new_ll - current_ll:.6f}")