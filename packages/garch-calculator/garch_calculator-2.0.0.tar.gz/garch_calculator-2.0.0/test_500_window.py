import garch_lib as gc
import pandas as pd
import numpy as np
from arch import arch_model

# è¯»å–æ•°æ®
print("ğŸ“Š è¯»å– brett.csv æ–‡ä»¶...")
df = pd.read_csv('brett.csv')
returns = df['c_scaled'].values

# ä¸“é—¨æµ‹è¯•500çª—å£å¤§å°
window_size = 500
print(f"\nğŸ” ä¸“é—¨æµ‹è¯•çª—å£å¤§å°: {window_size}")

converged_count = 0
total_count = 0
failed_cases = []

# æµ‹è¯•å‰100ä¸ª500çª—å£
for i in range(window_size, min(window_size + 100, len(returns))):
    window_data = returns[i-window_size:i]
    
    try:
        calc = gc.GarchCalculator(history_size=window_size + 10)
        calc.add_returns(window_data.tolist())
        result = calc.estimate_parameters()
        
        total_count += 1
        if result.converged:
            converged_count += 1
            print(f"âœ… ç´¢å¼• {i}: æ”¶æ•›, è¿­ä»£={result.iterations}, ä¼¼ç„¶={result.log_likelihood:.2f}")
        else:
            failed_cases.append(i)
            print(f"âŒ ç´¢å¼• {i}: æœªæ”¶æ•›, è¿­ä»£={result.iterations}, ä¼¼ç„¶={result.log_likelihood:.2f}")
            
    except Exception as e:
        print(f"ğŸ’¥ é”™è¯¯ at index {i}: {str(e)}")
        total_count += 1
        failed_cases.append(i)

convergence_rate = converged_count / total_count if total_count > 0 else 0
print(f"\nğŸ“Š 500çª—å£æ”¶æ•›ç‡: {convergence_rate:.2%} ({converged_count}/{total_count})")

# è¯¦ç»†åˆ†æç¬¬ä¸€ä¸ªå¤±è´¥æ¡ˆä¾‹
if failed_cases:
    failed_index = failed_cases[0]
    print(f"\nğŸ” è¯¦ç»†åˆ†æå¤±è´¥æ¡ˆä¾‹: ç´¢å¼• {failed_index}")
    window_data = returns[failed_index-window_size:failed_index]
    
    # æ•°æ®ç»Ÿè®¡
    print(f"æ•°æ®ç»Ÿè®¡:")
    print(f"  å‡å€¼: {np.mean(window_data):.6f}")
    print(f"  æ ‡å‡†å·®: {np.std(window_data):.6f}")
    print(f"  æœ€å°å€¼: {np.min(window_data):.6f}")
    print(f"  æœ€å¤§å€¼: {np.max(window_data):.6f}")
    print(f"  æ•°æ®ç‚¹æ•°: {len(window_data)}")
    
    # garch_libè¯¦ç»†ç»“æœ
    calc = gc.GarchCalculator(history_size=window_size + 10)
    calc.add_returns(window_data.tolist())
    garch_result = calc.estimate_parameters()
    
    print(f"\ngarch_lib è¯¦ç»†ç»“æœ:")
    print(f"  æ”¶æ•›: {garch_result.converged}")
    print(f"  è¿­ä»£æ¬¡æ•°: {garch_result.iterations}")
    print(f"  ä¼¼ç„¶å€¼: {garch_result.log_likelihood:.6f}")
    print(f"  AIC: {garch_result.aic:.6f}")
    print(f"  BIC: {garch_result.bic:.6f}")
    print(f"  å‚æ•°: Î¼={garch_result.parameters.mu:.6f}, Ï‰={garch_result.parameters.omega:.6f}")
    print(f"        Î±={garch_result.parameters.alpha:.6f}, Î²={garch_result.parameters.beta:.6f}, Î½={garch_result.parameters.nu:.6f}")
    
    # archåº“å¯¹æ¯”
    try:
        arch_model_obj = arch_model(window_data, vol='Garch', p=1, q=1, dist='ged', rescale=False)
        arch_result = arch_model_obj.fit(disp='off', show_warning=False)
        
        print(f"\narchåº“ å¯¹æ¯”ç»“æœ:")
        print(f"  ä¼¼ç„¶å€¼: {arch_result.loglikelihood:.6f}")
        print(f"  AIC: {arch_result.aic:.6f}")
        print(f"  BIC: {arch_result.bic:.6f}")
        print(f"  å‚æ•°: Ï‰={arch_result.params['omega']:.6f}, Î±={arch_result.params['alpha[1]']:.6f}")
        print(f"        Î²={arch_result.params['beta[1]']:.6f}, Î½={arch_result.params['nu']:.6f}")
        
        # ä¼¼ç„¶å€¼å·®å¼‚
        ll_diff = abs(garch_result.log_likelihood - arch_result.loglikelihood)
        print(f"\nğŸ“ˆ ä¼¼ç„¶å€¼å·®å¼‚: {ll_diff:.6f}")
        if ll_diff < 1.0:
            print("âœ… ä¼¼ç„¶å€¼éå¸¸æ¥è¿‘ï¼Œgarch_libç»“æœå®é™…ä¸Šæ˜¯åˆç†çš„ï¼")
        
    except Exception as e:
        print(f"archåº“å¤±è´¥: {str(e)}")

# æµ‹è¯•æ‰‹åŠ¨è®¾ç½®æ›´å¥½çš„åˆå§‹å‚æ•°æ˜¯å¦èƒ½æé«˜æ”¶æ•›ç‡
print(f"\nğŸ”§ æµ‹è¯•ä¼˜åŒ–çš„åˆå§‹å‚æ•°...")

improved_converged = 0
for i in failed_cases[:10]:  # æµ‹è¯•å‰10ä¸ªå¤±è´¥æ¡ˆä¾‹
    window_data = returns[i-window_size:i]
    
    calc = gc.GarchCalculator(history_size=window_size + 10)
    calc.add_returns(window_data.tolist())
    
    # è®¾ç½®åŸºäºæ•°æ®çš„æ™ºèƒ½åˆå§‹å‚æ•°
    sample_mean = np.mean(window_data)
    sample_var = np.var(window_data)
    
    # æ›´ä¿å®ˆçš„åˆå§‹å‚æ•°
    initial_params = gc.GarchParameters()
    initial_params.mu = sample_mean
    initial_params.omega = max(0.1, sample_var * 0.005)  # æ›´å°çš„omega
    initial_params.alpha = 0.03  # å¾ˆå°çš„alpha
    initial_params.beta = 0.95   # å¾ˆå¤§çš„betaï¼Œæ¥è¿‘IGARCH
    initial_params.nu = 1.8      # åˆç†çš„nu
    
    calc.set_parameters(initial_params)
    result = calc.estimate_parameters()
    
    if result.converged:
        improved_converged += 1
        print(f"âœ… ç´¢å¼• {i}: ä¼˜åŒ–åæ”¶æ•›! è¿­ä»£={result.iterations}, ä¼¼ç„¶={result.log_likelihood:.2f}")
    else:
        print(f"âŒ ç´¢å¼• {i}: ä»æœªæ”¶æ•›, è¿­ä»£={result.iterations}, ä¼¼ç„¶={result.log_likelihood:.2f}")

if failed_cases:
    improvement_rate = improved_converged / min(len(failed_cases), 10)
    print(f"\nğŸ“Š ä¼˜åŒ–åˆå§‹å‚æ•°åçš„æ”¹è¿›ç‡: {improvement_rate:.2%}") 