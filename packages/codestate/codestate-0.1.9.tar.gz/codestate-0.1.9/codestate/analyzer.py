import os
import pathlib
import re
from collections import defaultdict
import concurrent.futures
import hashlib
import subprocess
import ast

class Analyzer:
    """
    Analyzer for code statistics: lines of code, comment density, function complexity.
    """
    def __init__(self, root_dir, file_types=None, exclude_dirs=None):
        # root_dir: the directory to analyze
        # file_types: list of file extensions to include (e.g., ['.py', '.js'])
        # exclude_dirs: list of directory names to exclude
        self.root_dir = pathlib.Path(root_dir)
        self.file_types = file_types  # None means auto-detect all extensions
        self.exclude_dirs = set(exclude_dirs or ['.git', 'venv', 'node_modules'])
        self.stats = defaultdict(lambda: {
            'file_count': 0,
            'total_lines': 0,
            'comment_lines': 0,
            'function_count': 0,
            'complexity': 0,
            'todo_count': 0,
            'blank_lines': 0,
            'comment_only_lines': 0,
            'code_lines': 0
        })
        self.file_details = []  # List of per-file stats
        self.duplicates = []  # List of duplicate code info

    def analyze(self, regex_rules=None):
        # Recursively scan files and collect statistics (multithreaded, thread-safe aggregation)
        if self.file_types is None:
            files = [file_path for file_path in self._iter_files(self.root_dir) if file_path.suffix]
        else:
            files = [file_path for file_path in self._iter_files(self.root_dir) if file_path.suffix in self.file_types]
        def analyze_file_safe(file_path):
            try:
                return self._analyze_file_threadsafe(file_path)
            except Exception as e:
                print(f"Error analyzing {file_path}: {e}")
                return None
        results = []
        with concurrent.futures.ThreadPoolExecutor() as executor:
            for res in executor.map(analyze_file_safe, files):
                if res:
                    results.append(res)
        # Aggregate results
        self.stats = defaultdict(lambda: {
            'file_count': 0,
            'total_lines': 0,
            'comment_lines': 0,
            'function_count': 0,
            'complexity': 0,
            'todo_count': 0,
            'blank_lines': 0,
            'comment_only_lines': 0,
            'code_lines': 0
        })
        self.file_details = []
        for stat, file_stat in results:
            ext = file_stat['ext']
            for k in stat:
                self.stats[ext][k] += stat[k]
            self.file_details.append(file_stat)
        # Calculate comment density and average complexity
        for ext, data in self.stats.items():
            if data['file_count'] > 0:
                data['comment_density'] = data['comment_lines'] / data['total_lines'] if data['total_lines'] else 0
                data['avg_complexity'] = data['complexity'] / data['function_count'] if data['function_count'] else 0
                data['function_avg_length'] = (data['total_lines'] / data['function_count']) if data['function_count'] else 0
        self._detect_duplicates()
        self._detect_git_authors()
        self._check_naming_conventions()
        self._extract_api_doc_summaries()
        self._detect_large_warnings()
        self._analyze_git_hotspots()
        if regex_rules:
            self._check_regex_rules(regex_rules)
        self._generate_health_report()
        self._generate_grouped_stats()
        self._detect_unused_defs()
        self._analyze_api_param_type_stats()
        self._scan_security_issues()
        # Find max/min file by total_lines
        if self.file_details:
            self.max_file = max(self.file_details, key=lambda x: x['total_lines'])
            self.min_file = min(self.file_details, key=lambda x: x['total_lines'])
        else:
            self.max_file = self.min_file = None
        return self.stats

    def _analyze_file_threadsafe(self, file_path):
        # Returns (stat_dict, file_stat) for aggregation
        ext = file_path.suffix
        total_lines = 0
        comment_lines = 0
        function_count = 0
        complexity = 0
        todo_count = 0
        blank_lines = 0
        comment_only_lines = 0
        code_lines = 0
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    total_lines += 1
                    line_strip = line.strip()
                    if not line_strip:
                        blank_lines += 1
                        continue
                    is_comment = line_strip.startswith('#') or line_strip.startswith('//') or line_strip.startswith('/*') or line_strip.startswith('*')
                    if is_comment:
                        comment_lines += 1
                        if len(line_strip) == len(line):
                            comment_only_lines += 1
                    if 'TODO' in line_strip or 'FIXME' in line_strip:
                        todo_count += 1
                    if re.match(r'^(def |function |void |int |float |double |public |private |static |func )', line_strip):
                        function_count += 1
                        complexity += self._estimate_complexity(line_strip)
                    if not is_comment:
                        code_lines += 1
        except Exception as e:
            print(f"File read error in {file_path}: {e}")
        stat = {
            'file_count': 1,
            'total_lines': total_lines,
            'comment_lines': comment_lines,
            'function_count': function_count,
            'complexity': complexity,
            'todo_count': todo_count,
            'blank_lines': blank_lines,
            'comment_only_lines': comment_only_lines,
            'code_lines': code_lines
        }
        function_avg_length = (total_lines / function_count) if function_count else 0
        file_stat = {
            'path': str(file_path),
            'ext': ext,
            'total_lines': total_lines,
            'comment_lines': comment_lines,
            'function_count': function_count,
            'complexity': complexity,
            'function_avg_length': function_avg_length,
            'todo_count': todo_count,
            'blank_lines': blank_lines,
            'comment_only_lines': comment_only_lines,
            'code_lines': code_lines
        }
        return stat, file_stat

    def _scan_security_issues(self):
        # Scan for common insecure patterns
        import re
        patterns = [
            (r'\beval\s*\(', 'Use of eval()'),
            (r'\bexec\s*\(', 'Use of exec()'),
            (r'\bpickle\.load\s*\(', 'Use of pickle.load()'),
            (r'\bos\.system\s*\(', 'Use of os.system()'),
            (r'\bsubprocess\.Popen\s*\(', 'Use of subprocess.Popen()'),
            (r'\binput\s*\(', 'Use of input()'),
            (r'password\s*=\s*["\"][^"\"]+["\"]', 'Hardcoded password'),
            (r'token\s*=\s*["\"][^"\"]+["\"]', 'Hardcoded token'),
            (r'secret\s*=\s*["\"][^"\"]+["\"]', 'Hardcoded secret'),
            (r'api[_-]?key\s*=\s*["\"][^"\"]+["\"]', 'Hardcoded API key'),
        ]
        issues = []
        for file_stat in self.file_details:
            path = file_stat['path']
            try:
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    for lineno, line in enumerate(f, 1):
                        for pat, desc in patterns:
                            if re.search(pat, line, re.IGNORECASE):
                                issues.append({'file': path, 'line': lineno, 'desc': desc, 'content': line.strip(), 'note': 'Potentially false positive: regex match may include comments or strings.'})
            except Exception:
                continue
        self.security_issues = issues

    def get_security_issues(self):
        # Return list of detected security issues
        return getattr(self, 'security_issues', [])

    def _generate_grouped_stats(self):
        # Group stats by top-level directory and by extension
        from collections import defaultdict
        self.grouped_by_dir = defaultdict(lambda: {'file_count': 0, 'total_lines': 0, 'comment_lines': 0, 'function_count': 0})
        self.grouped_by_ext = defaultdict(lambda: {'file_count': 0, 'total_lines': 0, 'comment_lines': 0, 'function_count': 0})
        for f in self.file_details:
            # By directory (top-level folder)
            rel_path = os.path.relpath(f['path'], self.root_dir)
            parts = rel_path.split(os.sep)
            top_dir = parts[0] if len(parts) > 1 else '.'
            self.grouped_by_dir[top_dir]['file_count'] += 1
            self.grouped_by_dir[top_dir]['total_lines'] += f['total_lines']
            self.grouped_by_dir[top_dir]['comment_lines'] += f['comment_lines']
            self.grouped_by_dir[top_dir]['function_count'] += f['function_count']
            # By extension
            ext = f['ext']
            self.grouped_by_ext[ext]['file_count'] += 1
            self.grouped_by_ext[ext]['total_lines'] += f['total_lines']
            self.grouped_by_ext[ext]['comment_lines'] += f['comment_lines']
            self.grouped_by_ext[ext]['function_count'] += f['function_count']

    def get_grouped_stats(self, by='dir'):
        # Return grouped stats by 'dir' or 'ext'
        if by == 'dir':
            return dict(self.grouped_by_dir)
        elif by == 'ext':
            return dict(self.grouped_by_ext)
        else:
            return {}

    def _generate_health_report(self):
        # Compute a health score and suggestions
        score = 100
        suggestions = []
        # Comment density
        avg_comment_density = 0
        total_lines = 0
        total_comments = 0
        for ext, data in self.stats.items():
            total_lines += data['total_lines']
            total_comments += data['comment_lines']
        if total_lines:
            avg_comment_density = total_comments / total_lines
        if avg_comment_density < 0.05:
            score -= 10
            suggestions.append('Increase comment density (currently low).')
        # Duplicate code
        if self.duplicates and len(self.duplicates) > 0:
            score -= 10
            suggestions.append('Reduce duplicate code blocks.')
        # Large files/functions
        large_warn = getattr(self, 'large_warnings', {'files': [], 'functions': []})
        if large_warn['files']:
            score -= 5
            suggestions.append('Refactor or split large files.')
        if large_warn['functions']:
            score -= 5
            suggestions.append('Refactor or split large functions.')
        # TODO/FIXME
        todo_count = sum(f['todo_count'] for f in self.file_details)
        if todo_count > 10:
            score -= 5
            suggestions.append('Resolve outstanding TODO/FIXME comments.')
        # Naming violations
        naming_violations = getattr(self, 'naming_violations', [])
        if naming_violations:
            score -= 5
            suggestions.append('Fix function/class naming convention violations.')
        # Complexity
        avg_complexity = 0
        total_func = 0
        total_cplx = 0
        for ext, data in self.stats.items():
            total_func += data['function_count']
            total_cplx += data['complexity']
        if total_func:
            avg_complexity = total_cplx / total_func
        if avg_complexity > 3:
            score -= 5
            suggestions.append('Reduce average function complexity.')
        # Bound score
        score = max(0, min(100, score))
        self.health_report = {
            'score': score,
            'avg_comment_density': avg_comment_density,
            'avg_complexity': avg_complexity,
            'todo_count': todo_count,
            'naming_violations': len(naming_violations),
            'duplicate_blocks': len(self.duplicates) if self.duplicates else 0,
            'large_files': len(large_warn['files']),
            'large_functions': len(large_warn['functions']),
            'suggestions': suggestions
        }

    def get_health_report(self):
        # Return health report dict
        return getattr(self, 'health_report', None)

    def _analyze_git_hotspots(self):
        # Analyze git log to find most frequently changed files
        git_dir = self.root_dir / '.git'
        if not git_dir.exists():
            self.git_hotspots = None
            return
        import subprocess
        from collections import Counter
        try:
            cmd = ['git', '-C', str(self.root_dir), 'log', '--name-only', '--pretty=format:']
            output = subprocess.check_output(cmd, encoding='utf-8', errors='ignore')
            files = [line.strip() for line in output.splitlines() if line.strip()]
            counter = Counter(files)
            self.git_hotspots = counter.most_common()
        except Exception:
            self.git_hotspots = None

    def get_git_hotspots(self, top_n=10):
        # Return list of (file, commit_count) for the most frequently changed files
        if not getattr(self, 'git_hotspots', None):
            return []
        return self.git_hotspots[:top_n]

    def _detect_large_warnings(self, threshold_file=300, threshold_func=50):
        # Warn for large files and large functions (Python only for functions)
        self.large_warnings = {'files': [], 'functions': []}
        for file_stat in self.file_details:
            if file_stat['total_lines'] > threshold_file:
                self.large_warnings['files'].append({
                    'file': file_stat['path'],
                    'lines': file_stat['total_lines'],
                    'threshold': threshold_file
                })
            if file_stat['ext'] == '.py':
                path = file_stat['path']
                try:
                    with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                        tree = ast.parse(f.read(), filename=path)
                    for node in ast.walk(tree):
                        if isinstance(node, ast.FunctionDef):
                            start = node.lineno
                            end = max([n.lineno for n in ast.walk(node) if hasattr(n, 'lineno')], default=start)
                            func_lines = end - start + 1
                            if func_lines > threshold_func:
                                self.large_warnings['functions'].append({
                                    'file': path,
                                    'function': node.name,
                                    'lines': func_lines,
                                    'line': start,
                                    'threshold': threshold_func
                                })
                except Exception as e:
                    print(f"AST parse error in {path}: {e}")
                    continue

    def get_large_warnings(self, threshold_file=300, threshold_func=50):
        # Return large file/function warnings
        return getattr(self, 'large_warnings', {'files': [], 'functions': []})

    def _extract_api_doc_summaries(self):
        # Only extract for Python files
        self.api_doc_summaries = []
        for file_stat in self.file_details:
            if file_stat['ext'] != '.py':
                continue
            path = file_stat['path']
            try:
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    tree = ast.parse(f.read(), filename=path)
                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                        doc = ast.get_docstring(node)
                        self.api_doc_summaries.append({
                            'type': 'function' if isinstance(node, ast.FunctionDef) else 'class',
                            'name': node.name,
                            'file': path,
                            'line': node.lineno,
                            'docstring': doc or ''
                        })
            except Exception as e:
                print(f"AST parse error in {path}: {e}")
                continue

    def get_api_doc_summaries(self):
        # Return list of API doc summaries
        return getattr(self, 'api_doc_summaries', [])

    def _detect_duplicates(self, block_size=5):
        # Detect duplicate code blocks of block_size lines across all files
        block_map = {}  # hash -> list of (file, start_line, block)
        for file_stat in self.file_details:
            path = file_stat['path']
            try:
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()
                    for i in range(len(lines) - block_size + 1):
                        block = ''.join(lines[i:i+block_size])
                        block_hash = hashlib.md5(block.encode('utf-8')).hexdigest()
                        block_map.setdefault(block_hash, []).append((path, i+1, block))
            except Exception:
                continue
        # Collect duplicates (appearing in 2+ places)
        self.duplicates = [v for v in block_map.values() if len(v) > 1]

    def get_duplicates(self):
        # Return list of duplicate code blocks
        return self.duplicates

    def _detect_git_authors(self):
        # If .git exists, get main author and last modifier for each file
        git_dir = self.root_dir / '.git'
        if not git_dir.exists():
            self.git_authors = None
            return
        self.git_authors = {}
        for file_stat in self.file_details:
            path = file_stat['path']
            rel_path = os.path.relpath(path, self.root_dir)
            try:
                # Get main author (most commits)
                cmd = ['git', '-C', str(self.root_dir), 'log', '--format=%an', rel_path]
                authors = subprocess.check_output(cmd, encoding='utf-8', errors='ignore').splitlines()
                if authors:
                    main_author = max(set(authors), key=authors.count)
                    last_author = authors[0]
                else:
                    main_author = last_author = None
            except Exception as e:
                print(f"Git log error in {path}: {e}")
                main_author = last_author = None
            self.git_authors[path] = {'main_author': main_author, 'last_author': last_author}

    def get_git_authors(self):
        # Return dict: file path -> {'main_author', 'last_author'}
        return getattr(self, 'git_authors', None)

    def _check_naming_conventions(self):
        # Only check Python files for now
        self.naming_violations = []
        for file_stat in self.file_details:
            if file_stat['ext'] != '.py':
                continue
            path = file_stat['path']
            try:
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    tree = ast.parse(f.read(), filename=path)
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        if not self._is_snake_case(node.name):
                            self.naming_violations.append({'type': 'function', 'name': node.name, 'file': path, 'line': node.lineno, 'rule': 'snake_case'})
                    if isinstance(node, ast.ClassDef):
                        if not self._is_pascal_case(node.name):
                            self.naming_violations.append({'type': 'class', 'name': node.name, 'file': path, 'line': node.lineno, 'rule': 'PascalCase'})
            except Exception as e:
                print(f"AST parse error in {path}: {e}")
                continue

    def _is_snake_case(self, name):
        # Check if name is snake_case
        import re
        return bool(re.match(r'^[a-z_][a-z0-9_]*$', name))

    def _is_pascal_case(self, name):
        # Check if name is PascalCase
        import re
        return bool(re.match(r'^[A-Z][a-zA-Z0-9]*$', name))

    def get_naming_violations(self):
        # Return list of naming violations
        return getattr(self, 'naming_violations', [])

    def _check_regex_rules(self, regex_rules):
        # regex_rules: list of regex strings
        import re
        self.regex_matches = []
        for file_stat in self.file_details:
            path = file_stat['path']
            try:
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    for lineno, line in enumerate(f, 1):
                        for rule in regex_rules:
                            if re.search(rule, line):
                                self.regex_matches.append({
                                    'file': path,
                                    'line': lineno,
                                    'rule': rule,
                                    'content': line.strip()
                                })
            except Exception:
                continue

    def get_regex_matches(self):
        # Return list of regex matches
        return getattr(self, 'regex_matches', [])

    def _detect_unused_defs(self):
        # Only for Python files: find functions/classes defined but never used
        import ast
        defined = set()
        used = set()
        for file_stat in self.file_details:
            if file_stat['ext'] != '.py':
                continue
            path = file_stat['path']
            try:
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    tree = ast.parse(f.read(), filename=path)
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef) or isinstance(node, ast.ClassDef):
                        defined.add((node.name, path, node.lineno, type(node).__name__))
                    # Find all function/class usage (calls, instantiations)
                    if isinstance(node, ast.Call):
                        if hasattr(node.func, 'id'):
                            used.add(node.func.id)
                        elif hasattr(node.func, 'attr'):
                            used.add(node.func.attr)
                    if isinstance(node, ast.Attribute):
                        used.add(node.attr)
            except Exception as e:
                print(f"AST parse error in {path}: {e}")
                continue
        self.unused_defs = [
            {'name': name, 'file': file, 'line': line, 'type': typ}
            for (name, file, line, typ) in defined if name not in used
        ]

    def get_unused_defs(self):
        # Return list of unused function/class definitions
        return getattr(self, 'unused_defs', [])

    def _analyze_api_param_type_stats(self):
        # For Python files: count function parameters and type annotation coverage
        import ast
        total_funcs = 0
        total_params = 0
        total_annotated_params = 0
        total_annotated_returns = 0
        for file_stat in self.file_details:
            if file_stat['ext'] != '.py':
                continue
            path = file_stat['path']
            try:
                with open(path, 'r', encoding='utf-8', errors='ignore') as f:
                    tree = ast.parse(f.read(), filename=path)
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        total_funcs += 1
                        # Support posonlyargs, args, kwonlyargs
                        all_args = []
                        if hasattr(node.args, 'posonlyargs'):
                            all_args.extend(node.args.posonlyargs)
                        all_args.extend(node.args.args)
                        all_args.extend(node.args.kwonlyargs)
                        for arg in all_args:
                            total_params += 1
                            if arg.annotation is not None:
                                total_annotated_params += 1
                        if node.returns is not None:
                            total_annotated_returns += 1
            except Exception as e:
                print(f"AST parse error in {path}: {e}")
                continue
        self.api_param_type_stats = {
            'total_functions': total_funcs,
            'total_parameters': total_params,
            'annotated_parameters': total_annotated_params,
            'annotated_returns': total_annotated_returns,
            'param_annotation_coverage': (total_annotated_params / total_params) if total_params else 0,
            'return_annotation_coverage': (total_annotated_returns / total_funcs) if total_funcs else 0
        }

    def get_api_param_type_stats(self):
        # Return function parameter/type annotation statistics
        return getattr(self, 'api_param_type_stats', {})

    def _iter_files(self, root):
        # Generator that yields files, skipping excluded directories
        for dirpath, dirnames, filenames in os.walk(root):
            # Remove excluded directories in-place
            dirnames[:] = [d for d in dirnames if d not in self.exclude_dirs]
            for filename in filenames:
                yield pathlib.Path(dirpath) / filename

    def _estimate_complexity(self, line):
        # Simple cyclomatic complexity estimation: count keywords
        keywords = ['if ', 'for ', 'while ', 'case ', '&&', '||', 'elif ', 'except ', 'catch ']
        return 1 + sum(line.count(k) for k in keywords)

    def get_file_details(self):
        # Return per-file statistics
        return self.file_details

    def get_max_min_stats(self):
        # Return file with most/least lines
        return {'max_file': self.max_file, 'min_file': self.min_file} 