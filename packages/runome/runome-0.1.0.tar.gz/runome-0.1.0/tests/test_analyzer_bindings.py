#!/usr/bin/env python3
"""Test suite for Runome Analyzer Python bindings"""

import pytest


def test_charfilters():
    """Test CharFilter bindings"""
    from runome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter

    # Test UnicodeNormalizeCharFilter
    filter = UnicodeNormalizeCharFilter()
    assert filter("ï¼°ï½™ï½”ï½ˆï½ï½") == "Python"
    assert filter("ã»") == "å¹³æˆ"  # NFKC normalization

    # Test with different forms
    filter_nfc = UnicodeNormalizeCharFilter("NFC")
    filter_nfd = UnicodeNormalizeCharFilter("NFD")
    # NFC and NFD should handle combining characters differently
    text = "ãŒ"  # Hiragana GA (can be decomposed)
    assert len(filter_nfc(text)) <= len(filter_nfd(text))

    # Test RegexReplaceCharFilter
    filter = RegexReplaceCharFilter("è›‡ã®ç›®", "janome")
    assert filter("è›‡ã®ç›®ã¯å½¢æ…‹ç´ è§£æå™¨ã§ã™ã€‚") == "janomeã¯å½¢æ…‹ç´ è§£æå™¨ã§ã™ã€‚"

    # Test regex with groups
    filter = RegexReplaceCharFilter(r"(\d+)å¹´", r"\1 year")
    assert filter("2024å¹´ã§ã™") == "2024 yearã§ã™"

    # Test callable interface
    assert filter.__call__("2024å¹´ã§ã™") == "2024 yearã§ã™"


def test_tokenfilters_basic():
    """Test basic TokenFilter bindings"""
    from runome.tokenizer import Tokenizer
    from runome.tokenfilter import LowerCaseFilter, UpperCaseFilter, CompoundNounFilter

    tokenizer = Tokenizer()
    tokens = list(tokenizer.tokenize("ãƒ†ã‚¹ãƒˆTEST"))

    # Test LowerCaseFilter
    filter = LowerCaseFilter()
    filtered = list(filter(tokens))
    assert all(hasattr(t, "surface") for t in filtered)
    assert any(t.surface == "test" for t in filtered)

    # Test UpperCaseFilter
    filter = UpperCaseFilter()
    filtered = list(filter(tokens))
    assert any(t.surface == "TEST" for t in filtered)

    # Test CompoundNounFilter
    tokens = list(tokenizer.tokenize("å½¢æ…‹ç´ è§£æå™¨"))
    filter = CompoundNounFilter()
    filtered = list(filter(tokens))
    # Should combine consecutive nouns
    assert len(filtered) <= len(tokens)


def test_pos_filters():
    """Test POS-based TokenFilters"""
    from runome.tokenizer import Tokenizer
    from runome.tokenfilter import POSKeepFilter, POSStopFilter

    tokenizer = Tokenizer()
    tokens = list(tokenizer.tokenize("æ±äº¬é§…ã§é™ã‚Šã‚‹"))

    # Test POSKeepFilter
    filter = POSKeepFilter(["åè©"])
    filtered = list(filter(tokens))
    # Should only keep nouns
    assert all("åè©" in t.part_of_speech for t in filtered)
    assert any(t.surface == "æ±äº¬" for t in filtered)
    assert any(t.surface == "é§…" for t in filtered)

    # Test POSStopFilter
    filter = POSStopFilter(["åŠ©è©", "å‹•è©"])
    filtered = list(filter(tokens))
    # Should remove particles and verbs
    assert all("åŠ©è©" not in t.part_of_speech for t in filtered)
    assert all("å‹•è©" not in t.part_of_speech for t in filtered)


def test_terminal_filters():
    """Test terminal TokenFilters that change output type"""
    from runome.tokenizer import Tokenizer
    from runome.tokenfilter import (
        ExtractAttributeFilter,
        TokenCountFilter,
        POSKeepFilter,
    )

    tokenizer = Tokenizer()

    # Test ExtractAttributeFilter
    tokens = list(tokenizer.tokenize("æ±äº¬é§…"))
    filter = ExtractAttributeFilter("surface")
    results = list(filter(tokens))
    assert all(isinstance(r, str) for r in results)
    assert "æ±äº¬" in results
    assert "é§…" in results

    # Test with base_form
    filter = ExtractAttributeFilter("base_form")
    results = list(filter(tokens))
    assert all(isinstance(r, str) for r in results)

    # Test TokenCountFilter
    tokens = list(tokenizer.tokenize("ã™ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã®ã†ã¡"))
    pos_filter = POSKeepFilter(["åè©"])
    count_filter = TokenCountFilter("surface", sorted=True)

    filtered_tokens = list(pos_filter(tokens))
    results = list(count_filter(filtered_tokens))

    assert all(isinstance(r, tuple) and len(r) == 2 for r in results)
    assert all(isinstance(r[0], str) and isinstance(r[1], int) for r in results)

    # Check that results are sorted by count (descending)
    counts = [r[1] for r in results]
    assert counts == sorted(counts, reverse=True)

    # Check specific counts
    count_dict = dict(results)
    assert count_dict.get("ã‚‚ã‚‚", 0) == 2
    assert count_dict.get("ã™ã‚‚ã‚‚", 0) == 1
    assert count_dict.get("ã†ã¡", 0) == 1


def test_analyzer_basic():
    """Test basic Analyzer functionality"""
    from runome.analyzer import Analyzer
    from runome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter

    # Test default analyzer
    analyzer = Analyzer()
    results = list(analyzer.analyze("ãƒ†ã‚¹ãƒˆ"))
    assert len(results) > 0
    assert all(hasattr(t, "surface") for t in results)

    # Test with CharFilters
    analyzer = Analyzer(
        char_filters=[
            UnicodeNormalizeCharFilter(),
            RegexReplaceCharFilter("è›‡ã®ç›®", "janome"),
        ]
    )
    results = list(analyzer.analyze("è›‡ã®ç›®ã¯ï¼°ï½™ï½”ï½ˆï½ï½ã§ã™"))
    surfaces = [t.surface for t in results]
    assert "janome" in surfaces
    assert "Python" in surfaces  # Should be normalized


def test_analyzer_full_pipeline():
    """Test complete Analyzer pipeline"""
    from runome.analyzer import Analyzer
    from runome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter
    from runome.tokenfilter import CompoundNounFilter, POSStopFilter, LowerCaseFilter

    analyzer = Analyzer(
        char_filters=[
            UnicodeNormalizeCharFilter(),
            RegexReplaceCharFilter("è›‡ã®ç›®", "janome"),
        ],
        token_filters=[
            CompoundNounFilter(),
            POSStopFilter(["è¨˜å·", "åŠ©è©"]),
            LowerCaseFilter(),
        ],
    )

    text = "è›‡ã®ç›®ã¯Pure ï¼°ï½™ï½”ï½ˆï½ï½ãªå½¢æ…‹ç´ è§£æå™¨ã§ã™ã€‚"
    results = list(analyzer.analyze(text))

    # Extract surfaces
    surfaces = [t.surface for t in results]

    # Check expected tokens
    assert "janome" in surfaces
    assert "pure" in surfaces
    assert "python" in surfaces
    assert "ãª" in surfaces
    assert "å½¢æ…‹ç´ è§£æå™¨" in surfaces
    assert "ã§ã™" in surfaces

    # Check that particles were filtered out
    assert "ã¯" not in surfaces  # particle
    assert "ã€‚" not in surfaces  # symbol


def test_analyzer_with_terminal_filter():
    """Test Analyzer with terminal filters"""
    from runome.analyzer import Analyzer
    from runome.tokenfilter import (
        POSKeepFilter,
        ExtractAttributeFilter,
        TokenCountFilter,
    )

    # Test with ExtractAttributeFilter
    analyzer = Analyzer(
        token_filters=[POSKeepFilter(["åè©"]), ExtractAttributeFilter("surface")]
    )

    results = list(analyzer.analyze("æ±äº¬é§…ã§é™ã‚Šã‚‹"))
    assert all(isinstance(r, str) for r in results)
    assert "æ±äº¬" in results
    assert "é§…" in results
    assert "é™ã‚Šã‚‹" not in results  # verb should be filtered out

    # Test with TokenCountFilter
    analyzer = Analyzer(
        token_filters=[
            POSKeepFilter(["åè©"]),
            TokenCountFilter("surface", sorted=True),
        ]
    )

    results = list(analyzer.analyze("ã™ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã®ã†ã¡"))
    assert all(isinstance(r, tuple) for r in results)

    count_dict = dict(results)
    assert count_dict["ã‚‚ã‚‚"] == 2
    assert count_dict["ã™ã‚‚ã‚‚"] == 1
    assert count_dict["ã†ã¡"] == 1


def test_analyzer_wakati_rejection():
    """Test that Analyzer rejects wakati mode tokenizer"""
    from runome.analyzer import Analyzer
    from runome.tokenizer import Tokenizer

    # Create tokenizer with wakati=True
    wakati_tokenizer = Tokenizer(wakati=True)

    # Should raise exception
    with pytest.raises(Exception) as excinfo:
        Analyzer(tokenizer=wakati_tokenizer)

    assert "wakati=True" in str(excinfo.value)


def test_custom_tokenizer():
    """Test Analyzer with custom tokenizer settings"""
    from runome.analyzer import Analyzer
    from runome.tokenizer import Tokenizer
    from runome.tokenfilter import LowerCaseFilter

    # Create tokenizer with user dictionary
    # Note: This assumes no user dictionary file, but tests the parameter passing
    tokenizer = Tokenizer(max_unknown_length=512)

    analyzer = Analyzer(tokenizer=tokenizer, token_filters=[LowerCaseFilter()])

    results = list(analyzer.analyze("TEST"))
    assert any(t.surface == "test" for t in results)


def test_filter_chaining():
    """Test complex filter chaining"""
    from runome.analyzer import Analyzer
    from runome.charfilter import RegexReplaceCharFilter, UnicodeNormalizeCharFilter
    from runome.tokenfilter import CompoundNounFilter, POSKeepFilter, LowerCaseFilter

    # Create a complex pipeline
    analyzer = Analyzer(
        char_filters=[
            RegexReplaceCharFilter(r"\s+", " "),  # Normalize whitespace
            RegexReplaceCharFilter(r"[ï¼-ï½]", ""),  # Remove full-width symbols
            UnicodeNormalizeCharFilter(),
        ],
        token_filters=[
            CompoundNounFilter(),
            POSKeepFilter(["åè©", "å‹•è©", "å½¢å®¹è©"]),
            LowerCaseFilter(),
        ],
    )

    text = "æ±äº¬ã€€ã€€é§…ã§ã€€ã€€é™ã‚Šã‚‹ï¼ï¼"
    results = list(analyzer.analyze(text))

    surfaces = [t.surface for t in results]
    assert "æ±äº¬" in surfaces
    assert "é§…" in surfaces
    assert "é™ã‚Šã‚‹" in surfaces


def test_error_handling():
    """Test error handling in bindings"""
    from runome.charfilter import RegexReplaceCharFilter, UnicodeNormalizeCharFilter
    from runome.tokenfilter import ExtractAttributeFilter, TokenCountFilter

    # Test invalid regex pattern
    with pytest.raises(Exception):
        RegexReplaceCharFilter("[invalid", "replacement")

    # Test invalid normalization form
    with pytest.raises(Exception):
        UnicodeNormalizeCharFilter("INVALID")

    # Test invalid attribute name
    with pytest.raises(Exception):
        ExtractAttributeFilter("invalid_attribute")

    with pytest.raises(Exception):
        TokenCountFilter("invalid_attribute")


if __name__ == "__main__":
    # Run basic smoke test
    print("Running Analyzer bindings smoke test...")

    test_charfilters()
    print("âœ“ CharFilters working")

    test_tokenfilters_basic()
    print("âœ“ Basic TokenFilters working")

    test_pos_filters()
    print("âœ“ POS filters working")

    test_terminal_filters()
    print("âœ“ Terminal filters working")

    test_analyzer_basic()
    print("âœ“ Basic Analyzer working")

    test_analyzer_full_pipeline()
    print("âœ“ Full pipeline working")

    test_analyzer_with_terminal_filter()
    print("âœ“ Terminal filter integration working")

    test_filter_chaining()
    print("âœ“ Filter chaining working")

    print("\nAll tests passed! ğŸ‰")
