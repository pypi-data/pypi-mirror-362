# ðŸ“¦ {{ challenge.slug }} challenge pack
Thank you for hosting your challenge on Grand-Challenge.org, we appreciate it!

## Content

This Challenge Pack is a collection of challenge-tailored examples to help you on your
way to host your [{{ challenge.slug }} challenge]({{ challenge.url }}).


It contains the following:
* ï¸ðŸ¦¾ An example script to _automate uploading_ data to an archive
* ðŸ¦¿ An example _submission algorithm_ that can be uploaded to run as a submission in a challenge phase
* ðŸ§® An example _evaluation method_ that evaluates algorithm submissions and generates performance
  metrics for ranking

The examples are categorized per phase.

Please note that this is a supplementary pack to the [documentation](https://grand-challenge.org/documentation/challenges/).
If the documentation does not answer your question, feel free to reach out to us at
[support@grand-challenge.org](mailto:support@grandchallenge.org).

## Archives
Challenge phases pull their data from archives to test submissions on.

Your challenge has the following archives:

{% for archive in challenge.archives -%}
  * {{ archive.url }}
{% endfor %}


## Now What?

To ensure a smooth start and avoid unnecessary frustration, it helps to first establish a
successful baseline before making any significant changes to the provided examples.

Follow the steps below to get started with the challenge pack effectively.

### Step 1: Run the Test Scripts
Begin by using the provided test scripts to verify that the example templates work locally for both
the example algorithm and example evaluation method.


### Step 2: Manually Upload a Single Case
Continue by [uploading a single example](https://grand-challenge.org/documentation/uploading-data/) case manually to an phase-linked archive to verify formats and validations were setup correctly. Most troubles will show up directly when uploading or in your notifications.


### Step 3: Save and Upload the Example Evaluation Method
After successfully running the local test script, save the evaluation method: using the save script. The method can then [be uploaded to the platform](https://grand-challenge.org/documentation/automated-evaluation/) as a first version.


### Step 4: Save and Upload the Example Algorithm
Similarly, after successfully running the local test script, save the algorithm image: using the save script. On the platform: [create an Algorithm](https://grand-challenge.org/documentation/create-an-algorithm-page/#creating-an-algorithm-for-a-challenge) and upload the algorithm image.

### Step 5: Submit the Example Algorithm
[Submit your algorithm image](https://grand-challenge.org/documentation/automated-evaluation/) to your challenge.


### Step 6: (optionally) Hide Debug Results
At the start, all submissions are closed and the leaderboards are private (i.e. only visible to you).
Once you start the challenge proper you can hide any debug or testing results.

To hide any results go to
**Admin >> Phases >> 'Submissions & Evaluations'** and click the 'Hide Result' button.


By following the steps above you will gain a solid understanding of the submission and evaluation process. This approach makes it much easier to identify and resolve any issues if something goes wrong later.


---
Generated by [Grand-Challenge-Forge](https://github.com/DIAGNijmegen/rse-grand-challenge-forge) v{{ grand_challenge_forge_version }}
