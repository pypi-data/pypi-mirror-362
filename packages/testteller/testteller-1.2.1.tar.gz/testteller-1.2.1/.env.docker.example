# Docker Environment Configuration for TestTeller RAG Agent
# Copy this file to .env before running docker-compose

# =============================================================================
# LLM PROVIDER CONFIGURATION (Required: Choose one)
# =============================================================================
# Available providers: gemini, openai, claude, llama
LLM_PROVIDER=gemini

# =============================================================================
# API KEYS (Configure based on your chosen LLM provider)
# =============================================================================

# Google Gemini (required for gemini provider)
# Get your API key from: https://aistudio.google.com/
GOOGLE_API_KEY=your_gemini_api_key_here

# OpenAI (required for openai provider, also needed for claude embeddings)
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Claude (required for claude provider)
# Get your API key from: https://console.anthropic.com/
# Note: Claude also requires OPENAI_API_KEY for embeddings
CLAUDE_API_KEY=your_claude_api_key_here

# GitHub Personal Access Token (optional, for private repositories)
# Get from: https://github.com/settings/tokens
GITHUB_TOKEN=your_github_token_here

# =============================================================================
# LLM MODEL CONFIGURATION (Optional - defaults provided)
# =============================================================================

# Gemini models
GEMINI_EMBEDDING_MODEL=text-embedding-004
GEMINI_GENERATION_MODEL=gemini-2.0-flash

# OpenAI models
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
OPENAI_GENERATION_MODEL=gpt-4o-mini

# Claude models
CLAUDE_GENERATION_MODEL=claude-3-5-haiku-20241022

# Llama/Ollama models (for local deployment)
LLAMA_EMBEDDING_MODEL=llama3.2:1b
LLAMA_GENERATION_MODEL=llama3.2:3b
OLLAMA_BASE_URL=http://ollama:11434

# =============================================================================
# APPLICATION CONFIGURATION (Optional - defaults provided)
# =============================================================================

# ChromaDB settings
DEFAULT_COLLECTION_NAME=test_documents

# Logging configuration
LOG_LEVEL=INFO
LOG_FORMAT=json

# Document processing
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
CODE_EXTENSIONS=.py,.js,.ts,.java,.go,.rs,.cpp,.hpp,.c,.h,.cs,.rb,.php

# Output configuration
OUTPUT_FILE_PATH=testteller_output.md

# API retry configuration
API_RETRY_ATTEMPTS=3
API_RETRY_WAIT_SECONDS=2

# =============================================================================
# PROVIDER-SPECIFIC DOCKER COMPOSE COMMANDS
# =============================================================================

# For Gemini (default):
# docker-compose up -d

# For OpenAI:
# LLM_PROVIDER=openai docker-compose up -d

# For Claude:
# LLM_PROVIDER=claude docker-compose up -d

# For Llama (requires Ollama service):
# 1. Uncomment the ollama service in docker-compose.yml
# 2. Uncomment the ollama volume
# 3. Add ollama dependency to app service
# 4. LLM_PROVIDER=llama docker-compose up -d
# 5. Install models: docker-compose exec ollama ollama pull llama3.2:3b
