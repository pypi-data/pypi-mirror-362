#!/usr/bin/env python3
"""
Compare streaming vs bulk processing for historical data
"""
import numpy as np
import time
from numba import njit
from ta_numba.trend import sma_numba, ema_numba
from ta_numba.momentum import relative_strength_index_numba

print("=== STREAMING vs BULK PROCESSING FOR HISTORICAL DATA ===")

# Create single-value functions for streaming
@njit(fastmath=True)
def sma_numba_single(data: np.ndarray, window: int = 20) -> float:
    """Calculate SMA for ONLY the last window - single value output"""
    if len(data) < window:
        return np.nan
    last_window = data[-window:]
    return np.mean(last_window)

@njit(fastmath=True)
def ema_numba_single(data: np.ndarray, window: int = 20) -> float:
    """Calculate EMA for ONLY the final value - single value output"""
    if len(data) < 2:
        return np.nan
    
    alpha = 2.0 / (window + 1.0)
    ema = data[0]
    
    for i in range(1, len(data)):
        ema = alpha * data[i] + (1 - alpha) * ema
    
    return ema

@njit(fastmath=True)
def rsi_numba_single(data: np.ndarray, window: int = 14) -> float:
    """Calculate RSI for ONLY the final value - single value output"""
    if len(data) < window + 1:
        return np.nan
    
    alpha = 1.0 / window
    diff = data[1] - data[0]
    up_ema = max(diff, 0.0)
    down_ema = max(-diff, 0.0)
    
    for i in range(2, len(data)):
        diff = data[i] - data[i-1]
        gain = max(diff, 0.0)
        loss = max(-diff, 0.0)
        
        up_ema = alpha * gain + (1 - alpha) * up_ema
        down_ema = alpha * loss + (1 - alpha) * down_ema
    
    if down_ema == 0:
        return 100.0
    
    rs = up_ema / down_ema
    return 100.0 - (100.0 / (1.0 + rs))

# Streaming class with optimizations
class StreamingTANumba:
    def __init__(self, indicator_func, window, **kwargs):
        self.indicator_func = indicator_func
        self.window = window
        self.kwargs = kwargs
        self.values = []
        self.data_array = np.zeros(window * 2, dtype=np.float64)
        
        # Warm up
        dummy_data = np.array([1.0, 2.0, 3.0, 4.0, 5.0])
        _ = self.indicator_func(dummy_data, window, **kwargs)
        
    def update(self, value: float) -> float:
        self.values.append(value)
        
        if len(self.values) > self.window * 2:
            self.values = self.values[-self.window:]
        
        if len(self.values) >= self.window:
            n = len(self.values)
            self.data_array[:n] = self.values
            return self.indicator_func(self.data_array[:n], self.window, **self.kwargs)
        
        return np.nan

def test_historical_processing():
    """Test different approaches for processing historical data"""
    
    # Test different data sizes
    data_sizes = [100, 1000, 5000, 10000, 50000]
    
    # Proper JIT warm-up
    warmup_data = np.random.rand(100) * 100 + 50
    _ = sma_numba(warmup_data)  # Warm-up sma_numba
    _ = sma_numba_single(warmup_data, 20)  # Warm-up single version
    _ = relative_strength_index_numba(warmup_data)  # Warm-up RSI
    _ = rsi_numba_single(warmup_data, 14)  # Warm-up RSI single
    print("üî• JIT functions warmed up properly")
    
    for size in data_sizes:
        print(f"\nüî¨ TESTING WITH {size:,} DATA POINTS:")
        print("=" * 60)
        
        # Generate test data
        np.random.seed(42)
        historical_data = np.random.rand(size) * 100 + 50
        
        # Test SMA
        print(f"\nüìä SMA-20 Results:")
        print("-" * 40)
        
        # Method 1: Original bulk processing (one big run)
        start = time.perf_counter()
        bulk_result = sma_numba(historical_data)
        bulk_time = time.perf_counter() - start
        
        # Method 2: Streaming simulation (tick-by-tick) - OPTIMIZED
        streaming_results = []
        streaming_data = []
        
        start = time.perf_counter()
        for i, price in enumerate(historical_data):
            streaming_data.append(price)
            if len(streaming_data) >= 20:
                # Keep only the last 20 values to avoid growing arrays
                if len(streaming_data) > 20:
                    streaming_data = streaming_data[-20:]
                # Use optimized single-value function (only calculate what we need)
                result = sma_numba_single(np.array(streaming_data), 20)
                streaming_results.append(result)
            else:
                streaming_results.append(np.nan)
        streaming_time = time.perf_counter() - start
        
        # Method 3: Naive tick-by-tick (wasteful - calls full sma_numba)
        naive_results = []
        naive_data = []
        start = time.perf_counter()
        for i, price in enumerate(historical_data):
            naive_data.append(price)
            if len(naive_data) >= 20:
                # WASTEFUL: Calculate ALL SMAs, take only last one
                full_result = sma_numba(np.array(naive_data))
                naive_results.append(full_result[-1])
            else:
                naive_results.append(np.nan)
        naive_time = time.perf_counter() - start
        
        # Method 4: Simple numpy rolling (for comparison)
        start = time.perf_counter()
        numpy_results = []
        for i in range(len(historical_data)):
            if i >= 19:
                result = np.mean(historical_data[i-19:i+1])
                numpy_results.append(result)
            else:
                numpy_results.append(np.nan)
        numpy_time = time.perf_counter() - start
        
        # Results
        print(f"  Bulk processing:        {bulk_time*1000:>8.2f}ms")
        print(f"  Streaming (optimized):  {streaming_time*1000:>8.2f}ms")
        print(f"  Naive (wasteful):       {naive_time*1000:>8.2f}ms")
        print(f"  Numpy rolling:          {numpy_time*1000:>8.2f}ms")
        
        # Speed comparison
        print(f"\n  Speed comparison (vs bulk):")
        print(f"    Streaming (optimized): {streaming_time/bulk_time:.2f}x slower")
        print(f"    Naive (wasteful):      {naive_time/bulk_time:.2f}x slower")
        print(f"    Numpy rolling:         {numpy_time/bulk_time:.2f}x slower")
        
        # Accuracy check
        bulk_last = bulk_result[-1]
        streaming_last = streaming_results[-1]
        naive_last = naive_results[-1]
        numpy_last = numpy_results[-1]
        
        print(f"\n  Accuracy check (last value):")
        print(f"    Bulk:               {bulk_last:.8f}")
        print(f"    Streaming (opt):    {streaming_last:.8f}")
        print(f"    Naive (wasteful):   {naive_last:.8f}")
        print(f"    Numpy:              {numpy_last:.8f}")
        print(f"    Diff (bulk vs opt): {abs(bulk_last - streaming_last):.2e}")

def test_complex_indicator():
    """Test RSI which is more complex"""
    
    print(f"\nüî¨ TESTING RSI-14 (COMPLEX INDICATOR):")
    print("=" * 60)
    
    # Generate test data
    np.random.seed(42)
    size = 10000
    historical_data = np.random.rand(size) * 100 + 50
    
    # Method 1: Original bulk processing
    start = time.perf_counter()
    bulk_result = relative_strength_index_numba(historical_data)
    bulk_time = time.perf_counter() - start
    
    # Method 2: Streaming simulation (optimized)
    streaming_results = []
    streaming_data = []
    
    start = time.perf_counter()
    for price in historical_data:
        streaming_data.append(price)
        if len(streaming_data) >= 15:  # RSI needs at least 15 points
            # Keep only the last 50 values to avoid growing arrays (RSI needs more history)
            if len(streaming_data) > 50:
                streaming_data = streaming_data[-50:]
            result = rsi_numba_single(np.array(streaming_data), 14)
            streaming_results.append(result)
        else:
            streaming_results.append(np.nan)
    streaming_time = time.perf_counter() - start
    
    # Method 3: Naive approach (wasteful)
    naive_results = []
    naive_data = []
    start = time.perf_counter()
    for price in historical_data:
        naive_data.append(price)
        if len(naive_data) >= 15:  # RSI needs at least 15 points
            # WASTEFUL: Calculate ALL RSI values, take only last one
            full_result = relative_strength_index_numba(np.array(naive_data))
            naive_results.append(full_result[-1])
        else:
            naive_results.append(np.nan)
    naive_time = time.perf_counter() - start
    
    print(f"  Bulk processing:        {bulk_time*1000:>8.2f}ms")
    print(f"  Streaming (optimized):  {streaming_time*1000:>8.2f}ms")
    print(f"  Naive (wasteful):       {naive_time*1000:>8.2f}ms")
    
    print(f"\n  Speed comparison (vs bulk):")
    print(f"    Streaming (optimized): {streaming_time/bulk_time:.2f}x slower")
    print(f"    Naive (wasteful):      {naive_time/bulk_time:.2f}x slower")
    
    # Accuracy check
    bulk_last = bulk_result[-1]
    streaming_last = streaming_results[-1]
    naive_last = naive_results[-1]
    
    print(f"\n  Accuracy check (last value):")
    print(f"    Bulk:               {bulk_last:.8f}")
    print(f"    Streaming (opt):    {streaming_last:.8f}")
    print(f"    Naive (wasteful):   {naive_last:.8f}")
    print(f"    Diff (bulk vs opt): {abs(bulk_last - streaming_last):.2e}")

if __name__ == "__main__":
    test_historical_processing()
    test_complex_indicator()
    
    print("\nüéØ SUMMARY:")
    print("=" * 60)
    print("For HISTORICAL DATA processing:")
    print("‚úÖ BULK is fastest (processes all data at once)")
    print("‚ö†Ô∏è  STREAMING (optimized) is 2-5x slower (reasonable for real-time)")
    print("‚ùå NAIVE (wasteful) is 10-50x slower (recalculates everything!)")
    print()
    print("Key insights:")
    print("‚Ä¢ Bulk processing: Calculate each value once")
    print("‚Ä¢ Optimized streaming: Only calculate what you need")
    print("‚Ä¢ Wasteful streaming: Recalculates all values every tick")
    print()
    print("For REAL-TIME trading:")
    print("‚úÖ Use OPTIMIZED streaming (single-value functions)")
    print("‚ùå Avoid wasteful streaming (full array calculations)")
    print()
    print("üí° Use BULK for backtesting, OPTIMIZED STREAMING for live trading!")