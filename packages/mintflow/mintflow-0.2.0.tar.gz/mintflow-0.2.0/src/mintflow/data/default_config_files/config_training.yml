

flag_use_GPU: "True"  # if set to True, GPU is used for training/evaluation
val_scppnorm_total: 10000  # the target_sum arg of `sc.pp.normalize_total`


# wandb configs ============
flag_enable_wandb: "True"  # TODO:TUNE if set to Ture wandb logging is enabled. Recommended: True  # TODO:revert back to "True"
wandb_project_name: "MintFLow"  # TODO:TUNE your own wandb project name
wandb_run_name: "YOUR_RUNNAME"  # TODO:TUNE your wandb run name
wandb_stepsize_log: 2  # The step size to log to wandb, increase this number if you want less frequent wandb logs.

lr_training: 0.001  # TODO:check. Learning rate for training optimisers. The default value of 0.001 should be OK but one may need to tune it.

num_training_epochs: 50  # TODO:ESSENTIAL:TUNE, number of training epochs. Recommended: ~50 epochs. A good practice is to run inflow with `num_training_epochs` to get a sense of running time and to see if the code runs. Afterwards one can increase `num_training_epochs` to, e.g., 50.

numiters_updateduals_seprately_perepoch: 200  # TODO:check, TODO: make important notes about how it affects the running time and the correct set up.
batchsize_updateduals_seprately_perepoch: 100  # TODO:check
# Before each training epoch the dual functions are updates separately for `numiters_updateduals_seprately_perepoch` iterations.
# This is done to keep the dual functions a good, e.g., Wasserstein distance approximators.

numsteps_accumgrad: 2 # TODO:TUNE
num_updateseparate_afterGRLs: 5  # TODO:TUNE, TODO: make important notes about how it affects the running time and the correct set up.
# Above: two very important parameters.
#   - `numsteps_accumgrad`: inflow uses accumulated gradients, i.e. during training it performs `numsteps_accumgrad` calls to backward followed by a single call to optimiser step.
#   - `num_updateseparate_afterGRLs`: the dual functions (e.g. for Wasserstein distance estimation) have to be updated regularly.
#      To this end after each `numsteps_accumgrad` calls to backward and optimiser step, the dual functions are seperately updated `num_updateseparate_afterGRLs` times.
#      Ideally `num_updateseparate_afterGRLs` should be large, but increasing it too much is computationlly infeasible.


annealing_decoder_XintXspl_coef_min: 0.00001  # TODO:check, TODO:internalcheck: recomm a good setting
annealing_decoder_XintXspl_coef_max: 0.001  # TODO:check, TODO:internalcheck: recomm a good setting
annealing_decoder_XintXspl_fractionepochs_phase1: 0.5  # TODO:check, TODO:internalcheck: recomm a good setting
annealing_decoder_XintXspl_fractionepochs_phase2: 0.2  # TODO:check, TODO:internalcheck: recomm a good setting
# Above: four VERY IMPORTANT parameters that specify the annealing for the decoder ZINB loss of Xint and Xspl (please refer to the manuscript to the discussion about why this is needed).
# The annealing has three phases
#   - phase 1: Takes `annealing_decoder_XintXspl_fractionepochs_phase1` fraction of training epochs, during which the two zinb losses for Xint and Xspl are multiplied by `annealing_decoder_XintXspl_coef_min`.
#   - phase 2: Takes `annealing_decoder_XintXspl_fractionepochs_phase2` fraction of training epochs, during which the two zinb losses for Xint and Xspl are multiplied by a number linearly going up to `annealing_decoder_XintXspl_coef_max`.
#   - phase 3: the rest of training, the two zinb losses for Xint and Xspl are multipled by `annealing_decoder_XintXspl_coef_max`.
# To disable this annealing you can set `annealing_decoder_XintXspl_coef_min` and `annealing_decoder_XintXspl_coef_max` to equal numbers.
# TODO:internalcheck: see if the annealing coefficients are as expected.

sleeptime_gccollect_aftertraining: 120  #TODO:TUNE
# After training for `num_training_epochs`, garbage collection is called followed by `time.sleep(sleeptime_gccollect_aftertraining)` so some system/gpu memory is potentially released.
# `sleeptime_gccollect_aftertraining` is in sleeping time in seconds.

sleeptime_gccollect_dumpOnePred: 120 #TODO:cehck
# Similar to above, but after dumping the predictions for each tissue sample


flag_finaleval_enable_pertissue_violinplot: "True"  # TODO:check
# Whether in the final evaluation the per-tissue violin plots are dumped.
# Creating the violin plots may take a while, you can disable it by setting `flag_finaleval_enable_pertissue_violinplot` to "False".

flag_finaleval_enable_alltissuecombined_eval: "True"  # TODO:check
# Whether in the final evaluation the evaluation  is done on all samples combined.
# If there are many tissue samples, combining them may result in problems like out of memory (OOM).
# In that case you can disable the combined analysis by setting `flag_finaleval_enable_alltissuecombined_eval` to "False".

flag_finaleval_enable_alltissue_violinplot: "True"  # TODO:check
# Optionally you can disable only the violinplots when all tissues are combined in the end.
# To do so, you can set `flag_finaleval_enable_alltissue_violinplot` to "False".


flag_finaleval_createanndata_alltissuescombined: "True"  # TODO:check
# If set to "True", in the end all tissue sections are combined in a single anndata containing all inflow predictions.
# Inflow predictions are placed in `adata.obsm` and `adata.uns`.
# Two .h5ad files are created in the output path
#   - adata_inflowOutput_unnorm: the anndata "before" applying `sc.pp.normalize_total` where `adata.X` is not row normalised --> inflow predictions `Xint` and `Xspl` sum up to the unnormalised version of `adata.X`.
#   - adata_inflowOutput_norm: the anndata "after" applying `sc.pp.normalize_total` where `adata.X` is row normalised --> inflow predictions `Xint` and `Xspl` sum up to the normalised version of `adata.X`.


method_ODE_solver: "dopri5"
# The ODE solver, i.e. the `method` argument passed to the function `torchdiffeq.odeint`.
# TODO: report the effect on running time.
