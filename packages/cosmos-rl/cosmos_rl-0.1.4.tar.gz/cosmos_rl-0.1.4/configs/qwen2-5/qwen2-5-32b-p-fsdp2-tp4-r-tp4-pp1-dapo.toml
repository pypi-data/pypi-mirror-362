redis = "12800"

[train]
resume = false
epoch = 1
output_dir = "./outputs/qwen2-5-32b-p-fsdp2-tp4-r-tp4-pp1-dapo"
epsilon = 1e-6
optm_name = "AdamW"
optm_lr = 1e-6
optm_impl = "fused"
optm_weight_decay = 0.1
optm_betas = [ 0.9, 0.999,]
optm_warmup_steps = 10
optm_grad_norm_clip = 1.0
async_tp_enabled = false
compile = true
param_dtype = "bfloat16"
fsdp_reduce_dtype = "float32"
fsdp_offload = false
fsdp_reshard_after_forward = "default"
train_batch_per_replica = 384
sync_weight_interval = 1

[rollout]
gpu_memory_utilization = 0.7
enable_chunked_prefill = true
n_generation = 16
batch_size = 24 
quantization = "none"
max_response_length = 20480


[rollout.sampling_config]
temperature = 1.0
top_p = 1.0
top_k = -1

[policy]    
model_name_or_path = "Qwen/Qwen2.5-32B"
model_max_length = 1024
model_gradient_checkpointing = true

[logging]
logger = ['console', 'wandb']
project_name = "dapo"
experiment_name = "None"

[train.train_policy]
type = "grpo"
variant = "dapo"
dataset.name = "BytedTsinghua-SIA/DAPO-Math-17k"
dataset.split = "train"
reward_function = ['direct_math', 'overlong']
temperature = 0.9
epsilon_low = 0.2
epsilon_high = 0.28
kl_beta = 0.0
mu_iterations = 2
mini_batch = 8

[train.train_policy.overlong_reward]
enable_overlong_penalty = true
buffer_length = 4096
penalty_factor = 1.0

[train.ckpt]
enable_checkpoint = true
save_freq = 10
max_keep = 10
save_mode = "async"

[rollout.parallelism]
n_init_replicas = 1
tp_size = 4
cp_size = 1
dp_shard_size = 1
pp_size = 1
dp_replicate_size = 1


[policy.parallelism]
n_init_replicas = 1
tp_size = 4
cp_size = 1
dp_shard_size = 2
pp_size = 1
dp_replicate_size = 1
