Metadata-Version: 2.4
Name: yahia_mcp_client_cli
Version: 1.3.6
Summary: Command line interface for MCP client
Project-URL: Homepage, https://github.com/Yahialqur/mcp-client-cli
Author-email: Yahia Alqurnawi <ymq2003@gmail.com>
License: MIT
License-File: LICENSE
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Requires-Python: >=3.12
Requires-Dist: aiosqlite>=0.20.0
Requires-Dist: commentjson>=0.9.0
Requires-Dist: jsonschema-pydantic>=0.6
Requires-Dist: langchain-anthropic>=0.3.0
Requires-Dist: langchain-google-genai>=2.0.7
Requires-Dist: langchain-ollama==0.3.4
Requires-Dist: langchain-openai>=0.2.10
Requires-Dist: langchain>=0.3.8
Requires-Dist: langgraph-checkpoint-sqlite>=2.0.1
Requires-Dist: langgraph-prebuilt>=0.1.0
Requires-Dist: langgraph>=0.4.1
Requires-Dist: mcp>=1.6.0
Requires-Dist: ollama==0.5.1
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: pywin32>=306; sys_platform == 'win32' or platform_system == 'Windows'
Requires-Dist: rich>=13.9.0
Requires-Dist: standard-imghdr>=3.13.0
Requires-Dist: uv==0.7.21
Provides-Extra: clipboard
Requires-Dist: pngpaste; (sys_platform == 'darwin' and python_version < '3.12') and extra == 'clipboard'
Requires-Dist: pyperclip>=1.8.2; extra == 'clipboard'
Description-Content-Type: text/markdown

# MCP CLI Client

#### Github: https://github.com/Yahialqur/mcp-client-cli/tree/yahia

A simple CLI program to run LLM prompt and implement [Model Context Protocol (MCP)](https://modelcontextprotocol.io/) client.

You can use any [MCP-compatible servers](https://github.com/punkpeye/awesome-mcp-servers) from the convenience of your terminal.

This act as alternative client beside Claude Desktop. Additionally you can use any LLM provider like OpenAI, Groq, or local LLM model via [llama](https://github.com/ggerganov/llama.cpp).


## Setup

1. Install via pip:
   ```bash
   pip install yahia-mcp-client-cli
   ```

2. Create a config.json file to configure your LLM and MCP servers: 

(On Mac and Linux `~/.llm/config.json`)

(On windows `C:\Users\YourUsername\.llm\config.json`)
   ```json
    {
      "systemPrompt": "You are an AI assistant helping a software engineer...",
      "llm": {
        "provider": "ollama",
        "model": "qwen3:1.7b",
        "base_url": "http://localhost:11434"
        },
      "mcpServers": {
        "neo4j-aura": {
            "command": "uvx",
            "args": [ "mcp-neo4j-cypher@0.2.4", "--transport", "stdio" ],
            "env": {
                "NEO4J_URI": "neo4j://localhost:7687",
                "NEO4J_USERNAME": "neo4j",
                "NEO4J_PASSWORD": "password",
                "NEO4J_DATABASE": "neo4j"
            }
        }
        }
    }
   ```

   Note:
   - See [CONFIG.md](CONFIG.md) for complete documentation of the configuration format
   - Use `requires_confirmation` to specify which tools need user confirmation before execution
   - The LLM API key can also be set via environment variables `LLM_API_KEY` or `OPENAI_API_KEY`
   - The config file can be placed in either `~/.llm/config.json` or `$PWD/.llm/config.json`
   - You can comment the JSON config file with `//` if you like to switch around the configuration

3. Run the CLI:
   
   For Single Queries:
   ```bash
   llm "What is the capital city of North Sumatra?"
   ```

   For Interactive Mode (Continuous queries):
   ```bash
   llm
   ```

## Setup Extras

For Neo4j MCP:
1. You may need to add the following in the desired neo4j instance's neo4j.conf if it does not exist already:
```
dbms.security.procedures.unrestricted=apoc.*
dbms.security.procedures.allowlist=apoc.*
dbms.security.allow_csv_import_from_file_urls=true
dbms.connector.bolt.listen_address=:7687
```

To open Neo4j.conf:
1. Open Neo4j Desktop
2. Click the options button on the instance
3. Click the Open neo4j.conf option

## Usage

### With Neo4j
Using the Neo4j mcp server, we can call any of the server functions through the cli client

### Image Input

You can pipe image files to analyze them with multimodal LLMs:

```bash
$ cat image.jpg | llm "What do you see in this image?"
[LLM will analyze and describe the image]

$ cat screenshot.png | llm "Is there any error in this screenshot?"
[LLM will analyze the screenshot and point out any errors]
```

### Using Prompt Templates

You can use predefined prompt templates by using the `p` prefix followed by the template name and its arguments:

```bash
# List available prompt templates
$ llm --list-prompts

# Use a template
$ llm p review  # Review git changes
$ llm p commit  # Generate commit message
$ llm p yt url=https://youtube.com/...  # Summarize YouTube video
```

### Triggering a tool

```bash
$ llm What is the top article on hackernews today?

================================== Ai Message ==================================
Tool Calls:
  brave_web_search (call_eXmFQizLUp8TKBgPtgFo71et)
 Call ID: call_eXmFQizLUp8TKBgPtgFo71et
  Args:
    query: site:news.ycombinator.com
    count: 1
Brave Search MCP Server running on stdio

# If the tool requires confirmation, you'll be prompted:
Confirm tool call? [y/n]: y

================================== Ai Message ==================================
Tool Calls:
  fetch (call_xH32S0QKqMfudgN1ZGV6vH1P)
 Call ID: call_xH32S0QKqMfudgN1ZGV6vH1P
  Args:
    url: https://news.ycombinator.com/
================================= Tool Message =================================
Name: fetch

[TextContent(type='text', text='Contents [REDACTED]]
================================== Ai Message ==================================

The top article on Hacker News today is:

### [Why pipes sometimes get "stuck": buffering](https://jvns.ca)
- **Points:** 31
- **Posted by:** tanelpoder
- **Posted:** 1 hour ago

You can view the full list of articles on [Hacker News](https://news.ycombinator.com/)
```

To bypass tool confirmation requirements, use the `--no-confirmations` flag:

```bash
$ llm --no-confirmations "What is the top article on hackernews today?"
```

To use in bash scripts, add the --no-intermediates, so it doesn't print intermediate messages, only the concluding end message.
```bash
$ llm --no-intermediates "What is the time in Tokyo right now?"
```

### Continuation

Add a `c ` prefix to your message to continue the last conversation.

```bash
$ llm asldkfjasdfkl
It seems like your message might have been a typo or an error. Could you please clarify or provide more details about what you need help with?
$ llm c what did i say previously?
You previously typed "asldkfjasdfkl," which appears to be a random string of characters. If you meant to ask something specific or if you have a question, please let me know!
```

### Clipboard Support

You can use content from your clipboard using the `cb` command:

```bash
# After copying text to clipboard
$ llm cb
[LLM will process the clipboard text]

$ llm cb "What language is this code written in?"
[LLM will analyze the clipboard text with your question]

# After copying an image to clipboard
$ llm cb "What do you see in this image?"
[LLM will analyze the clipboard image]

# You can combine it with continuation
$ llm cb c "Tell me more about what you see"
[LLM will continue the conversation about the clipboard content]
```

The clipboard feature works in:
- Native Windows/macOS/Linux environments
  - Windows: Uses PowerShell
  - macOS: Uses `pbpaste` for text, `pngpaste` for images (optional)
  - Linux: Uses `xclip` (required for clipboard support)
- Windows Subsystem for Linux (WSL)
  - Accesses the Windows clipboard through PowerShell
  - Works with both text and images
  - Make sure you have access to `powershell.exe` from WSL

Required tools for clipboard support:
- Windows: PowerShell (built-in)
- macOS: 
  - `pbpaste` (built-in) for text
  - `pngpaste` (optional) for images: `brew install pngpaste`
- Linux: 
  - `xclip`: `sudo apt install xclip` or equivalent

The CLI automatically detects if the clipboard content is text or image and handles it appropriately.

### Additional Options

```bash
$ llm --list-tools                # List all available tools
$ llm --list-prompts              # List available prompt templates
$ llm --no-tools                  # Run without any tools
$ llm --force-refresh             # Force refresh tool capabilities cache
$ llm --text-only                 # Output raw text without markdown formatting
$ llm --show-memories             # Show user memories
$ llm --model gpt-4               # Override the model specified in config
```

## Contributing

Feel free to submit issues and pull requests for improvements or bug fixes.

<br><br><br>

##### Other LLM Setup
```
  "llm": {
      "provider": "openai",
      "model": "<deployment-id>",  // This is the Azure OpenAI deployment name
      "api_key": "<your-api-key>",
      "temperature": 0,
      "base_url": "https://<your-resource-name>.openai.azure.com/openai/deployments/<deployment-id>"
    },
```
