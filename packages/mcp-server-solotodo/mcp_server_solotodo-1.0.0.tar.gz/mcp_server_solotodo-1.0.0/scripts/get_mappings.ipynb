{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import requests\n",
    "\n",
    "url = \"https://publicapi.solotodo.com/categories/1/browse/?page_size=100&page={}\"\n",
    "results = []\n",
    "for i in itertools.count(1):\n",
    "    print(f\"Fetching page {i}...\")\n",
    "    response = requests.get(url.format(i))\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    if not data[\"results\"]:\n",
    "        print(\"No more results found.\")\n",
    "        break\n",
    "    results.extend(data[\"results\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6386bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a70ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load your data (assuming it's in a variable called 'results')\n",
    "# If it's in a file, you might load it like:\n",
    "# with open('your_file.json', 'r') as f:\n",
    "#     results = json.load(f)\n",
    "\n",
    "# Extract all product entries from the nested structure\n",
    "product_entries = []\n",
    "for item in results:  # results is your list of objects\n",
    "    for product_entry in item['product_entries']:\n",
    "        product_entries.append(product_entry)\n",
    "\n",
    "# Method 1: Using pd.json_normalize() - This is what you were looking for!\n",
    "df = pd.json_normalize(product_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d988b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_id_mappings(df):\n",
    "    \"\"\"\n",
    "    Extract mappings from ID columns to their related attribute columns\n",
    "    \"\"\"\n",
    "    # Get all column names\n",
    "    columns = df.columns.tolist()\n",
    "    \n",
    "    # Dictionary to store mappings\n",
    "    id_mappings = {}\n",
    "    \n",
    "    # Find all columns ending with '_id'\n",
    "    id_columns = [col for col in columns if col.endswith('_id')]\n",
    "    \n",
    "    print(f\"Found {len(id_columns)} ID columns:\")\n",
    "    for col in id_columns:\n",
    "        print(f\"  - {col}\")\n",
    "    \n",
    "    # For each ID column, find related columns\n",
    "    for id_col in id_columns:\n",
    "        # Extract the base name (remove '_id' suffix)\n",
    "        base_name = id_col[:-3]  # Remove '_id'\n",
    "        \n",
    "        # Find all columns that start with the base name\n",
    "        related_cols = [col for col in columns if col.startswith(base_name) and col != id_col]\n",
    "        \n",
    "        if related_cols:\n",
    "            print(f\"\\n--- Processing {id_col} ---\")\n",
    "            print(f\"Base name: {base_name}\")\n",
    "            print(f\"Related columns: {related_cols}\")\n",
    "            \n",
    "            # Extract unique mappings\n",
    "            relevant_cols = [id_col] + related_cols\n",
    "            \n",
    "            # Get unique combinations of ID and related attributes\n",
    "            unique_mappings = df[relevant_cols].drop_duplicates().dropna(subset=[id_col])\n",
    "            \n",
    "            if len(unique_mappings) > 0:\n",
    "                print(f\"Found {len(unique_mappings)} unique mappings\")\n",
    "                \n",
    "                # Create mapping dictionary\n",
    "                mapping_dict = {}\n",
    "                for _, row in unique_mappings.iterrows():\n",
    "                    id_value = row[id_col]\n",
    "                    if pd.notna(id_value):\n",
    "                        mapping_dict[id_value] = {}\n",
    "                        for col in related_cols:\n",
    "                            if col in row and pd.notna(row[col]):\n",
    "                                # Clean column name for the mapping key\n",
    "                                clean_col_name = col.replace(base_name + '_', '')\n",
    "                                mapping_dict[id_value][clean_col_name] = row[col]\n",
    "                \n",
    "                id_mappings[id_col] = {\n",
    "                    'base_name': base_name,\n",
    "                    'related_columns': related_cols,\n",
    "                    'mappings': mapping_dict\n",
    "                }\n",
    "                \n",
    "                # Show a sample of the mappings\n",
    "                print(\"Sample mappings:\")\n",
    "                for i, (id_val, attrs) in enumerate(list(mapping_dict.items())[:3]):\n",
    "                    print(f\"  ID {id_val}: {attrs}\")\n",
    "                    if i >= 2:  # Show max 3 samples\n",
    "                        break\n",
    "                \n",
    "                if len(mapping_dict) > 3:\n",
    "                    print(f\"  ... and {len(mapping_dict) - 3} more\")\n",
    "    \n",
    "    return id_mappings\n",
    "\n",
    "def create_lookup_tables(id_mappings):\n",
    "    \"\"\"\n",
    "    Create individual lookup tables for each ID mapping\n",
    "    \"\"\"\n",
    "    lookup_tables = {}\n",
    "    \n",
    "    for id_col, mapping_info in id_mappings.items():\n",
    "        base_name = mapping_info['base_name']\n",
    "        mappings = mapping_info['mappings']\n",
    "        \n",
    "        if mappings:\n",
    "            # Convert to DataFrame\n",
    "            lookup_df = pd.DataFrame.from_dict(mappings, orient='index')\n",
    "            lookup_df.index.name = base_name + '_id'\n",
    "            lookup_df = lookup_df.reset_index()\n",
    "            \n",
    "            lookup_tables[base_name] = lookup_df\n",
    "            \n",
    "            print(f\"\\nLookup table for {base_name}:\")\n",
    "            print(f\"Shape: {lookup_df.shape}\")\n",
    "            print(f\"Columns: {lookup_df.columns.tolist()}\")\n",
    "            print(\"Sample rows:\")\n",
    "            print(lookup_df.head(3))\n",
    "    \n",
    "    return lookup_tables\n",
    "\n",
    "def analyze_id_relationships(df):\n",
    "    \"\"\"\n",
    "    Analyze and extract all ID-based relationships in the dataframe\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ANALYZING ID-BASED RELATIONSHIPS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Extract ID mappings\n",
    "    id_mappings = extract_id_mappings(df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CREATING LOOKUP TABLES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create lookup tables\n",
    "    lookup_tables = create_lookup_tables(id_mappings)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"Total ID columns found: {len(id_mappings)}\")\n",
    "    print(f\"Lookup tables created: {len(lookup_tables)}\")\n",
    "    \n",
    "    print(\"\\nAvailable lookup tables:\")\n",
    "    for name, table in lookup_tables.items():\n",
    "        print(f\"  - {name}: {table.shape[0]} unique values, {table.shape[1]} attributes\")\n",
    "    \n",
    "    return id_mappings, lookup_tables\n",
    "\n",
    "# Run the analysis\n",
    "# Assuming your DataFrame is called 'df'\n",
    "id_mappings, lookup_tables = analyze_id_relationships(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a0c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save lookup tables to CSV files\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\nSaving lookup tables to CSV files...\")\n",
    "# Create lookup directory if it doesn't exist\n",
    "lookup_dir = Path(\"lookup\")\n",
    "lookup_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for name, table in lookup_tables.items():\n",
    "    filename = lookup_dir / f\"{name}.csv\"\n",
    "    table.to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d5c2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_mappings[\"product.specs.family_line_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4690b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_tables[\"product.specs.family_line\"][[\"product.specs.family_line_id\", \"unicode\"]]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b130e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "def csv_to_dict(csv_path: str | Path, key_column: str, value_column: str) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Create a dictionary from a CSV file using specified columns.\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the CSV file\n",
    "        key_column: Name of the column to use as dictionary keys\n",
    "        value_column: Name of the column to use as dictionary values\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with key_column values as keys and value_column values as values,\n",
    "        sorted by keys\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If CSV file doesn't exist\n",
    "        KeyError: If specified columns don't exist in CSV\n",
    "        ValueError: If CSV is empty or malformed\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    \n",
    "    with open(csv_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        \n",
    "        # Check if columns exist\n",
    "        if key_column not in reader.fieldnames:\n",
    "            raise KeyError(f\"Column '{key_column}' not found in CSV. Available columns: {reader.fieldnames}\")\n",
    "        if value_column not in reader.fieldnames:\n",
    "            raise KeyError(f\"Column '{value_column}' not found in CSV. Available columns: {reader.fieldnames}\")\n",
    "        \n",
    "        for row in reader:\n",
    "            key = row[key_column].strip()\n",
    "            value = row[value_column].strip()\n",
    "            \n",
    "            # Skip empty keys\n",
    "            if key:\n",
    "                result[key] = value\n",
    "    \n",
    "    # Return sorted dictionary\n",
    "    return dict(sorted(result.items()))\n",
    "\n",
    "# Usage examples:\n",
    "mapping = csv_to_dict('lookup/product.specs.keyboard_layout.csv', 'unicode', 'product.specs.keyboard_layout_id')\n",
    "mapping\n",
    "# line_mapping = csv_to_dict('product_lines.csv', 'name', 'product.specs.family_line_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
