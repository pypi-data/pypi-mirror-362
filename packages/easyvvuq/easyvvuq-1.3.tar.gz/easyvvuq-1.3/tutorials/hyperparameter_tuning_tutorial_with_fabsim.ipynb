{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c43ea9",
   "metadata": {},
   "source": [
    "## Tuning the hyperparameters of a neural network using EasyVVUQ and FabSim3\n",
    "\n",
    "In this tutorial we will use the EasyVVUQ `GridSampler` to perform a grid search on the hyperparameters of a simple Keras neural network model, trained to recognize hand-written digits. This is the famous MNIST data set, of which 4 input features (of size 28 x 28) are show below. These are fed into a standard feed-forward neural network, which will predict the label 0-9.\n",
    "\n",
    "The (Keras) neural network script is located in `mnist/keras_mnist.template`, which will form the input template for the EasyVVUQ encoder. We will assume you are familiar with the basic EasyVVUQ building blocks. If not, you can look at the [basic tutorial](https://github.com/UCL-CCS/EasyVVUQ/blob/dev/tutorials/basic_tutorial.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83545e38",
   "metadata": {},
   "source": [
    "![](mnist/mnist_feats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf467821",
   "metadata": {},
   "source": [
    "We need EasyVVUQ, TensorFlow and the TensorFlow data sets to execute this tutorial. If you need to install these, uncomment the corresponding line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00f7ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/wouter/anaconda3/lib/python3.9/site-packages (2.11.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (23.1)\n",
      "Collecting protobuf<3.20,>=3.9.2 (from tensorflow)\n",
      "  Using cached protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (787 bytes)\n",
      "Requirement already satisfied: setuptools in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: tensorboard<2.12,>=2.11 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: keras<2.12,>=2.11.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (2.11.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/wouter/.local/lib/python3.9/site-packages/cachetools-5.3.0-py3.9.egg (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/wouter/.local/lib/python3.9/site-packages/pyasn1_modules-0.3.0rc1-py3.9.egg (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.3.0rc1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/wouter/.local/lib/python3.9/site-packages/rsa-4.9-py3.9.egg (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (1.26.18)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/wouter/.local/lib/python3.9/site-packages/requests_oauthlib-1.3.1-py3.9.egg (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/wouter/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wouter/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wouter/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wouter/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/wouter/anaconda3/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/wouter/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/wouter/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/wouter/.local/lib/python3.9/site-packages/oauthlib-3.2.2-py3.9.egg (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
      "Using cached protobuf-3.19.6-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-datasets 4.9.3 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
      "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.19.6\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: tensorflow_datasets in /home/wouter/anaconda3/lib/python3.9/site-packages (4.9.3)\n",
      "Requirement already satisfied: absl-py in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: array-record in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (0.5.1)\n",
      "Requirement already satisfied: click in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (0.1.8)\n",
      "Requirement already satisfied: etils>=0.9.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (1.5.2)\n",
      "Requirement already satisfied: numpy in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (1.24.3)\n",
      "Requirement already satisfied: promise in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (2.3)\n",
      "Collecting protobuf>=3.20 (from tensorflow_datasets)\n",
      "  Using cached protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: psutil in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (5.9.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (2.31.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: termcolor in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (2.1.0)\n",
      "Requirement already satisfied: toml in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (4.65.0)\n",
      "Requirement already satisfied: wrapt in /home/wouter/anaconda3/lib/python3.9/site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Requirement already satisfied: fsspec in /home/wouter/anaconda3/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2023.4.0)\n",
      "Requirement already satisfied: importlib_resources in /home/wouter/anaconda3/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (6.1.0)\n",
      "Requirement already satisfied: typing_extensions in /home/wouter/anaconda3/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.7.1)\n",
      "Requirement already satisfied: zipp in /home/wouter/anaconda3/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/wouter/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/wouter/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/wouter/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/wouter/anaconda3/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.2.2)\n",
      "Requirement already satisfied: six in /home/wouter/anaconda3/lib/python3.9/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "  Using cached protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (679 bytes)\n",
      "Using cached protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.6\n",
      "    Uninstalling protobuf-3.19.6:\n",
      "      Successfully uninstalled protobuf-3.19.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install easyvvuq\n",
    "!pip install tensorflow\n",
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c7fde",
   "metadata": {},
   "source": [
    "### FabSim3\n",
    "\n",
    "While running on the localhost, we will use the [FabSim3](https://github.com/djgroen/FabSim3) automation toolkit for the data processing workflow, i.e. to move the UQ ensemble to/from the localhost. To connect EasyVVUQ with FabSim3, the [FabUQCampaign](https://github.com/wedeling/FabUQCampaign) plugin must be installed.\n",
    "\n",
    "The advantage of this construction is that we could offload the ensemble to a remote supercomputer using this same script by simply changing the `MACHINE='localhost'` flag, provided that FabSIm3 is set up on the remote resource.\n",
    "\n",
    "For an example **without FabSim3**, see `tutorials/hyperparameter_tuning_tutorial.ipynb`.\n",
    "\n",
    "For now, import the required libraries below. `fabsim3_cmd_api` is an interface with fabSim3 such that the command-line FabSim3 commands can be executed in a Python script. It is stored locally in `fabsim3_cmd_api.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7347053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wouter/anaconda3/lib/python3.9/site-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "import easyvvuq as uq\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "############################################\n",
    "# Import the FabSim3 commandline interface #\n",
    "############################################\n",
    "import fabsim3_cmd_api as fab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22672c4c",
   "metadata": {},
   "source": [
    "We now set some flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52064503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work directory, where the EasyVVUQ directory will be placed\n",
    "WORK_DIR = '/tmp'\n",
    "# machine to run ensemble on\n",
    "MACHINE = \"localhost\"\n",
    "# target output filename generated by the code\n",
    "TARGET_FILENAME = 'output.csv'\n",
    "# EasyVVUQ campaign name\n",
    "CAMPAIGN_NAME = 'grid_test'\n",
    "\n",
    "# FabSim3 config name\n",
    "CONFIG = 'grid_search'\n",
    "# Use QCG PilotJob or not\n",
    "PILOT_JOB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898a0d57",
   "metadata": {},
   "source": [
    "Most of these are self explanatory. Here, `CONFIG` is the name of the script that gets executed for each sample, in this case `grid_search`, which is located in `FabUQCampaign/templates/grid_search`. Its contents are essentially just runs our Python code `hyper_param_tune.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229e7b9",
   "metadata": {},
   "source": [
    "```\n",
    "cd $job_results\n",
    "$run_prefix\n",
    "\n",
    "/usr/bin/env > env.log\n",
    "\n",
    "python3 hyper_param_tune.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0c006f",
   "metadata": {},
   "source": [
    "Here, `hyper_param_tune` is generated by the EasyVVUQ encoder, see below. The flag `PILOT_JOB` regulates the use of the QCG PilotJob mechanism. If `True`, FabSim will submit the ensemble to the (remote) host as a QCG PilotJob, which essentially means that all invididual jobs of the ensemble will get packaged into a single job allocation, thereby circumventing the limit on the maximum number of simultaneous jobs that is present on many supercomputers. For more info on the QCG PilotJob click [here](https://github.com/vecma-project/QCG-PilotJob). In this example we'll run the samples on the localhost (see `MACHINE`), and hence we set `PILOT_JOB=False`.\n",
    "\n",
    "As is standard in EasyVVUQ, we now define the parameter space. In this case these are 4 hyperparameters. There is one hidden layer with `n_neurons` neurons, a Dropout layer after the input and hidden layer, with dropout probability `dropout_prob_in` and `dropout_prob_hidden` respectively. We made the `learning_rate` tuneable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a3a8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"n_neurons\"] = {\"type\":\"integer\", \"default\": 32}\n",
    "params[\"dropout_prob_in\"] = {\"type\":\"float\", \"default\": 0.0}\n",
    "params[\"dropout_prob_hidden\"] = {\"type\":\"float\", \"default\": 0.0}\n",
    "params[\"learning_rate\"] = {\"type\":\"float\", \"default\": 0.001}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b41214c",
   "metadata": {},
   "source": [
    "These 4 hyperparameter appear as flags in the input template `mnist/keras_mnist.template`. Typically this is generated from an input file used by some simualtion code. In this case however, `mnist/keras_mnist.template` is directly our Python script, with the hyperparameters replaced by flags. For instance:\n",
    "\n",
    "```python\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dropout($dropout_prob_in),\n",
    "  tf.keras.layers.Dense($n_neurons, activation='relu'),\n",
    "  tf.keras.layers.Dropout($dropout_prob_hidden),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "```\n",
    "\n",
    "is simply the neural network construction part with flags for the dropout probabilities and the number of neurons in the hidden layer. The encoder reads the flags and replaces them with numeric values, and it subsequently writes the corresponding `target_filename=hyper_param_tune.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed08818",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = uq.encoders.GenericEncoder('./mnist/keras_mnist.template', target_filename='hyper_param_tune.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02644574",
   "metadata": {},
   "source": [
    "Now we create the first set of EasyVVUQ `actions` to create separate run directories and to encode the template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a10d571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actions: create directories and encode input template, placing 1 hyper_param_tune.py file in each directory.\n",
    "actions = uq.actions.Actions(\n",
    "    uq.actions.CreateRunDirectory(root=WORK_DIR, flatten=True),\n",
    "    uq.actions.Encode(encoder),\n",
    ")\n",
    "\n",
    "# create the EasyVVUQ main campaign object\n",
    "campaign = uq.Campaign(\n",
    "    name=CAMPAIGN_NAME,\n",
    "    work_dir=WORK_DIR,\n",
    ")\n",
    "\n",
    "# add the param definitions and actions to the campaign\n",
    "campaign.add_app(\n",
    "    name=CAMPAIGN_NAME,\n",
    "    params=params,\n",
    "    actions=actions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbba5f8",
   "metadata": {},
   "source": [
    "As with the uncertainty-quantification (UQ) samplers, the `vary` is used to select which of the `params` we actually vary. Unlike the UQ samplers we do not specify an input probability distribution. This being a grid search, we simply specify a list of values for each hyperparameter. Parameters not in `vary`, but with a flag in the template, will be given the default value specified in `params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3247048",
   "metadata": {},
   "outputs": [],
   "source": [
    "vary = {\"n_neurons\": [64, 128], \"learning_rate\": [0.005, 0.01, 0.015]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e912c",
   "metadata": {},
   "source": [
    "**Note:** we are mixing integer and floats in the `vary` dict. Other data types (string, boolean) can also be used.\n",
    "\n",
    "The `vary` dict is passed to the `Grid_Sampler`. As can be seen, it created a tensor product of all 1D points specified in `vary`. If a single tensor product is not useful (e.g. because it creates combinations of parameters that do not makes sense), you can also pass a list of different `vary` dicts. For even more flexibility you can also write the required parameter combinations to a CSV file, and pass it to the `CSV_Sampler` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29e62d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 points:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([[64, 0.005],\n",
       "        [64, 0.01],\n",
       "        [64, 0.015],\n",
       "        [128, 0.005],\n",
       "        [128, 0.01],\n",
       "        [128, 0.015]], dtype=object)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an instance of the Grid Sampler\n",
    "sampler = uq.sampling.Grid_Sampler(vary)\n",
    "\n",
    "# Associate the sampler with the campaign\n",
    "campaign.set_sampler(sampler)\n",
    "\n",
    "# print the points\n",
    "print(\"There are %d points:\" % (sampler.n_samples()))\n",
    "sampler.points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c39b4b",
   "metadata": {},
   "source": [
    "Run the `actions` (create directories with `hyper_param_tune.py` files in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2095968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# execute the defined actions #\n",
    "###############################\n",
    "\n",
    "campaign.execute().collate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b149dd32",
   "metadata": {},
   "source": [
    "To run the ensemble, execute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b9cb1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing fabsim localhost run_uq_ensemble:grid_search,campaign_dir=/tmp/grid_test9hb35tv6,script=grid_search,skip=0,PJ=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wouter/anaconda3/lib/python3.9/site-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wouter/VECMA/FabSim/results/grid_search_localhost_16/RUNS/run_6/hyper_param_tune.py\", line 7, in <module>\n",
      "    import tensorflow as tf\n",
      "ModuleNotFoundError: No module named 'tensorflow'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/bin/fabsim\", line 46, in <module>\n",
      "    sys.exit(fabsim_main.main())\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/base/fabsim_main.py\", line 162, in main\n",
      "    env.exec_func(*env.task_args, **env.task_kwargs)\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/base/decorators.py\", line 75, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/wouter/VECMA/FabSim3/plugins/FabUQCampaign/FabUQCampaign.py\", line 67, in run_uq_ensemble\n",
      "    uq_ensemble(config, script, **args)\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/base/decorators.py\", line 75, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/wouter/VECMA/FabSim3/plugins/FabUQCampaign/FabUQCampaign.py\", line 38, in uq_ensemble\n",
      "    run_ensemble(config, sweep_dir, **args)\n",
      "  File \"<@beartype(fabsim.base.fab.run_ensemble) at 0x7584fd9758b0>\", line 134, in run_ensemble\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/base/fab.py\", line 1252, in run_ensemble\n",
      "    job_scripts_to_submit = job(\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/base/fab.py\", line 705, in job\n",
      "    job_submission(dict(job_script=job_script))\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/base/fab.py\", line 1037, in job_submission\n",
      "    run(\n",
      "  File \"<@beartype(fabsim.base.networks.run) at 0x7584fda13670>\", line 77, in run\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/base/networks.py\", line 146, in run\n",
      "    return manual(cmd, cd=cd, capture=capture)\n",
      "  File \"<@beartype(fabsim.base.networks.manual) at 0x7584fda13c10>\", line 77, in manual\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/base/networks.py\", line 209, in manual\n",
      "    return local(pre_cmd + \"'\" + manual_command + \"'\", capture=capture)\n",
      "  File \"<@beartype(fabsim.base.networks.local) at 0x7584feab5f70>\", line 54, in local\n",
      "  File \"/home/wouter/VECMA/FabSim3/fabsim/base/networks.py\", line 55, in local\n",
      "    raise RuntimeError(\n",
      "RuntimeError: \n",
      "local() encountered an error (return code 1)while executing 'ssh -Y -p 22 wouter@lh ' /home/wouter/VECMA/FabSim/results/grid_search_localhost_16/RUNS/run_6/grid_search_localhost_16_run_6.sh''\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################\n",
    "# run the UQ ensemble using the FabSim3 interface #\n",
    "###################################################\n",
    "\n",
    "fab.run_uq_ensemble(CONFIG, campaign.campaign_dir, script='grid_search',\n",
    "                    machine=MACHINE, PJ=PILOT_JOB)\n",
    "\n",
    "# wait for job to complete\n",
    "fab.wait(machine=MACHINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d2c0ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing fabsim localhost fetch_results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wouter/anaconda3/lib/python3.9/site-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing fabsim localhost verify_last_ensemble:grid_search,campaign_dir=/tmp/grid_test9hb35tv6,target_filename=output.csv,machine=localhost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wouter/anaconda3/lib/python3.9/site-packages/paramiko/transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "# check if all output files are retrieved from the remote machine, returns a Boolean flag\n",
    "all_good = fab.verify(CONFIG, campaign.campaign_dir, TARGET_FILENAME, machine=MACHINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b9838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not all samples executed correctly\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wouter/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if all_good:\n",
    "    # copy the results from the FabSim results dir to the EasyVVUQ results dir\n",
    "    fab.get_uq_samples(CONFIG, campaign.campaign_dir, sampler.n_samples(), machine=MACHINE)\n",
    "else:\n",
    "    print(\"Not all samples executed correctly\")\n",
    "    import sys\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907c295d",
   "metadata": {},
   "source": [
    "Briely:\n",
    "\n",
    "* `fab.run_uq_ensemble`: this command submits the ensemble to the (remote) host for execution. Under the hood it uses the FabSim3 `campaign2ensemble` subroutine to copy the run directories from `WORK_DIR` to the FabSim3 `SWEEP` directory, located in `config_files/grid_search/SWEEP`. From there the ensemble will be sent to the (remote) host.\n",
    "* `fab.wait`: this will check every minute on the status of the jobs on the remote host, and sleep otherwise, halting further execution of the script. On the localhost this command doesn't do anything.\n",
    "* `fab.verify`: this will execute the `verify_last_ensemble` subroutine to see if the output file `target_filename` for each run in the `SWEEP` directory is present in the corresponding FabSim3 results directory. Returns a boolean flag. `fab.verify` will also call the FabSim `fetch_results` method, which actually retreives the results from the (remote) host. So, if you want to just get the results without verifying the presence of output files, call `fab.fetch_results(machine=MACHINE)` instead. However, if something went wrong on the (remote) host, this will cause an error later on since not all required output files will be transfered on the EasyVVUQ `WORK_DIR`.\n",
    "* `fab.get_uq_samples`: copies the samples from the (local) FabSim results directory to the (local) EasyVVUQ campaign directory. It will not delete the results from the FabSim results directory. If you want to save space, you can delete the results on the FabSim side (see `results` directory in your FabSim home directory). You can also call `fab.clear_results(machine, name_results_dir)` to remove a specific FabSim results directory on a given machine.\n",
    "\n",
    "#### Error handling\n",
    "\n",
    "If `all_good == False` something went wrong on the (remote) host, and `sys.exit()` is called in our example, giving you the opportunity of investigating what went wrong. It can happen that a (small) number of jobs did not get executed on the remote host for some reason, whereas (most) jobs did execute succesfully. In this case simply resubmitting the failed jobs could be an option:\n",
    "\n",
    "```python\n",
    "fab.remove_succesful_runs(CONFIG, campaign.campaign_dir)\n",
    "fab.resubmit_previous_ensemble(CONFIG, 'grid_search')\n",
    "```\n",
    "\n",
    "The first command removes all succesful run directories from the `SWEEP` dir for which the output file `TARGET_FILENAME` has been found. For this to work, `fab.verify` must have been called. Then, `fab.resubmit_previous_ensemble` simply resubmits the runs that are present in the `SWEEP` directory, which by now only contains the failed runs. After the jobs have finished, call `fab.verify` again to see if now `TARGET_FILENAME` is present in the results directory, for every run in the `SWEEP` dir.\n",
    "\n",
    "Once we are sure we have all required output files, the role of FabSim is over, and we proceed with decoding the output files. In this case, our Python script wrote the training and test accuracy to a CSV file, hence we use the `SimpleCSV` decoder. \n",
    "\n",
    "**Note**: It is also possible to use a more flexible HDF5 format, by using `uq.decoders.HDF5` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b55725a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "# All output files are present, decode them #\n",
    "#############################################\n",
    "output_columns = [\"accuracy_train\", \"accuracy_test\"]\n",
    "\n",
    "decoder = uq.decoders.SimpleCSV(\n",
    "    target_filename=TARGET_FILENAME,\n",
    "    output_columns=output_columns)\n",
    "\n",
    "actions = uq.actions.Actions(\n",
    "    uq.actions.Decode(decoder),\n",
    ")\n",
    "\n",
    "campaign.replace_actions(CAMPAIGN_NAME, actions)\n",
    "\n",
    "###########################\n",
    "# Execute decoding action #\n",
    "###########################\n",
    "\n",
    "campaign.execute().collate()\n",
    "\n",
    "data_frame = campaign.get_collation_result()\n",
    "data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e62a8c",
   "metadata": {},
   "source": [
    "Display the hyperparameters with the maximum test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best hyperparameters with %.2f%% test accuracy:\" % (data_frame['accuracy_test'].max().values * 100,))\n",
    "data_frame.loc[data_frame['accuracy_test'].idxmax()][vary.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3a07de",
   "metadata": {},
   "source": [
    "## Executing a grid search on a remote host\n",
    "\n",
    "To run the example script on a remote host, a number of changes must be made. Ensure the remote host is defined in `machines.yml` in your FabSim3 directory, as well as the user login information. Assuming we'll run the ensemble on the Eagle super computer at the Poznan Supercomputing and Networking Center , the entry in `machines_user.yml` could look similar to the following:\n",
    "\n",
    "```\n",
    "eagle_vecma:\n",
    "  username: \"<your_username>\"\n",
    "  home_path_template: \"/tmp/lustre/<your_username>\"\n",
    "  budget: \"plgvecma2021\"\n",
    "  cores: 1\n",
    "  # job wall time for each job, format Days-Hours:Minutes:Seconds\n",
    "  job_wall_time : \"0-0:59:00\" # job wall time for each single job without PJ\n",
    "  PJ_size : \"1\" # number of requested nodes for PJ\n",
    "  PJ_wall_time : \"0-00:59:00\" # job wall time for PJ\n",
    "  modules:\n",
    "    loaded: [\"python/3.7.3\"] \n",
    "    unloaded: [] \n",
    "```\n",
    " Here:\n",
    " \n",
    " * `home_path_template`: the remote root directory for FabSim3, such that for instance the results on the remote machine will be stored in `home_path_template/FabSim3/results`.\n",
    " * `budget`: the name of the computational budget that you are allowed to use.\n",
    " * `cores`: the number of cores to use *per run*. Our simple Keras script justs need a single core, but applications which already have some built-in paralellism will require more cores.\n",
    " * `job_wall_time`: a time limit *per run*, and *without* the use of the QCG PilotJob framework.\n",
    " * `PJ_size`: the number of *nodes*, in the case *with* the use of the QCG PilotJob framework. \n",
    " * `PJ_wall_time`:  a *total* time limit, and *with* the use of the QCG PilotJob framework.\n",
    "\n",
    "To automatically setup the ssh keys, and prevent having to login manually for every random sample, run the following from the command line:\n",
    "\n",
    "```\n",
    "fabsim eagle_vecma setup_ssh_keys\n",
    "```\n",
    "\n",
    "Once the remote machine is properly setup, we can just set:\n",
    "\n",
    "```python\n",
    "# Use QCG PilotJob or not\n",
    "PILOT_JOB = False\n",
    "# machine to run ensemble on\n",
    "MACHINE = \"eagle_vecma\"\n",
    "```\n",
    "\n",
    "If you now re-run the example script, the ensemble will execute on the remote host, submitting each run as a separate job. By setting `PILOT_JOB=True`, all runs will be packaged in a single job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57104b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
